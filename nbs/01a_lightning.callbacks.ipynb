{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp lightning.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import _logger as log\n",
    "\n",
    "from src.lightning.core import *\n",
    "from src.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class WandbImageClassificationCallback(pl.Callback):\n",
    "    \"\"\" Custom callback to add some extra functionalites to the wandb logger \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dm: pl.LightningDataModule, \n",
    "                 default_config: dict = None, \n",
    "                 num_batches:int = 16, \n",
    "                 log_train_batch: bool = False,\n",
    "                 log_preds: bool = False,\n",
    "                 log_conf_mat: bool = True,):\n",
    "        \n",
    "        # class names for the confusion matrix\n",
    "        self.class_names = list(conf_mat_idx2lbl.values())\n",
    "        \n",
    "        # counter to log training batch images\n",
    "        self.dm = dm\n",
    "        self.num_bs = num_batches\n",
    "        self.curr_epoch = 0\n",
    "        self.log_train_batch = log_train_batch\n",
    "        self.log_preds = log_preds\n",
    "        self.val_imgs, self.val_labels = None, None\n",
    "        self.log_conf_mat = log_conf_mat\n",
    "        self.default_config = default_config\n",
    "        \n",
    "    def on_train_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        try:\n",
    "            # log model to the wandb experiment\n",
    "            wandb.watch(models=pl_module.model, criterion=pl_module.loss_func)\n",
    "        except:\n",
    "            log.info(\"Skipping wandb.watch --->\")\n",
    "            \n",
    "        train_augs, valid_augs = self.dm.train_augs, self.dm.valid_augs\n",
    "        self.train_config = dict(train_augments=train_augs, valid_augments=valid_augs)\n",
    "        \n",
    "        if self.default_config is not None: \n",
    "            try:\n",
    "                wandb.config.update(self.default_config)\n",
    "                wandb.config.update(self.train_config)\n",
    "                log.info(\"wandb config updated -->\")\n",
    "            except: \n",
    "                log.info(\"Skipping update wandb config -->\")\n",
    "        \n",
    "    def on_train_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_train_batch:\n",
    "            if pl_module.one_batch_of_image is None:\n",
    "                log.info(f\"{self.config_defaults['mixmethod']} samples not available . Skipping --->\")\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                one_batch = pl_module.one_batch_of_image[:self.num_bs]\n",
    "                train_ims = one_batch.data.to('cpu')\n",
    "                trainer.logger.experiment.log({\"train_batch\":[wandb.Image(x) for x in train_ims]}, commit=False)\n",
    "        \n",
    "    def on_validation_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_preds:\n",
    "            if self.val_imgs is None and self.val_labels is None:\n",
    "                self.val_imgs, self.val_labels = next(iter(self.dm.val_dataloader()))\n",
    "                self.val_imgs, self.val_labels = self.val_imgs[:self.num_bs], self.val_labels[:self.num_bs]\n",
    "                self.val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "\n",
    "            logits = pl_module(self.val_imgs)\n",
    "            preds  = torch.argmax(logits, 1)\n",
    "            preds  = preds.data.cpu()\n",
    "            trainer.logger.experiment.log({\"predictions\": [wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\") for x, pred, y in zip(self.val_imgs, preds, self.val_labels)]},\n",
    "                                          commit=False)\n",
    "            \n",
    "    def on_epoch_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        pl_module.val_labels_list = []\n",
    "        pl_module.val_preds_list  = []\n",
    "    \n",
    "    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_conf_mat:\n",
    "            val_preds  = torch.tensor(pl_module.val_preds_list).data.cpu().numpy()\n",
    "            val_labels = torch.tensor(pl_module.val_labels_list).data.cpu().numpy()\n",
    "            log_dict = {'conf_mat': wandb.plot.confusion_matrix(val_preds,val_labels,self.class_names)}\n",
    "            wandb.log(log_dict,commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LitProgressBar(pl.callbacks.ProgressBar):\n",
    "    \"Custom Progressbar callback for Lightning Training\"\n",
    "    def init_sanity_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for the validation sanity run. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Validation sanity check',\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            leave=False,\n",
    "            dynamic_ncols=True,\n",
    "            file=sys.stdout,\n",
    "        )\n",
    "        return bar\n",
    "    \n",
    "    def init_train_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Training',\n",
    "            initial=self.train_batch_idx,\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            leave=False,\n",
    "            dynamic_ncols=True,\n",
    "            file=sys.stdout,\n",
    "            smoothing=0,\n",
    "        )\n",
    "        return bar\n",
    "    \n",
    "    def init_validation_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for validation. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Validating',\n",
    "            position=(2 * self.process_position + 1),\n",
    "            disable=True,\n",
    "            leave=False,\n",
    "            dynamic_ncols=True,\n",
    "            file=sys.stdout\n",
    "        )\n",
    "        return bar\n",
    "    \n",
    "    def init_test_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for testing. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Testing',\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            leave=False,\n",
    "            dynamic_ncols=True,\n",
    "            file=sys.stdout\n",
    "        )\n",
    "        return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "from torch import nn\n",
    "from src.networks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss Function : LabelSmoothingCrossEntropy()\n",
      "12/31/2020 21:05:33 - INFO - lightning -   Loss Function : LabelSmoothingCrossEntropy()\n"
     ]
    }
   ],
   "source": [
    "train_augs = A.Compose([\n",
    "    A.RandomResizedCrop(224, 224, p=1.0),\n",
    "    A.RandomBrightness(limit=0.1),\n",
    "    A.HueSaturationValue(20, 20, 20),\n",
    "    A.HorizontalFlip(),\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensorV2(p=1.0)])\n",
    "\n",
    "valid_augs = A.Compose([\n",
    "    A.Resize(224, 224, p=1.0),\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensorV2(p=1.0)])\n",
    "\n",
    "csv = \"../../leaf-disease-classification-kaggle/data/stratified-data-5folds.csv\"\n",
    "ims = \"../../Datasets/cassava/train_images/\"\n",
    "dm = CassavaLightningDataModule(csv, ims, curr_fold=0, train_augs=train_augs, valid_augs=valid_augs, bs=32, num_workers=0)\n",
    "\n",
    "example_conf = dict(\n",
    "    mixmethod = None,\n",
    "    loss_function = dict(_target_='src.core.LabelSmoothingCrossEntropy', eps=0.1),\n",
    "    learning_rate = 1e-03,\n",
    "    lr_mult = 100,\n",
    "    optimizer = dict(_target_='src.opts.Ranger', betas=(0.9, 0.99), eps=1e-06, weight_decay=0),\n",
    "    scheduler = dict(_target_='src.opts.FlatCos', num_epochs=100),\n",
    "    metric_to_track = None,\n",
    "    step_after = \"step\",\n",
    "    frequency = 1,\n",
    ")\n",
    "\n",
    "\n",
    "encoder = timm.create_model('resnet18', pretrained=False)\n",
    "model = TransferLearningModel(encoder, cut=-2, c=5, act=nn.ReLU(inplace=True))\n",
    "model = LightningCassava(model=model, conf=example_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PrintLogsCallback(pl.Callback):\n",
    "    \"Logs Training logs to console after every epoch\"\n",
    "    def __init__(self, print_str: str = None, logger=None):\n",
    "        self.print_str = 'Epoch: [{}] eta: {} loss: {:.4f} acc: {:.4f} valid_loss: {:.4f} valid_acc: {:.4f}'\n",
    "        \n",
    "        if logger is None: self.logger = log\n",
    "        else             : self.logger = logger\n",
    "    \n",
    "    def on_epoch_start(self, *args, **kwargs):\n",
    "        self.eta_start = time.time()\n",
    "    \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        train_loss = metrics['train/loss']\n",
    "        train_acc  = metrics['train/acc']\n",
    "        valid_loss = metrics['valid/loss']\n",
    "        valid_acc  = metrics['valid/acc']\n",
    "        \n",
    "        end_time = time.time()\n",
    "        self.eta_string = str(datetime.timedelta(seconds=int(end_time-self.eta_start)))\n",
    "        self.curr_epoch = int(trainer.current_epoch)\n",
    "        print_str = self.print_str.format(self.curr_epoch, self.eta_string, \n",
    "                                        train_loss, train_acc, \n",
    "                                        valid_loss, valid_acc)\n",
    "        self.logger.info(print_str)\n",
    "    \n",
    "    def on_test_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        metrics = trainer.callback_metrics\n",
    "        train_loss = metrics['train/loss']\n",
    "        train_acc  = metrics['train/acc']\n",
    "        valid_loss = metrics['valid/loss']\n",
    "        valid_acc  = metrics['valid/acc']\n",
    "        test_loss  = metrics['test/loss']\n",
    "        test_acc   = metrics['test/acc']\n",
    "        \n",
    "        \n",
    "        fmt_str1 = \"Summary: [Train] loss: {:.4f} acc: {:.4f}\"\n",
    "        fmt_str2 = \"Summary: [Valid] loss: {:.4f} acc: {:.4f}\"\n",
    "        fmt_str3 = \"Summary: [Test]  loss: {:.4f} acc: {:.4f}\"\n",
    "        \n",
    "        str1 = fmt_str1.format(train_loss, train_acc)\n",
    "        str2 = fmt_str2.format(valid_loss, valid_acc)\n",
    "        str3 = fmt_str3.format(test_loss, test_acc)\n",
    "        \n",
    "        self.logger.info(str1)\n",
    "        self.logger.info(str2)\n",
    "        self.logger.info(str3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(\"metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "12/31/2020 21:08:39 - INFO - lightning -   GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "12/31/2020 21:08:39 - INFO - lightning -   TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(callbacks=[LitProgressBar(), PrintLogsCallback(logger=logger)], \n",
    "                     num_sanity_val_steps=0, max_epochs=2, limit_train_batches=1, \n",
    "                     limit_val_batches=1, limit_test_batches=1, weights_summary=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizer: Ranger  LR's: (1e-05, 0.001)\n",
      "12/31/2020 21:08:40 - INFO - lightning -   Optimizer: Ranger  LR's: (1e-05, 0.001)\n",
      "LR Scheculer: FlatCos\n",
      "12/31/2020 21:08:40 - INFO - lightning -   LR Scheculer: FlatCos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Epoch 0: 100%|██████████| 2/2 [00:08<00:00,  4.22s/it, loss=1.700, v_num=21, valid/loss=1.64, valid/acc=0.125]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/31/2020 21:08:48 - INFO - metrics -   Epoch: [0] eta: 0:00:08 loss: 1.6997 acc: 0.1875 valid_loss: 1.6374 valid_acc: 0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [00:08<00:00,  4.21s/it, loss=1.763, v_num=21, valid/loss=1.64, valid/acc=0.125]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/31/2020 21:08:57 - INFO - metrics -   Epoch: [1] eta: 0:00:08 loss: 1.8265 acc: 0.0938 valid_loss: 1.6421 valid_acc: 0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/31/2020 21:09:00 - INFO - metrics -   Summary: [Train] loss: 1.8265 acc: 0.0938\n",
      "12/31/2020 21:09:00 - INFO - metrics -   Summary: [Valid] loss: 1.6421 acc: 0.1250\n",
      "12/31/2020 21:09:00 - INFO - metrics -   Summary: [Test]  loss: 1.6421 acc: 0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    }
   ],
   "source": [
    "_ = trainer.test(model, datamodule=dm, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 00a_lightning.core.ipynb.\n",
      "Converted 00b_fastai.core.ipynb.\n",
      "Converted 01_mixmethods.ipynb.\n",
      "Converted 01a_lightning.callbacks.ipynb.\n",
      "Converted 02_layers.ipynb.\n",
      "Converted 02a_networks.ipynb.\n",
      "Converted 03_optimizers.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
