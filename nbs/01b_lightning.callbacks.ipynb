{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp lightning.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import _logger as log\n",
    "\n",
    "from src.lightning.core import *\n",
    "from src.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class WandbImageClassificationCallback(pl.Callback):\n",
    "    \"\"\" Custom callback to add some extra functionalites to the wandb logger \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dm: pl.LightningDataModule, \n",
    "                 default_config: dict = None, \n",
    "                 num_batches:int = 16, \n",
    "                 log_train_batch: bool = False,\n",
    "                 log_preds: bool = False,\n",
    "                 log_conf_mat: bool = True,):\n",
    "        \n",
    "        # class names for the confusion matrix\n",
    "        self.class_names = list(conf_mat_idx2lbl.values())\n",
    "        \n",
    "        # counter to log training batch images\n",
    "        self.dm = dm\n",
    "        self.num_bs = num_batches\n",
    "        self.curr_epoch = 0\n",
    "        self.log_train_batch = log_train_batch\n",
    "        self.log_preds = log_preds\n",
    "        self.val_imgs, self.val_labels = None, None\n",
    "        self.log_conf_mat = log_conf_mat\n",
    "        self.default_config = default_config\n",
    "        \n",
    "    def on_train_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        try:\n",
    "            # log model to the wandb experiment\n",
    "            wandb.watch(models=pl_module.model, criterion=pl_module.loss_func)\n",
    "        except:\n",
    "            log.info(\"Skipping wandb.watch --->\")\n",
    "            \n",
    "        train_augs, valid_augs = self.dm.train_augs, self.dm.valid_augs\n",
    "        self.train_config = dict(train_augments=train_augs, valid_augments=valid_augs)\n",
    "        \n",
    "        if self.default_config is not None: \n",
    "            try:\n",
    "                wandb.config.update(self.default_config)\n",
    "                wandb.config.update(self.train_config)\n",
    "                log.info(\"wandb config updated -->\")\n",
    "            except: \n",
    "                log.info(\"Skipping update wandb config -->\")\n",
    "        \n",
    "    def on_train_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_train_batch:\n",
    "            if pl_module.one_batch_of_image is None:\n",
    "                log.info(f\"{self.config_defaults['mixmethod']} samples not available . Skipping --->\")\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                one_batch = pl_module.one_batch_of_image[:self.num_bs]\n",
    "                train_ims = one_batch.data.to('cpu')\n",
    "                trainer.logger.experiment.log({\"train_batch\":[wandb.Image(x) for x in train_ims]}, commit=False)\n",
    "        \n",
    "    def on_validation_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_preds:\n",
    "            if self.val_imgs is None and self.val_labels is None:\n",
    "                self.val_imgs, self.val_labels = next(iter(self.dm.val_dataloader()))\n",
    "                self.val_imgs, self.val_labels = self.val_imgs[:self.num_bs], self.val_labels[:self.num_bs]\n",
    "                self.val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "\n",
    "            logits = pl_module(self.val_imgs)\n",
    "            preds  = torch.argmax(logits, 1)\n",
    "            preds  = preds.data.cpu()\n",
    "            trainer.logger.experiment.log({\"predictions\": [wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\") for x, pred, y in zip(self.val_imgs, preds, self.val_labels)]},\n",
    "                                          commit=False)\n",
    "            \n",
    "    def on_epoch_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        pl_module.val_labels_list = []\n",
    "        pl_module.val_preds_list  = []\n",
    "    \n",
    "    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_conf_mat:\n",
    "            val_preds  = torch.tensor(pl_module.val_preds_list).data.cpu().numpy()\n",
    "            val_labels = torch.tensor(pl_module.val_labels_list).data.cpu().numpy()\n",
    "            log_dict = {'conf_mat': wandb.plot.confusion_matrix(val_preds,val_labels,self.class_names)}\n",
    "            wandb.log(log_dict,commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LitProgressBar(pl.callbacks.ProgressBar):\n",
    "    \"Custom Progressbar callback for Lightning Training\"\n",
    "    def init_sanity_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for the validation sanity run. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Validation sanity check',\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            leave=False,\n",
    "            dynamic_ncols=True,\n",
    "            file=sys.stdout,\n",
    "        )\n",
    "        return bar\n",
    "    \n",
    "    def init_train_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Training',\n",
    "            initial=self.train_batch_idx,\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            leave=False,\n",
    "            dynamic_ncols=True,\n",
    "            file=sys.stdout,\n",
    "            smoothing=0,\n",
    "        )\n",
    "        return bar\n",
    "    \n",
    "    def init_validation_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for validation. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Validating',\n",
    "            position=(2 * self.process_position + 1),\n",
    "            disable=True,\n",
    "            leave=False,\n",
    "            dynamic_ncols=True,\n",
    "            file=sys.stdout\n",
    "        )\n",
    "        return bar\n",
    "    \n",
    "    def init_test_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for testing. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Testing',\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            leave=True,\n",
    "            dynamic_ncols=True,\n",
    "            file=sys.stdout\n",
    "        )\n",
    "        return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "from torch import nn\n",
    "from src.networks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss Function : LabelSmoothingCrossEntropy()\n"
     ]
    }
   ],
   "source": [
    "train_augs = A.Compose([\n",
    "    A.RandomResizedCrop(224, 224, p=1.0),\n",
    "    A.RandomBrightness(limit=0.1),\n",
    "    A.HueSaturationValue(20, 20, 20),\n",
    "    A.HorizontalFlip(),\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensorV2(p=1.0)])\n",
    "\n",
    "valid_augs = A.Compose([\n",
    "    A.Resize(224, 224, p=1.0),\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensorV2(p=1.0)])\n",
    "\n",
    "csv = \"../../leaf-disease-classification-kaggle/data/stratified-data-5folds.csv\"\n",
    "ims = \"../../Datasets/cassava/train_images/\"\n",
    "dm = CassavaLightningDataModule(csv, ims, curr_fold=0, train_augs=train_augs, valid_augs=valid_augs, bs=32, num_workers=0)\n",
    "\n",
    "example_conf = dict(\n",
    "    mixmethod = None,\n",
    "    loss_function = dict(_target_='src.core.LabelSmoothingCrossEntropy', eps=0.1),\n",
    "    learning_rate = 1e-03,\n",
    "    lr_mult = 100,\n",
    "    optimizer = dict(_target_='src.opts.Ranger', betas=(0.9, 0.99), eps=1e-06, weight_decay=0),\n",
    "    scheduler = dict(_target_='src.opts.FlatCos', num_epochs=100),\n",
    "    metric_to_track = None,\n",
    "    step_after = \"step\",\n",
    "    frequency = 1,\n",
    ")\n",
    "\n",
    "\n",
    "encoder = timm.create_model('resnet18', pretrained=False)\n",
    "model = TransferLearningModel(encoder, cut=-2, c=5, act=nn.ReLU(inplace=True))\n",
    "model = LightningCassava(model=model, conf=example_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PrintLogsCallback(pl.Callback):\n",
    "    \"Logs Training logs to console after every epoch\"\n",
    "    def __init__(self, print_str: str = None):\n",
    "        self.print_str = 'Epoch: [{}] eta: {} loss: {:.4f} acc: {:.4f} valid_loss: {:.4f} valid_acc: {:.4f}'\n",
    "    \n",
    "    def on_epoch_start(self, *args, **kwargs):\n",
    "        self.eta_start = time.time()\n",
    "    \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        train_loss = metrics['train/loss']\n",
    "        train_acc  = metrics['train/acc']\n",
    "        valid_loss = metrics['valid/loss']\n",
    "        valid_acc  = metrics['valid/acc']\n",
    "        \n",
    "        end_time = time.time()\n",
    "        self.eta_string = str(datetime.timedelta(seconds=int(end_time-self.eta_start)))\n",
    "        self.curr_epoch = int(trainer.current_epoch)\n",
    "        print_str = self.print_str.format(self.curr_epoch, self.eta_string, \n",
    "                                        train_loss, train_acc, \n",
    "                                        valid_loss, valid_acc)\n",
    "        log.info(print_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(callbacks=[LitProgressBar(), PrintLogsCallback()], \n",
    "                     num_sanity_val_steps=0, max_epochs=2, limit_train_batches=1, limit_val_batches=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DATA: ../../Datasets/cassava/train_images/\n",
      "FOLD: 0  BATCH_SIZE: 32\n",
      "Optimizer: Ranger  LR's: (1e-05, 0.001)\n",
      "LR Scheculer: FlatCos\n",
      "\n",
      "  | Name      | Type                       | Params\n",
      "---------------------------------------------------------\n",
      "0 | model     | TransferLearningModel      | 11.7 M\n",
      "1 | loss_func | LabelSmoothingCrossEntropy | 0     \n",
      "2 | accuracy  | Accuracy                   | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Epoch 0: 100%|██████████| 2/2 [00:08<00:00,  4.33s/it, loss=1.543, v_num=14, valid/loss=1.61, valid/acc=0.125]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0] eta: 0:00:09 loss: 1.5433 acc: 0.3125 valid_loss: 1.6130 valid_acc: 0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [00:08<00:00,  4.07s/it, loss=1.579, v_num=14, valid/loss=1.61, valid/acc=0.125]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1] eta: 0:00:08 loss: 1.6147 acc: 0.2500 valid_loss: 1.6081 valid_acc: 0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = WandbImageClassificationCallback(dm=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 00a_lightning.core.ipynb.\n",
      "Converted 00b_fastai.core.ipynb.\n",
      "Converted 01_mixmethods.ipynb.\n",
      "Converted 01b_lightning.callbacks.ipynb.\n",
      "Converted 02_layers.ipynb.\n",
      "Converted 02a_networks.ipynb.\n",
      "Converted 03_optimizers.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
