{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp lightning.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "from time import perf_counter\n",
    "import logging\n",
    "\n",
    "from collections import namedtuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.memory import ModelSummary, get_human_readable_count\n",
    "\n",
    "from src.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorlog\n",
    "\n",
    "handler = colorlog.StreamHandler()\n",
    "\n",
    "fmt = \"%(green)s[%(asctime)s %(name)s]%(reset)s: %(message)s\"\n",
    "colors = dict(DEBUG=\"purple\", INFO=\"green\", WARNING=\"yellow\", ERROR=\"red\", CRITICAL=\"red\")\n",
    "formatter = colorlog.ColoredFormatter(fmt=fmt, log_colors=colors, datefmt=\"%m/%d %H:%M:%S\")\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "logging.basicConfig(format=fmt, level=logging.INFO, handlers=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# from : https://github.com/facebookresearch/fvcore/blob/master/fvcore/common/timer.py\n",
    "class Timer:\n",
    "    \"\"\"\n",
    "    A timer which computes the time elapsed since the start/reset of the timer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset the timer.\n",
    "        \"\"\"\n",
    "        self._start = perf_counter()\n",
    "        self._paused: Optional[float] = None\n",
    "        self._total_paused = 0\n",
    "        self._count_start = 1\n",
    "\n",
    "    def pause(self) -> None:\n",
    "        \"\"\"\n",
    "        Pause the timer.\n",
    "        \"\"\"\n",
    "        if self._paused is not None:\n",
    "            raise ValueError(\"Trying to pause a Timer that is already paused!\")\n",
    "        self._paused = perf_counter()\n",
    "\n",
    "    def is_paused(self) -> bool:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            bool: whether the timer is currently paused\n",
    "        \"\"\"\n",
    "        return self._paused is not None\n",
    "\n",
    "    def resume(self) -> None:\n",
    "        \"\"\"\n",
    "        Resume the timer.\n",
    "        \"\"\"\n",
    "        if self._paused is None:\n",
    "            raise ValueError(\"Trying to resume a Timer that is not paused!\")\n",
    "        self._total_paused += perf_counter() - self._paused  # pyre-ignore\n",
    "        self._paused = None\n",
    "        self._count_start += 1\n",
    "\n",
    "    def seconds(self) -> float:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            (float): the total number of seconds since the start/reset of the\n",
    "                timer, excluding the time when the timer is paused.\n",
    "        \"\"\"\n",
    "        if self._paused is not None:\n",
    "            end_time: float = self._paused  # type: ignore\n",
    "        else:\n",
    "            end_time = perf_counter()\n",
    "        return end_time - self._start - self._total_paused\n",
    "\n",
    "    def avg_seconds(self) -> float:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            (float): the average number of seconds between every start/reset and\n",
    "            pause.\n",
    "        \"\"\"\n",
    "        return self.seconds() / self._count_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class WandbImageClassificationCallback(pl.Callback):\n",
    "    \"\"\" Custom callback to add some extra functionalites to the wandb logger \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_batches: int = 16,\n",
    "        log_train_batch: bool = False,\n",
    "        log_preds: bool = False,\n",
    "        log_conf_mat: bool = True,\n",
    "    ):\n",
    "\n",
    "        # class names for the confusion matrix\n",
    "        self.class_names = list(conf_mat_idx2lbl.values())\n",
    "\n",
    "        # counter to log training batch images\n",
    "        self.num_bs = num_batches\n",
    "        self.curr_epoch = 0\n",
    "\n",
    "        self.log_train_batch = log_train_batch\n",
    "        self.log_preds = log_preds\n",
    "        self.log_conf_mat = log_conf_mat\n",
    "\n",
    "        self.val_imgs, self.val_labels = None, None\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        try:\n",
    "            # log model to the wandb experiment\n",
    "            wandb.watch(models=pl_module.model, criterion=pl_module.loss_func)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_train_batch:\n",
    "            if pl_module.one_batch is None:\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                one_batch = pl_module.one_batch[: self.num_bs]\n",
    "                train_ims = one_batch.data.to(\"cpu\")\n",
    "                trainer.logger.experiment.log(\n",
    "                    {\"train_batch\": [wandb.Image(x) for x in train_ims]}, commit=False\n",
    "                )\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_preds:\n",
    "            if self.val_imgs is None and self.val_labels is None:\n",
    "                self.val_imgs, self.val_labels = next(iter(pl_module.val_dataloader()))\n",
    "                self.val_imgs, self.val_labels = (\n",
    "                    self.val_imgs[: self.num_bs],\n",
    "                    self.val_labels[: self.num_bs],\n",
    "                )\n",
    "                self.val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "\n",
    "            logits = pl_module(self.val_imgs)\n",
    "            preds = torch.argmax(logits, 1)\n",
    "            preds = preds.data.cpu()\n",
    "\n",
    "            ims = [\n",
    "                wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\")\n",
    "                for x, pred, y in zip(self.val_imgs, preds, self.val_labels)\n",
    "            ]\n",
    "            log_dict = {\"predictions\": ims}\n",
    "            wandb.log(ims, commit=False)\n",
    "\n",
    "    def on_epoch_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        pl_module.val_labels_list = []\n",
    "        pl_module.val_preds_list = []\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_conf_mat:\n",
    "            val_preds = torch.tensor(pl_module.val_preds_list).data.cpu().numpy()\n",
    "            val_labels = torch.tensor(pl_module.val_labels_list).data.cpu().numpy()\n",
    "            log_dict = {\n",
    "                \"conf_mat\": wandb.plot.confusion_matrix(\n",
    "                    val_preds, val_labels, self.class_names\n",
    "                )\n",
    "            }\n",
    "            wandb.log(log_dict, commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DisableValidationBar(pl.callbacks.ProgressBar):\n",
    "    \"Custom Progressbar callback for Lightning Training which disables the validation bar\"\n",
    "\n",
    "    def init_sanity_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for the validation sanity run. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Validation sanity check\",\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_train_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Training\",\n",
    "            initial=self.train_batch_idx,\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_validation_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for validation. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Validating\",\n",
    "            position=(2 * self.process_position + 1),\n",
    "            disable=True,\n",
    "            dynamic_ncols=False,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_test_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for testing. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Testing\",\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PrintLogsCallback(pl.Callback):\n",
    "    \"Logs Training logs to console after every epoch\"\n",
    "    TrainResult = namedtuple(\"TrainOutput\", [\"loss\", \"acc\", \"val_loss\", \"val_acc\"])\n",
    "    TestResult = namedtuple(\"TestOutput\", [\"test_loss\", \"test_acc\"])\n",
    "\n",
    "    logger = logging.getLogger(\"train\")\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        train_loss = metrics[\"train/loss_epoch\"]\n",
    "        train_acc = metrics[\"train/acc_epoch\"]\n",
    "        valid_loss = metrics[\"valid/loss\"]\n",
    "        valid_acc = metrics[\"valid/acc\"]\n",
    "        trn_res = self.TrainResult(\n",
    "            round(train_loss.data.cpu().numpy().item(), 3),\n",
    "            round(train_acc.data.cpu().numpy().item(), 3),\n",
    "            round(valid_loss.data.cpu().numpy().item(), 3),\n",
    "            round(valid_acc.data.cpu().numpy().item(), 3),\n",
    "        )\n",
    "\n",
    "        curr_epoch = int(trainer.current_epoch)\n",
    "        self.logger.info(f\"EPOCH {curr_epoch}: {trn_res}\")\n",
    "\n",
    "    def on_test_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        metrics = trainer.callback_metrics\n",
    "        test_loss = metrics[\"test/loss\"]\n",
    "        test_acc = metrics[\"test/acc\"]\n",
    "        self.logger.info(\n",
    "            f\"{self.TestResult(round(test_loss.data.cpu().numpy().item(), 2), round(test_acc.data.cpu().numpy().item(), 2))}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DisableProgressBar(pl.callbacks.ProgressBar):\n",
    "    \"Custom Progressbar callback for Lightning Training which disables the validation bar\"\n",
    "\n",
    "    def init_sanity_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for the validation sanity run. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Validation sanity check\",\n",
    "            position=(2 * self.process_position),\n",
    "            disable=True,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_train_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Training\",\n",
    "            initial=self.train_batch_idx,\n",
    "            position=(2 * self.process_position),\n",
    "            disable=True,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_validation_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for validation. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Validating\",\n",
    "            position=(2 * self.process_position + 1),\n",
    "            disable=True,\n",
    "            dynamic_ncols=False,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_test_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for testing. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Testing\",\n",
    "            position=(2 * self.process_position),\n",
    "            disable=True,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# modified from : https://github.com/facebookresearch/detectron2/blob/master/detectron2/engine/hooks.py\n",
    "class IterationTimer:\n",
    "    \"\"\"\n",
    "    Track the time spent for each iteration (each run_step call in the trainer).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._step_timer = Timer()\n",
    "        self._total_timer = Timer()\n",
    "        self._start_time = time.perf_counter()\n",
    "        self.eta_list = []\n",
    "\n",
    "    def before_train(self):\n",
    "        self._start_time = time.perf_counter()\n",
    "        self._total_timer.reset()\n",
    "        self._total_timer.pause()\n",
    "\n",
    "    def after_train(self, num_iter: int, stage:str = \"training\"):\n",
    "        logger = logging.getLogger(\"train\")\n",
    "        total_time = time.perf_counter() - self._start_time\n",
    "        total_time_minus_hooks = self._total_timer.seconds()\n",
    "        hook_time = total_time - total_time_minus_hooks\n",
    "\n",
    "        if num_iter > 0 and total_time_minus_hooks > 0:\n",
    "            logger.info(\n",
    "                \"Overall {} speed: {} iterations in {} ({:.4f} s / it)\".format(\n",
    "                    stage,\n",
    "                    num_iter,\n",
    "                    str(datetime.timedelta(seconds=int(total_time_minus_hooks))),\n",
    "                    total_time_minus_hooks / num_iter,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def before_step(self):\n",
    "        self._step_timer.reset()\n",
    "        self._total_timer.resume()\n",
    "\n",
    "    def after_step(self):\n",
    "        sec = self._step_timer.seconds()\n",
    "        self.eta_list.append(sec)\n",
    "        self._total_timer.pause()\n",
    "\n",
    "    def get_eta(self, max_iter: int, iteration: int):\n",
    "        eta_seconds = np.median(self.eta_list[-1000:]) * (max_iter - iteration - 1)\n",
    "        eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "        return eta_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ConsoleLogger(pl.Callback):\n",
    "    \"Fancy logger for console-logging\"\n",
    "    trn_res = namedtuple(\"TrainOutput\", [\"loss\", \"acc\", \"val_loss\", \"val_acc\"])\n",
    "    tst_res = namedtuple(\"TestOutput\", [\"test_loss\", \"test_acc\"])\n",
    "    curr_step = 0\n",
    "    \n",
    "    logger = logging.getLogger(\"train\")\n",
    "\n",
    "    def __init__(self, print_every: int = 50):\n",
    "        self.print_every = print_every\n",
    "        self.train_timer = IterationTimer()\n",
    "        self.test_timer = IterationTimer()\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        self.start_time = time.perf_counter()\n",
    "        \n",
    "        cfg = pl_module.hparams\n",
    "\n",
    "        model_class = str(cfg.network.transfer_learning_model._target_).split(\".\")[-1]\n",
    "        summary = ModelSummary(pl_module)\n",
    "        param_count = get_human_readable_count(summary.param_nums[0])\n",
    "            \n",
    "        # log information about the model\n",
    "        _model = namedtuple(\"Model\", [\"base_classifier\", \"kind\"])\n",
    "        self.log_msg(f\"{_model(cfg.encoder, model_class)} created, param_count: {param_count}\")\n",
    "\n",
    "        path = os.path.relpath(cfg.datamodule.im_dir)\n",
    "        oof_fold = str(cfg.datamodule.curr_fold)\n",
    "        trn_batches = len(pl_module.train_dataloader())\n",
    "        val_batches = len(pl_module.val_dataloader())\n",
    "        tst_batches = len(pl_module.test_dataloader())\n",
    "        dl = pl_module.train_dataloader()\n",
    "        try:\n",
    "            _trn_tfms = list(dl.dataset.tfms.transforms)\n",
    "        except:\n",
    "            _trn_tfms = list(dl.dataset.transforms.transforms)\n",
    "            \n",
    "        # log information about the dataset\n",
    "        self.log_msg(f\"Loaded dataset from {path}, using {oof_fold} fold as the OOF validation dataset.\")\n",
    "        self.log_msg(f\"Loaded dataset has {trn_batches} train + {val_batches} valid + {tst_batches} test.\")\n",
    "        self.log_msg(f\"Transformations used in training: {_trn_tfms}.\")\n",
    "        self.log_msg(f\"Images are resized to {(cfg.image_dims, cfg.image_dims, 3)}.\")\n",
    "        self.log_msg(f\"Creating batches of size {str(cfg.datamodule.bs)} and concating the images.\")\n",
    "\n",
    "        lr_scheduler = str(cfg.scheduler.function._target_).split(\".\")[-1]\n",
    "        optimizer = str(cfg.optimizer._target_).split(\".\")[-1]\n",
    "        # log information about the training parameters\n",
    "            \n",
    "        self.log_msg(f\"Scheduled epochs: {trainer.max_epochs}\")\n",
    "        self.log_msg(f\"Accumulate batches: {trainer.accumulate_grad_batches}\")\n",
    "            \n",
    "        self.log_msg(f\"Training with {optimizer}, lrs={str(pl_module.lr_list)}, wd={str(cfg.optimizer.weight_decay)}\")\n",
    "        self.log_msg(f\"Scheduling learining_rate via {lr_scheduler} scheduler.\")\n",
    "            \n",
    "        self.log_msg(f\"Loss Function used : {pl_module.loss_func}\")\n",
    "        \n",
    "        if pl_module.mix_fn is not None:\n",
    "            self.log_msg(f\"Training with {pl_module.mix_fn}\")\n",
    "\n",
    "        # start the training job\n",
    "        self.log_msg(f\"Start TRAIN / VALIDATION from epoch {trainer.current_epoch}\")\n",
    "\n",
    "        self.total_iters = trn_batches * trainer.max_epochs\n",
    "        self.current_iteration = 0\n",
    "        self.train_timer.before_train()\n",
    "        self.total_test_iters = tst_batches\n",
    "\n",
    "    def on_train_epoch_start(self, *args, **kwargs):\n",
    "        self.step_loss = 0\n",
    "        self.step_acc = 0\n",
    "        self.seen_batches = 0\n",
    "\n",
    "    def on_train_batch_start(self, *args, **kwargs):\n",
    "        self.train_timer.before_step()\n",
    "        self.seen_batches += 1\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        msg = \"eta: {} iteration: {} loss: {:.3f} accuracy: {:.3f} lrs: {}\"\n",
    "\n",
    "        mini_batch_size = pl_module.hparams.datamodule.bs\n",
    "\n",
    "        step_metrics = trainer.callback_metrics\n",
    "\n",
    "        self.step_loss += step_metrics[\"train/loss_step\"]\n",
    "        self.step_acc += step_metrics[\"train/acc_step\"]\n",
    "\n",
    "        self.train_timer.after_step()\n",
    "\n",
    "        if self.curr_step % self.print_every == 0:\n",
    "            # compute average loss/accuracy\n",
    "            avg_loss = self.step_loss / self.seen_batches\n",
    "            avg_acc = self.step_acc / self.seen_batches\n",
    "\n",
    "            # the learning-rates of the model parameters\n",
    "            lrs = tuple(trainer.lr_schedulers[0][\"scheduler\"].get_lr())\n",
    "            lrs = tuple([float(\"{0:.1e}\".format(v)) for v in lrs])\n",
    "\n",
    "            # get eta from timer\n",
    "            eta = self.train_timer.get_eta(max_iter=self.total_iters, iteration=self.current_iteration)\n",
    "\n",
    "            self.log_msg(msg.format(eta, self.current_iteration, avg_loss, avg_acc, lrs))\n",
    "\n",
    "        # increment iterations\n",
    "        self.current_iteration += 1\n",
    "        self.curr_step += 1\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        metrics = trainer.callback_metrics\n",
    "\n",
    "        train_loss = metrics[\"train/loss_epoch\"]\n",
    "        train_acc = metrics[\"train/acc_epoch\"]\n",
    "\n",
    "        valid_loss = metrics[\"valid/loss\"]\n",
    "        valid_acc = metrics[\"valid/acc\"]\n",
    "\n",
    "        _res = self.trn_res(\n",
    "            round(train_loss.data.cpu().numpy().item(), 3),\n",
    "            round(train_acc.data.cpu().numpy().item(), 3),\n",
    "            round(valid_loss.data.cpu().numpy().item(), 3),\n",
    "            round(valid_acc.data.cpu().numpy().item(), 3),\n",
    "        )\n",
    "\n",
    "        curr_epoch = int(trainer.current_epoch)\n",
    "        self.log_msg(f\"EPOCH {curr_epoch}: (100 % done) {_res}\")\n",
    "\n",
    "    def on_train_end(self, *args, **kwargs):\n",
    "        time_elasped = time.perf_counter() - self.start_time\n",
    "        self.log_msg(f\"Total training time: {str(datetime.timedelta(seconds=int(time_elasped)))}\")\n",
    "        self.train_timer.after_train(num_iter=self.current_iteration)\n",
    "\n",
    "    def on_test_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        self.test_iters = 0\n",
    "        self.test_start = time.perf_counter()\n",
    "        self.test_timer.before_train()\n",
    "\n",
    "        path = os.path.relpath(trainer.checkpoint_callback.dirpath)\n",
    "        self.log_msg(f\"Start TEST on {self.total_test_iters} batches\")\n",
    "        \n",
    "        dl = pl_module.val_dataloader()\n",
    "        try:\n",
    "            _val_tfms = list(dl.dataset.tfms.transforms)\n",
    "        except:\n",
    "            _val_tfms = list(dl.dataset.transforms.transforms)\n",
    "        self.log_msg(f\"Transformations used: {_val_tfms}\")\n",
    "        \n",
    "    def on_test_batch_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        self.test_timer.before_step()\n",
    "\n",
    "    def on_test_batch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        self.test_timer.after_step()\n",
    "\n",
    "        if self.test_iters % self.print_every == 0:\n",
    "            eta = self.test_timer.get_eta(max_iter=self.total_test_iters, iteration=self.test_iters)\n",
    "            self.log_msg(f\"Testing done {self.test_iters}/{self.total_test_iters}. ETA={eta}\")\n",
    "        self.test_iters += 1\n",
    "\n",
    "    def on_test_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        metrics = trainer.callback_metrics\n",
    "        test_loss = metrics[\"test/loss\"]\n",
    "        test_acc = metrics[\"test/acc\"]\n",
    "\n",
    "        loss = round(test_loss.data.cpu().numpy().item(), 2)\n",
    "        accuracy = round(test_acc.data.cpu().numpy().item(), 2)\n",
    "        self.log_msg(f\"Test results: (100 % done) {self.tst_res(loss, accuracy)}\")\n",
    "\n",
    "    def on_test_end(self, *args, **kwargs):\n",
    "        time_elasped = time.perf_counter() - self.test_start\n",
    "        self.log_msg(f\"Total testing time: {str(datetime.timedelta(seconds=int(time_elasped)))}\")\n",
    "        self.test_timer.after_train(num_iter=self.test_iters, stage=\"testing\")\n",
    "\n",
    "    def log_msg(self, msg: str):\n",
    "        self.logger.info(msg)\n",
    "\n",
    "    def log_line(self):\n",
    "        self.logger.info(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "from torch import nn\n",
    "from src.networks import *\n",
    "\n",
    "from hydra.experimental import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overrides = [\n",
    "    \"image_dims=120\",\n",
    "    \"datamodule.bs=5\",\n",
    "    \"datamodule.num_workers=0\",\n",
    "    \"general=default\",\n",
    "    \"trainer=fast-dev-cpu\",\n",
    "    \"mixmethod=mixup\",\n",
    "    \"network=transferlearning\",\n",
    "    \"augmentations=custom-augs\",\n",
    "    \"mixmethod.steps=2\",\n",
    "    \"network.activation=mish\",\n",
    "]\n",
    "\n",
    "with initialize(config_path=os.path.relpath(\"../conf/\")):\n",
    "    cfg = compose(config_name=\"effnet-base\", overrides=overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/25 01:54:20 root]\u001b[0m: Using Mish() activation function.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "\u001b[32m[01/25 01:54:22 lightning]\u001b[0m: GPU available: False, used: False\u001b[0m\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\u001b[32m[01/25 01:54:22 lightning]\u001b[0m: TPU available: False, using: 0 TPU cores\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cfg.image_dims = 255\n",
    "\n",
    "trn_augs = None\n",
    "val_augs = None\n",
    "\n",
    "dm = instantiate(\n",
    "    cfg.datamodule, train_augs=trn_augs, valid_augs=val_augs, default_config=cfg\n",
    ")\n",
    "\n",
    "activation_func = activation_map[cfg.network.activation]\n",
    "logging.info(f\"Using {activation_func()} activation function.\")\n",
    "encoder = timm.create_model(cfg.encoder, pretrained=False, act_layer=activation_func)\n",
    "model = TransferLearningModel(encoder, cut=-2, c=5, act=activation_func(inplace=True))\n",
    "model = LightningCassava(model=model, conf=cfg)\n",
    "\n",
    "lg = ConsoleLogger(print_every=5)\n",
    "\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "\n",
    "trainer = instantiate(\n",
    "    cfg.trainer,\n",
    "    callbacks=[DisableProgressBar(), lg],\n",
    "    max_epochs=2,\n",
    "    limit_train_batches=10,\n",
    "    limit_val_batches=2,\n",
    "    limit_test_batches=5,\n",
    "    weights_summary=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/25 01:54:23 train]\u001b[0m: Model(base_classifier='tf_efficientnet_b0_ns', kind='TransferLearningModel') created, param_count: 5.3 M\u001b[0m\n",
      "\u001b[32m[01/25 01:54:23 train]\u001b[0m: Loaded dataset from ../../Datasets/cassava/train_images, using 0 fold as the OOF validation dataset.\u001b[0m\n",
      "\u001b[32m[01/25 01:54:23 train]\u001b[0m: Loaded dataset has 3424 train + 856 valid + 856 test.\u001b[0m\n",
      "\u001b[32m[01/25 01:54:23 train]\u001b[0m: Transformations used in training: [RandomResizedCropAndInterpolation(size=(255, 255), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR PIL.Image.BICUBIC), RandomHorizontalFlip(p=0.5), RandomVerticalFlip(p=0.5), ColorJitter(brightness=[0.9, 1.1], contrast=[0.9, 1.1], saturation=[0.9, 1.1], hue=None), ToTensor(), Normalize(mean=tensor([0.4298, 0.4962, 0.3130]), std=tensor([0.2142, 0.2191, 0.1954])), <timm.data.random_erasing.RandomErasing object at 0x7fe3fcffcdd0>].\u001b[0m\n",
      "\u001b[32m[01/25 01:54:23 train]\u001b[0m: Images are resized to (255, 255, 3).\u001b[0m\n",
      "\u001b[32m[01/25 01:54:23 train]\u001b[0m: Creating batches of size 5 and concating the images.\u001b[0m\n",
      "\u001b[32m[01/25 01:54:23 train]\u001b[0m: Scheduled epochs: 2\u001b[0m\n",
      "\u001b[32m[01/25 01:54:23 train]\u001b[0m: Accumulate batches: 1\u001b[0m\n",
      "\u001b[32m[01/25 01:54:23 train]\u001b[0m: Training with AdamW, lrs=Lrs(encoder_lr=0.0001, fc_lr=0.001), wd=0.1\u001b[0m\n",
      "\u001b[32m[01/25 01:54:23 train]\u001b[0m: Scheduling learining_rate via LinearSchedulerWithWarmup scheduler.\u001b[0m\n",
      "\u001b[32m[01/25 01:54:23 train]\u001b[0m: Loss Function used : BiTemperedLogisticLoss()\u001b[0m\n",
      "\u001b[32m[01/25 01:54:23 train]\u001b[0m: Training with Mixup(probability=1.0, alpha=0.4, iters=2)\u001b[0m\n",
      "\u001b[32m[01/25 01:54:23 train]\u001b[0m: Start TRAIN / VALIDATION from epoch 0\u001b[0m\n",
      "\u001b[32m[01/25 01:54:25 train]\u001b[0m: eta: 3:30:41 iteration: 0 loss: 0.958 accuracy: 0.000 lrs: (0.0, 0.0)\u001b[0m\n",
      "\u001b[32m[01/25 01:54:27 src.mixmethods]\u001b[0m: Threshold steps reached, stopping Mixup(probability=1.0, alpha=0.4, iters=2)\u001b[0m\n",
      "\u001b[32m[01/25 01:54:34 train]\u001b[0m: eta: 3:23:38 iteration: 5 loss: 0.922 accuracy: 0.133 lrs: (1.5e-07, 1.5e-06)\u001b[0m\n",
      "\u001b[32m[01/25 01:54:43 train]\u001b[0m: EPOCH 0: (100 % done) TrainOutput(loss=0.877, acc=0.08, val_loss=0.722, val_acc=0.0)\u001b[0m\n",
      "\u001b[32m[01/25 01:54:45 train]\u001b[0m: eta: 3:28:07 iteration: 10 loss: 1.022 accuracy: 0.000 lrs: (2.9e-07, 2.9e-06)\u001b[0m\n",
      "\u001b[32m[01/25 01:54:55 train]\u001b[0m: eta: 3:28:42 iteration: 15 loss: 0.955 accuracy: 0.067 lrs: (4.4e-07, 4.4e-06)\u001b[0m\n",
      "\u001b[32m[01/25 01:55:04 train]\u001b[0m: EPOCH 1: (100 % done) TrainOutput(loss=0.907, acc=0.08, val_loss=0.76, val_acc=0.0)\u001b[0m\n",
      "\u001b[32m[01/25 01:55:04 train]\u001b[0m: Total training time: 0:00:41\u001b[0m\n",
      "\u001b[32m[01/25 01:55:04 train]\u001b[0m: Overall training speed: 20 iterations in 0:00:36 (1.8401 s / it)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/25 01:55:06 train]\u001b[0m: Start TEST on 856 batches\u001b[0m\n",
      "\u001b[32m[01/25 01:55:06 train]\u001b[0m: Transformations used: [Resize(size=291, interpolation=PIL.Image.BILINEAR), CenterCrop(size=(255, 255)), ToTensor(), Normalize(mean=tensor([0.4298, 0.4962, 0.3130]), std=tensor([0.2142, 0.2191, 0.1954]))]\u001b[0m\n",
      "\u001b[32m[01/25 01:55:06 train]\u001b[0m: Testing done 0/856. ETA=0:06:35\u001b[0m\n",
      "\u001b[32m[01/25 01:55:09 train]\u001b[0m: Test results: (100 % done) TestOutput(test_loss=0.75, test_acc=0.04)\u001b[0m\n",
      "\u001b[32m[01/25 01:55:09 train]\u001b[0m: Total testing time: 0:00:02\u001b[0m\n",
      "\u001b[32m[01/25 01:55:09 train]\u001b[0m: Overall testing speed: 5 iterations in 0:00:02 (0.4672 s / it)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = trainer.test(datamodule=dm, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_mixmethods.ipynb.\n",
      "Converted 02_losses.ipynb.\n",
      "Converted 03_layers.ipynb.\n",
      "Converted 03a_networks.ipynb.\n",
      "Converted 04_optimizers_schedules.ipynb.\n",
      "Converted 05_lightning.data.ipynb.\n",
      "Converted 05a_lightning.core.ipynb.\n",
      "Converted 05b_lightning.callbacks.ipynb.\n",
      "Converted 06_fastai.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
