{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n",
    "from nbdev.export import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning.metrics import Accuracy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src import _logger\n",
    "from src.data import DatasetMapper\n",
    "from src.models.builder import Net\n",
    "from src.optimizers import create_optimizer\n",
    "from src.schedulers import create_scheduler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Task(pl.LightningModule):\n",
    "    \"A general Task for Cassave Leaf Disease Classification\"\n",
    "    \n",
    "    def __init__(self, conf: DictConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.trn_metric = Accuracy()\n",
    "        self.val_metric = Accuracy()\n",
    "        self.tst_metric = Accuracy()\n",
    "        self.save_hyperparameters(conf)\n",
    "        \n",
    "        # instantiate objects\n",
    "        self.model = Net(self.hparams)\n",
    "        self.criterion   = instantiate(self.hparams.loss)\n",
    "        self.mixfunction = instantiate(self.hparams.mixmethod)\n",
    "\n",
    "        _logger.info(f\"LossFunction: {self.criterion}\")\n",
    "        if self.mixfunction is not None:\n",
    "            _logger.info(f\"Training with {self.mixfunction}.\")\n",
    "        \n",
    "        self.lrs= None\n",
    "            \n",
    "    def setup(self, stage: str):\n",
    "        \"setups datasetMapper\"\n",
    "        mapper = DatasetMapper(self.hparams)\n",
    "        mapper.generate_datasets()\n",
    "        \n",
    "        # Loads in the repective datasets\n",
    "        self.train_dset = mapper.get_test_dataset()\n",
    "        self.valid_dset = mapper.get_valid_dataset()\n",
    "        self.test_dset  = mapper.get_test_dataset()\n",
    "        \n",
    "        # Loads in the transformations to be applied after mixmethod\n",
    "        self.final_augs = mapper.get_transforms()\n",
    "\n",
    "    def forward(self, x: Any) -> Any:\n",
    "        \"call the model\"\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch: Any, batch_idx: int) -> Any:\n",
    "        \"The Training Step: This is where the Magic Happens !!!\"\n",
    "        imgs, targs = batch\n",
    "        self.preds, self.labels = None, None\n",
    "        \n",
    "        if self.mixfunction is not None:\n",
    "            if self.current_epoch < self.hparams.training.mix_epochs :\n",
    "                imgs = self.mixfunction(imgs, targs, model=self.model)\n",
    "                logits= self.forward(imgs)\n",
    "                loss= self.mixfunction.lf(logits, loss_func=self.criterion)\n",
    "                acc = self.trn_metric(logits, targs)\n",
    "            else:\n",
    "                logits = self.forward(imgs)\n",
    "                loss   = self.criterion(logits, targs)\n",
    "                acc    = self.trn_metric(logits, targs)\n",
    "        \n",
    "        else:\n",
    "            logits = self.forward(imgs)\n",
    "            loss   = self.criterion(logits, targs)\n",
    "            acc    = self.trn_metric(logits, targs)\n",
    "        \n",
    "        preds  = torch.argmax(logits, 1)\n",
    "        self.labels = list(targs.data.cpu().numpy())\n",
    "        self.preds  = list(preds.data.cpu().numpy())\n",
    "        \n",
    "        result_dict = {\"train/loss\": loss, \"train/acc\": acc}\n",
    "        self.log_dict(result_dict, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: Any, batch_idx: int) -> None:\n",
    "        \"The Validation Step\"\n",
    "        imgs, targs = batch\n",
    "        self.preds, self.labels = None, None\n",
    "        \n",
    "        logits = self.forward(imgs)\n",
    "        loss = self.criterion(logits, targs)\n",
    "        acc = self.val_metric(logits, targs)\n",
    "        \n",
    "        preds  = torch.argmax(logits, 1)\n",
    "        self.labels = list(targs.data.cpu().numpy())\n",
    "        self.preds  = list(preds.data.cpu().numpy())\n",
    "        \n",
    "        result_dict = {\"valid/loss\": loss, \"valid/acc\": acc} \n",
    "        self.log_dict(result_dict)\n",
    "    \n",
    "    def test_step(self, batch: Any, batch_idx: int) -> None:\n",
    "        \"The Test Step\"\n",
    "        imgs, targs = batch\n",
    "        self.preds, self.labels = None, None\n",
    "        \n",
    "        logits = self.forward(imgs)\n",
    "        loss = self.criterion(logits, targs)\n",
    "        acc = self.tst_metric(logits, targs)\n",
    "        \n",
    "        preds  = torch.argmax(logits, 1)\n",
    "        self.labels = list(targs.data.cpu().numpy())\n",
    "        self.preds  = list(preds.data.cpu().numpy())\n",
    "        \n",
    "        result_dict = {\"test/loss\": loss, \"test/acc\": acc} \n",
    "        self.log_dict(result_dict)\n",
    "    \n",
    "    def configure_optimizers(self) -> Tuple[List[torch.optim.Optimizer], List[Dict]]:\n",
    "        \n",
    "        lrs = (self.hparams.training.learning_rate/self.hparams.training.lr_mult, \n",
    "               self.hparams.training.learning_rate)\n",
    "        \n",
    "        lr_tuple = namedtuple(\"LearningRates\", [\"base\", \"head\"])\n",
    "        \n",
    "        self.lrs = lr_tuple(lrs[0], lrs[1])\n",
    "        \n",
    "        epochs  = self.hparams.training.num_epochs\n",
    "        steps   = len(self.train_dataloader())/ self.hparams.training.accumulate_grad_batches\n",
    "        \n",
    "        total_params = self.model.get_param_list()\n",
    "        params = [\n",
    "            {\"params\": total_params[0], \"lr\":lrs[0]}, \n",
    "            {\"params\": total_params[1], \"lr\":lrs[1]},\n",
    "        ]\n",
    "        \n",
    "        optim = create_optimizer(self.hparams.optimizer, params=params)\n",
    "        sched = create_scheduler(self.hparams.scheduler, optim, steps, epochs)\n",
    "        return [optim], [sched]\n",
    "        \n",
    "    def train_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        \"returns a PyTorch DataLoader for Training\"\n",
    "        if self.current_epoch == self.hparams.training.mix_epochs:\n",
    "            if self.mixfunction is not None:\n",
    "                name = self.mixfunction.__class__.__name__\n",
    "                _logger.info(f\"Train [ {self.current_epoch}/{self.trainer.max_epochs}]: Stopping {name} !\")\n",
    "                self.mixfunction.stop()\n",
    "            \n",
    "            self.train_dset.reload_transforms(self.final_augs)\n",
    "            dataloader = torch.utils.data.DataLoader(self.train_dset, **self.hparams.data.dataloader)\n",
    "        else:\n",
    "            dataloader = torch.utils.data.DataLoader(self.train_dset, **self.hparams.data.dataloader)\n",
    "        return dataloader\n",
    "    \n",
    "    def val_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        \"returns a PyTorch DataLoader for Validation\"\n",
    "        return torch.utils.data.DataLoader(self.valid_dset, **self.hparams.data.dataloader)\n",
    "    \n",
    "    def test_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        \"returns a PyTorch DataLoader for Testing\"\n",
    "        return torch.utils.data.DataLoader(self.test_dset, **self.hparams.data.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hydra.experimental import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "overrides = [\n",
    "    \"input.input_size=120\",\n",
    "    \"training.batch_size=5\",\n",
    "    \"augmentations=tfms-v0\",\n",
    "    \"data.dataloader.num_workers=0\",\n",
    "    \"general=default\",\n",
    "    \"trainer=fast-dev-cpu\",\n",
    "    \"optimizer=ranger\",\n",
    "    \"mixmethod=snapmix\",\n",
    "    \"training.mix_epochs=1\",\n",
    "    \"training.batch_size=64\",\n",
    "    \"model=v0\",\n",
    "    \"model.base_model.activation=mish\",\n",
    "    \"model.head.params.act_layer=mish\",\n",
    "    \"training.accumulate_grad_batches=1\",\n",
    "    \"loss=crossentropy\",\n",
    "]\n",
    "\n",
    "with initialize(config_path=os.path.relpath(\"../conf/\")):\n",
    "    cfg = compose(config_name=\"effnet-base\", overrides=overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[32m01/31 20:52:01\u001b[0m \u001b[35msrc.models.builder\u001b[0m]: Configuration for the current model :\n",
      "[\u001b[32m01/31 20:52:01\u001b[0m \u001b[35msrc.models.builder\u001b[0m]:  feature_extractor: tf_efficientnet_b3_ns\n",
      "[\u001b[32m01/31 20:52:01\u001b[0m \u001b[35msrc.models.builder\u001b[0m]:  activation: mish\n",
      "[\u001b[32m01/31 20:52:01\u001b[0m \u001b[35msrc.models.builder\u001b[0m]:  params: {'drop_path_rate': 0.25}\n",
      "[\u001b[32m01/31 20:52:01\u001b[0m \u001b[35msrc.models.builder\u001b[0m]:  head: CnnHeadV0\n",
      "[\u001b[32m01/31 20:52:01\u001b[0m \u001b[35msrc.models.builder\u001b[0m]:  params: {'n_out': 5, 'pool_type': 'avg', 'use_conv': False, 'act_layer': 'mish'}\n",
      "[\u001b[32m01/31 20:52:02\u001b[0m \u001b[35m__main__\u001b[0m]: LossFunction: CrossEntropyLoss()\n",
      "[\u001b[32m01/31 20:52:02\u001b[0m \u001b[35m__main__\u001b[0m]: Training with Snapmix(alpha=5.0, conf_prob=0.5, num_iters=1).\n"
     ]
    }
   ],
   "source": [
    "model = Task(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(reload_dataloaders_every_epoch=True, \n",
    "                  limit_train_batches=2, limit_test_batches=2, \n",
    "                  limit_val_batches=2, weights_summary=None, \n",
    "                  accumulate_grad_batches=1, num_sanity_val_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[32m01/31 20:52:04\u001b[0m \u001b[35msrc.data.dataset_factory\u001b[0m]: Generating Datasets for FOLD :0\n",
      "[\u001b[32m01/31 20:52:05\u001b[0m \u001b[35msrc.data.dataset_factory\u001b[0m]: Train Dataset has 17117, Validation Dataset has 17117 instances.\n",
      "[\u001b[32m01/31 20:52:05\u001b[0m \u001b[35msrc.optimizers\u001b[0m]: Ranger loaded from OPTIM_REGISTERY\n",
      "[\u001b[32m01/31 20:52:05\u001b[0m \u001b[35msrc.schedulers\u001b[0m]: FlatCosScheduler loaded from SCHEDULER_REGISTERY\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d6c1cab45c45329eb12596313bec09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[32m01/31 20:52:48\u001b[0m \u001b[35m__main__\u001b[0m]: Train [ 1/1000]: Stopping Snapmix !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01a_data.datasets.ipynb.\n",
      "Converted 01b_data.datasests_factory.ipynb.\n",
      "Converted 01c_data.mixmethods.ipynb.\n",
      "Converted 02_losses.ipynb.\n",
      "Converted 03a_optimizers.ipynb.\n",
      "Converted 03b_schedulers.ipynb.\n",
      "Converted 04a_models.utils.ipynb.\n",
      "Converted 04b_models.layers.ipynb.\n",
      "Converted 04c_models.classifiers.ipynb.\n",
      "Converted 04d_models.builder.ipynb.\n",
      "Converted 04e_models.task.ipynb.\n",
      "Converted 05_callbacks.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
