{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from fvcore.common import registry\n",
    "from omegaconf import DictConfig\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "from src import _logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Optimizer Registery -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "OPTIM_REGISTERY = registry.Registry(\"Optimizers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# register all default optimizers\n",
    "OPTIM_REGISTERY.register(torch.optim.SGD)\n",
    "OPTIM_REGISTERY.register(torch.optim.RMSprop)\n",
    "OPTIM_REGISTERY.register(torch.optim.Adam)\n",
    "OPTIM_REGISTERY.register(torch.optim.AdamW)\n",
    "OPTIM_REGISTERY.register(torch.optim.Adamax)\n",
    "OPTIM_REGISTERY.register(torch.optim.Adadelta)\n",
    "OPTIM_REGISTERY.register(torch.optim.Adagrad)\n",
    "OPTIM_REGISTERY.register(torch.optim.SparseAdam)\n",
    "OPTIM_REGISTERY.register(torch.optim.ASGD)\n",
    "OPTIM_REGISTERY.register(torch.optim.LBFGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranger Optimizer - \n",
    "> from https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger2020.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def centralized_gradient(x, use_gc=True, gc_conv_only=False):\n",
    "    \"\"\"credit - https://github.com/Yonghongwei/Gradient-Centralization \"\"\"\n",
    "    if use_gc:\n",
    "        if gc_conv_only:\n",
    "            if len(list(x.size())) > 3:\n",
    "                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n",
    "        else:\n",
    "            if len(list(x.size())) > 1:\n",
    "                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@OPTIM_REGISTERY.register()\n",
    "class Ranger(Optimizer):\n",
    "    \"Convinence class for RAdam + LookAhead\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        alpha=0.5,\n",
    "        k=6,\n",
    "        N_sma_threshhold=5,\n",
    "        betas=(0.95, 0.999),\n",
    "        eps=1e-5,\n",
    "        weight_decay=0,\n",
    "        use_gc=True,\n",
    "        gc_conv_only=False,\n",
    "        gc_loc=True,\n",
    "    ):\n",
    "\n",
    "        # parameter checks\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f\"Invalid slow update rate: {alpha}\")\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f\"Invalid lookahead steps: {k}\")\n",
    "        if not lr > 0:\n",
    "            raise ValueError(f\"Invalid Learning Rate: {lr}\")\n",
    "        if not eps > 0:\n",
    "            raise ValueError(f\"Invalid eps: {eps}\")\n",
    "\n",
    "        # parameter comments:\n",
    "        # beta1 (momentum) of .95 seems to work better than .90...\n",
    "        # N_sma_threshold of 5 seems better in testing than 4.\n",
    "        # In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n",
    "\n",
    "        # prep defaults and init torch.optim base\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            alpha=alpha,\n",
    "            k=k,\n",
    "            step_counter=0,\n",
    "            betas=betas,\n",
    "            N_sma_threshhold=N_sma_threshhold,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "        # adjustable threshold\n",
    "        self.N_sma_threshhold = N_sma_threshhold\n",
    "\n",
    "        # look ahead params\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "        # radam buffer for state\n",
    "        self.radam_buffer = [[None, None, None] for ind in range(10)]\n",
    "\n",
    "        # gc on or off\n",
    "        self.gc_loc = gc_loc\n",
    "        self.use_gc = use_gc\n",
    "        self.gc_conv_only = gc_conv_only\n",
    "        # level of gradient centralization\n",
    "        # self.gc_gradient_threshold = 3 if gc_conv_only else 1\n",
    "        \n",
    "        if self.use_gc and self.gc_conv_only == False:\n",
    "            _logger.info(f\"GC applied to both conv and fc layers\")\n",
    "\n",
    "        elif self.use_gc and self.gc_conv_only == True:\n",
    "            _logger.info(f\"GC applied to conv layers only\")\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        _logger.info(\"set state called\")\n",
    "        super(Ranger, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        # note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.\n",
    "        # Uncomment if you need to use the actual closure...\n",
    "\n",
    "        # if closure is not None:\n",
    "        # loss = closure()\n",
    "\n",
    "        # Evaluate averages and grad, update param tensors\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        \"Ranger optimizer does not support sparse gradients\"\n",
    "                    )\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]  # get state dict for this param\n",
    "\n",
    "                if (\n",
    "                    len(state) == 0\n",
    "                ):  # if first time to run...init dictionary with our desired entries\n",
    "                    # if self.first_run_check==0:\n",
    "                    # self.first_run_check=1\n",
    "                    # print(\"Initializing slow buffer...should not see this at load from saved model!\")\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n",
    "\n",
    "                    # look ahead weight storage now in state dict\n",
    "                    state[\"slow_buffer\"] = torch.empty_like(p.data)\n",
    "                    state[\"slow_buffer\"].copy_(p.data)\n",
    "\n",
    "                else:\n",
    "                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n",
    "                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n",
    "\n",
    "                # begin computations\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                # GC operation for Conv layers and FC layers\n",
    "                # if grad.dim() > self.gc_gradient_threshold:\n",
    "                #    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n",
    "                if self.gc_loc:\n",
    "                    grad = centralized_gradient(\n",
    "                        grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only\n",
    "                    )\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                # compute variance mov avg\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                # compute mean moving avg\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                buffered = self.radam_buffer[int(state[\"step\"] % 10)]\n",
    "\n",
    "                if state[\"step\"] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state[\"step\"]\n",
    "                    beta2_t = beta2 ** state[\"step\"]\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "                    if N_sma > self.N_sma_threshhold:\n",
    "                        step_size = math.sqrt(\n",
    "                            (1 - beta2_t)\n",
    "                            * (N_sma - 4)\n",
    "                            / (N_sma_max - 4)\n",
    "                            * (N_sma - 2)\n",
    "                            / N_sma\n",
    "                            * N_sma_max\n",
    "                            / (N_sma_max - 2)\n",
    "                        ) / (1 - beta1 ** state[\"step\"])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state[\"step\"])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                # if group['weight_decay'] != 0:\n",
    "                #    p_data_fp32.add_(-group['weight_decay']\n",
    "                #                     * group['lr'], p_data_fp32)\n",
    "\n",
    "                # apply lr\n",
    "                if N_sma > self.N_sma_threshhold:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "                    G_grad = exp_avg / denom\n",
    "                else:\n",
    "                    G_grad = exp_avg\n",
    "\n",
    "                if group[\"weight_decay\"] != 0:\n",
    "                    G_grad.add_(p_data_fp32, alpha=group[\"weight_decay\"])\n",
    "                # GC operation\n",
    "                if self.gc_loc == False:\n",
    "                    G_grad = centralized_gradient(\n",
    "                        G_grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only\n",
    "                    )\n",
    "\n",
    "                p_data_fp32.add_(G_grad, alpha=-step_size * group[\"lr\"])\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "                # integrated look ahead...\n",
    "                # we do it at the param level instead of group level\n",
    "                if state[\"step\"] % group[\"k\"] == 0:\n",
    "                    # get access to slow param tensor\n",
    "                    slow_p = state[\"slow_buffer\"]\n",
    "                    # (fast weights - slow weights) * alpha\n",
    "                    slow_p.add_(p.data - slow_p, alpha=self.alpha)\n",
    "                    # copy interpolated weights to RAdam param tensor\n",
    "                    p.data.copy_(slow_p)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registry of Optimizers:\n",
      "╒════════════╤══════════════════════════════════════════════╕\n",
      "│ Names      │ Objects                                      │\n",
      "╞════════════╪══════════════════════════════════════════════╡\n",
      "│ SGD        │ <class 'torch.optim.sgd.SGD'>                │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ RMSprop    │ <class 'torch.optim.rmsprop.RMSprop'>        │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ Adam       │ <class 'torch.optim.adam.Adam'>              │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ AdamW      │ <class 'torch.optim.adamw.AdamW'>            │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ Adamax     │ <class 'torch.optim.adamax.Adamax'>          │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ Adadelta   │ <class 'torch.optim.adadelta.Adadelta'>      │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ Adagrad    │ <class 'torch.optim.adagrad.Adagrad'>        │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ SparseAdam │ <class 'torch.optim.sparse_adam.SparseAdam'> │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ ASGD       │ <class 'torch.optim.asgd.ASGD'>              │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ LBFGS      │ <class 'torch.optim.lbfgs.LBFGS'>            │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ Ranger     │ <class '__main__.Ranger'>                    │\n",
      "╘════════════╧══════════════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "print(OPTIM_REGISTERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_optimizer(cfg: DictConfig, params):\n",
    "    \"loaded a optimizer in OPTIM_REGISTERY from `cfg`\"\n",
    "    optimizer = OPTIM_REGISTERY.get(cfg.name)(params=params, **cfg.params)\n",
    "    _logger.info(f\"{cfg.name} loaded from OPTIM_REGISTERY\")\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "model  = torch.nn.Linear(24, 2, bias=True)\n",
    "params = model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD loaded from OPTIM_REGISTERY\n"
     ]
    }
   ],
   "source": [
    "# cfg = OmegaConf.load(\"../conf/optimizer/adam.yaml\")\n",
    "# cfg = OmegaConf.load(\"../conf/optimizer/ranger.yaml\")\n",
    "# cfg = OmegaConf.load(\"../conf/optimizer/adamw.yaml\")\n",
    "cfg = OmegaConf.load(\"../conf/optimizer/sgd.yaml\")\n",
    "\n",
    "optimizer = create_optimizer(cfg, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_mixmethods.ipynb.\n",
      "Converted 02_losses.ipynb.\n",
      "Converted 03_layers.ipynb.\n",
      "Converted 03a_networks.ipynb.\n",
      "Converted 04_optimizers.ipynb.\n",
      "Converted 04a_schedulers.ipynb.\n",
      "Converted 05_lightning.data.ipynb.\n",
      "Converted 05a_lightning.core.ipynb.\n",
      "Converted 05b_lightning.callbacks.ipynb.\n",
      "Converted 06_fastai.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
