{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp lightning.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# export\\nimport os\\nimport sys\\nimport time\\nimport datetime\\nimport logging\\nfrom collections import namedtuple\\nfrom tqdm.auto import tqdm\\n\\nimport wandb\\n\\nimport torch\\nimport pytorch_lightning as pl\\nfrom pytorch_lightning import _logger as log\\nfrom pytorch_lightning.core.memory import ModelSummary\\n\\nfrom src.all import *\";\n",
       "                var nbb_formatted_code = \"# export\\nimport os\\nimport sys\\nimport time\\nimport datetime\\nimport logging\\nfrom collections import namedtuple\\nfrom tqdm.auto import tqdm\\n\\nimport wandb\\n\\nimport torch\\nimport pytorch_lightning as pl\\nfrom pytorch_lightning import _logger as log\\nfrom pytorch_lightning.core.memory import ModelSummary\\n\\nfrom src.all import *\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "from collections import namedtuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import _logger as log\n",
    "from pytorch_lightning.core.memory import ModelSummary\n",
    "\n",
    "from src.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"import colorlog\\n\\nhandler = colorlog.StreamHandler()\\n\\nfmt = \\\"[%(cyan)s%(asctime)s%(reset)s][%(blue)s%(name)s%(reset)s][%(log_color)s%(levelname)s%(reset)s] - %(message)s\\\"\\ncolors = dict(\\n    DEBUG=\\\"purple\\\", INFO=\\\"green\\\", WARNING=\\\"yellow\\\", ERROR=\\\"red\\\", CRITICAL=\\\"red\\\"\\n)\\nformatter = colorlog.ColoredFormatter(fmt=fmt, log_colors=colors)\\nhandler.setFormatter(formatter)\\n\\nlogging.basicConfig(format=fmt, level=logging.INFO, handlers=[handler])\";\n",
       "                var nbb_formatted_code = \"import colorlog\\n\\nhandler = colorlog.StreamHandler()\\n\\nfmt = \\\"[%(cyan)s%(asctime)s%(reset)s][%(blue)s%(name)s%(reset)s][%(log_color)s%(levelname)s%(reset)s] - %(message)s\\\"\\ncolors = dict(\\n    DEBUG=\\\"purple\\\", INFO=\\\"green\\\", WARNING=\\\"yellow\\\", ERROR=\\\"red\\\", CRITICAL=\\\"red\\\"\\n)\\nformatter = colorlog.ColoredFormatter(fmt=fmt, log_colors=colors)\\nhandler.setFormatter(formatter)\\n\\nlogging.basicConfig(format=fmt, level=logging.INFO, handlers=[handler])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import colorlog\n",
    "\n",
    "handler = colorlog.StreamHandler()\n",
    "\n",
    "fmt = \"[%(cyan)s%(asctime)s%(reset)s][%(blue)s%(name)s%(reset)s][%(log_color)s%(levelname)s%(reset)s] - %(message)s\"\n",
    "colors = dict(\n",
    "    DEBUG=\"purple\", INFO=\"green\", WARNING=\"yellow\", ERROR=\"red\", CRITICAL=\"red\"\n",
    ")\n",
    "formatter = colorlog.ColoredFormatter(fmt=fmt, log_colors=colors)\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "logging.basicConfig(format=fmt, level=logging.INFO, handlers=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# export\\nclass WandbImageClassificationCallback(pl.Callback):\\n    \\\"\\\"\\\" Custom callback to add some extra functionalites to the wandb logger \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        num_batches: int = 16,\\n        log_train_batch: bool = False,\\n        log_preds: bool = False,\\n        log_conf_mat: bool = True,\\n    ):\\n\\n        # class names for the confusion matrix\\n        self.class_names = list(conf_mat_idx2lbl.values())\\n\\n        # counter to log training batch images\\n        self.num_bs = num_batches\\n        self.curr_epoch = 0\\n\\n        self.log_train_batch = log_train_batch\\n        self.log_preds = log_preds\\n        self.log_conf_mat = log_conf_mat\\n\\n        self.val_imgs, self.val_labels = None, None\\n\\n    def on_train_start(self, trainer, pl_module, *args, **kwargs):\\n        try:\\n            # log model to the wandb experiment\\n            wandb.watch(models=pl_module.model, criterion=pl_module.loss_func)\\n        except:\\n            pass\\n\\n    def on_train_epoch_end(self, trainer, pl_module, *args, **kwargs):\\n        if self.log_train_batch:\\n            if pl_module.one_batch is None:\\n                pass\\n\\n            else:\\n                one_batch = pl_module.one_batch[: self.num_bs]\\n                train_ims = one_batch.data.to(\\\"cpu\\\")\\n                trainer.logger.experiment.log(\\n                    {\\\"train_batch\\\": [wandb.Image(x) for x in train_ims]}, commit=False\\n                )\\n\\n    def on_validation_epoch_end(self, trainer, pl_module, *args, **kwargs):\\n        if self.log_preds:\\n            if self.val_imgs is None and self.val_labels is None:\\n                self.val_imgs, self.val_labels = next(iter(pl_module.val_dataloader()))\\n                self.val_imgs, self.val_labels = (\\n                    self.val_imgs[: self.num_bs],\\n                    self.val_labels[: self.num_bs],\\n                )\\n                self.val_imgs = self.val_imgs.to(device=pl_module.device)\\n\\n            logits = pl_module(self.val_imgs)\\n            preds = torch.argmax(logits, 1)\\n            preds = preds.data.cpu()\\n\\n            ims = [\\n                wandb.Image(x, caption=f\\\"Pred:{pred}, Label:{y}\\\")\\n                for x, pred, y in zip(self.val_imgs, preds, self.val_labels)\\n            ]\\n            log_dict = {\\\"predictions\\\": ims}\\n            wandb.log(ims, commit=False)\\n\\n    def on_epoch_start(self, trainer, pl_module, *args, **kwargs):\\n        pl_module.val_labels_list = []\\n        pl_module.val_preds_list = []\\n\\n    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):\\n        if self.log_conf_mat:\\n            val_preds = torch.tensor(pl_module.val_preds_list).data.cpu().numpy()\\n            val_labels = torch.tensor(pl_module.val_labels_list).data.cpu().numpy()\\n            log_dict = {\\n                \\\"conf_mat\\\": wandb.plot.confusion_matrix(\\n                    val_preds, val_labels, self.class_names\\n                )\\n            }\\n            wandb.log(log_dict, commit=False)\";\n",
       "                var nbb_formatted_code = \"# export\\nclass WandbImageClassificationCallback(pl.Callback):\\n    \\\"\\\"\\\" Custom callback to add some extra functionalites to the wandb logger \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        num_batches: int = 16,\\n        log_train_batch: bool = False,\\n        log_preds: bool = False,\\n        log_conf_mat: bool = True,\\n    ):\\n\\n        # class names for the confusion matrix\\n        self.class_names = list(conf_mat_idx2lbl.values())\\n\\n        # counter to log training batch images\\n        self.num_bs = num_batches\\n        self.curr_epoch = 0\\n\\n        self.log_train_batch = log_train_batch\\n        self.log_preds = log_preds\\n        self.log_conf_mat = log_conf_mat\\n\\n        self.val_imgs, self.val_labels = None, None\\n\\n    def on_train_start(self, trainer, pl_module, *args, **kwargs):\\n        try:\\n            # log model to the wandb experiment\\n            wandb.watch(models=pl_module.model, criterion=pl_module.loss_func)\\n        except:\\n            pass\\n\\n    def on_train_epoch_end(self, trainer, pl_module, *args, **kwargs):\\n        if self.log_train_batch:\\n            if pl_module.one_batch is None:\\n                pass\\n\\n            else:\\n                one_batch = pl_module.one_batch[: self.num_bs]\\n                train_ims = one_batch.data.to(\\\"cpu\\\")\\n                trainer.logger.experiment.log(\\n                    {\\\"train_batch\\\": [wandb.Image(x) for x in train_ims]}, commit=False\\n                )\\n\\n    def on_validation_epoch_end(self, trainer, pl_module, *args, **kwargs):\\n        if self.log_preds:\\n            if self.val_imgs is None and self.val_labels is None:\\n                self.val_imgs, self.val_labels = next(iter(pl_module.val_dataloader()))\\n                self.val_imgs, self.val_labels = (\\n                    self.val_imgs[: self.num_bs],\\n                    self.val_labels[: self.num_bs],\\n                )\\n                self.val_imgs = self.val_imgs.to(device=pl_module.device)\\n\\n            logits = pl_module(self.val_imgs)\\n            preds = torch.argmax(logits, 1)\\n            preds = preds.data.cpu()\\n\\n            ims = [\\n                wandb.Image(x, caption=f\\\"Pred:{pred}, Label:{y}\\\")\\n                for x, pred, y in zip(self.val_imgs, preds, self.val_labels)\\n            ]\\n            log_dict = {\\\"predictions\\\": ims}\\n            wandb.log(ims, commit=False)\\n\\n    def on_epoch_start(self, trainer, pl_module, *args, **kwargs):\\n        pl_module.val_labels_list = []\\n        pl_module.val_preds_list = []\\n\\n    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):\\n        if self.log_conf_mat:\\n            val_preds = torch.tensor(pl_module.val_preds_list).data.cpu().numpy()\\n            val_labels = torch.tensor(pl_module.val_labels_list).data.cpu().numpy()\\n            log_dict = {\\n                \\\"conf_mat\\\": wandb.plot.confusion_matrix(\\n                    val_preds, val_labels, self.class_names\\n                )\\n            }\\n            wandb.log(log_dict, commit=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class WandbImageClassificationCallback(pl.Callback):\n",
    "    \"\"\" Custom callback to add some extra functionalites to the wandb logger \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_batches: int = 16,\n",
    "        log_train_batch: bool = False,\n",
    "        log_preds: bool = False,\n",
    "        log_conf_mat: bool = True,\n",
    "    ):\n",
    "\n",
    "        # class names for the confusion matrix\n",
    "        self.class_names = list(conf_mat_idx2lbl.values())\n",
    "\n",
    "        # counter to log training batch images\n",
    "        self.num_bs = num_batches\n",
    "        self.curr_epoch = 0\n",
    "\n",
    "        self.log_train_batch = log_train_batch\n",
    "        self.log_preds = log_preds\n",
    "        self.log_conf_mat = log_conf_mat\n",
    "\n",
    "        self.val_imgs, self.val_labels = None, None\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        try:\n",
    "            # log model to the wandb experiment\n",
    "            wandb.watch(models=pl_module.model, criterion=pl_module.loss_func)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_train_batch:\n",
    "            if pl_module.one_batch is None:\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                one_batch = pl_module.one_batch[: self.num_bs]\n",
    "                train_ims = one_batch.data.to(\"cpu\")\n",
    "                trainer.logger.experiment.log(\n",
    "                    {\"train_batch\": [wandb.Image(x) for x in train_ims]}, commit=False\n",
    "                )\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_preds:\n",
    "            if self.val_imgs is None and self.val_labels is None:\n",
    "                self.val_imgs, self.val_labels = next(iter(pl_module.val_dataloader()))\n",
    "                self.val_imgs, self.val_labels = (\n",
    "                    self.val_imgs[: self.num_bs],\n",
    "                    self.val_labels[: self.num_bs],\n",
    "                )\n",
    "                self.val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "\n",
    "            logits = pl_module(self.val_imgs)\n",
    "            preds = torch.argmax(logits, 1)\n",
    "            preds = preds.data.cpu()\n",
    "\n",
    "            ims = [\n",
    "                wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\")\n",
    "                for x, pred, y in zip(self.val_imgs, preds, self.val_labels)\n",
    "            ]\n",
    "            log_dict = {\"predictions\": ims}\n",
    "            wandb.log(ims, commit=False)\n",
    "\n",
    "    def on_epoch_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        pl_module.val_labels_list = []\n",
    "        pl_module.val_preds_list = []\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_conf_mat:\n",
    "            val_preds = torch.tensor(pl_module.val_preds_list).data.cpu().numpy()\n",
    "            val_labels = torch.tensor(pl_module.val_labels_list).data.cpu().numpy()\n",
    "            log_dict = {\n",
    "                \"conf_mat\": wandb.plot.confusion_matrix(\n",
    "                    val_preds, val_labels, self.class_names\n",
    "                )\n",
    "            }\n",
    "            wandb.log(log_dict, commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# export\\nclass DisableValidationBar(pl.callbacks.ProgressBar):\\n    \\\"Custom Progressbar callback for Lightning Training which disables the validation bar\\\"\\n\\n    def init_sanity_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for the validation sanity run. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Validation sanity check\\\",\\n            position=(2 * self.process_position),\\n            disable=self.is_disabled,\\n            dynamic_ncols=True,\\n        )\\n\\n        return bar\\n\\n    def init_train_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for training. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Training\\\",\\n            initial=self.train_batch_idx,\\n            position=(2 * self.process_position),\\n            disable=self.is_disabled,\\n            dynamic_ncols=True,\\n        )\\n\\n        return bar\\n\\n    def init_validation_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for validation. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Validating\\\",\\n            position=(2 * self.process_position + 1),\\n            disable=True,\\n            dynamic_ncols=False,\\n        )\\n\\n        return bar\\n\\n    def init_test_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for testing. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Testing\\\",\\n            position=(2 * self.process_position),\\n            disable=self.is_disabled,\\n            dynamic_ncols=True,\\n        )\\n\\n        return bar\";\n",
       "                var nbb_formatted_code = \"# export\\nclass DisableValidationBar(pl.callbacks.ProgressBar):\\n    \\\"Custom Progressbar callback for Lightning Training which disables the validation bar\\\"\\n\\n    def init_sanity_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for the validation sanity run. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Validation sanity check\\\",\\n            position=(2 * self.process_position),\\n            disable=self.is_disabled,\\n            dynamic_ncols=True,\\n        )\\n\\n        return bar\\n\\n    def init_train_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for training. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Training\\\",\\n            initial=self.train_batch_idx,\\n            position=(2 * self.process_position),\\n            disable=self.is_disabled,\\n            dynamic_ncols=True,\\n        )\\n\\n        return bar\\n\\n    def init_validation_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for validation. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Validating\\\",\\n            position=(2 * self.process_position + 1),\\n            disable=True,\\n            dynamic_ncols=False,\\n        )\\n\\n        return bar\\n\\n    def init_test_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for testing. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Testing\\\",\\n            position=(2 * self.process_position),\\n            disable=self.is_disabled,\\n            dynamic_ncols=True,\\n        )\\n\\n        return bar\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class DisableValidationBar(pl.callbacks.ProgressBar):\n",
    "    \"Custom Progressbar callback for Lightning Training which disables the validation bar\"\n",
    "\n",
    "    def init_sanity_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for the validation sanity run. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Validation sanity check\",\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_train_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Training\",\n",
    "            initial=self.train_batch_idx,\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_validation_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for validation. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Validating\",\n",
    "            position=(2 * self.process_position + 1),\n",
    "            disable=True,\n",
    "            dynamic_ncols=False,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_test_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for testing. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Testing\",\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# export\\nclass PrintLogsCallback(pl.Callback):\\n    \\\"Logs Training logs to console after every epoch\\\"\\n    TrainResult = namedtuple(\\\"TrainResult\\\", [\\\"loss\\\", \\\"acc\\\", \\\"valid_loss\\\", \\\"valid_acc\\\"])\\n    TestResult  = namedtuple(\\\"TestResult\\\", [\\\"test_loss\\\", \\\"test_acc\\\"])\\n    \\n    logger = logging.getLogger(\\\"src.logs\\\")\\n#     logger.setLevel(logging.INFO)\\n#     logger.addHandler(logging.StreamHandler())\\n\\n    def on_epoch_end(self, trainer, pl_module):\\n        metrics = trainer.callback_metrics\\n        train_loss = metrics[\\\"train/loss_epoch\\\"]\\n        train_acc = metrics[\\\"train/acc_epoch\\\"]\\n        valid_loss = metrics[\\\"valid/loss\\\"]\\n        valid_acc = metrics[\\\"valid/acc\\\"]\\n        trn_res = self.TrainResult(\\n            round(train_loss.data.cpu().numpy().item(), 3),\\n            round(train_acc.data.cpu().numpy().item(), 3),\\n            round(valid_loss.data.cpu().numpy().item(), 3),\\n            round(valid_acc.data.cpu().numpy().item(), 3),\\n        )\\n\\n        curr_epoch = int(trainer.current_epoch)\\n        self.logger.info(f\\\"[{curr_epoch}]: (100.00% done), {trn_res}\\\")\\n\\n    def on_test_epoch_end(self, trainer, pl_module, *args, **kwargs):\\n        metrics = trainer.callback_metrics\\n        test_loss = metrics[\\\"test/loss\\\"]\\n        test_acc = metrics[\\\"test/acc\\\"]\\n        self.logger.info(\\n            f\\\"{self.TestResult(round(test_loss.data.cpu().numpy().item(), 2), round(test_acc.data.cpu().numpy().item(), 2))}\\\"\\n        )\";\n",
       "                var nbb_formatted_code = \"# export\\nclass PrintLogsCallback(pl.Callback):\\n    \\\"Logs Training logs to console after every epoch\\\"\\n    TrainResult = namedtuple(\\\"TrainResult\\\", [\\\"loss\\\", \\\"acc\\\", \\\"valid_loss\\\", \\\"valid_acc\\\"])\\n    TestResult = namedtuple(\\\"TestResult\\\", [\\\"test_loss\\\", \\\"test_acc\\\"])\\n\\n    logger = logging.getLogger(\\\"src.logs\\\")\\n    #     logger.setLevel(logging.INFO)\\n    #     logger.addHandler(logging.StreamHandler())\\n\\n    def on_epoch_end(self, trainer, pl_module):\\n        metrics = trainer.callback_metrics\\n        train_loss = metrics[\\\"train/loss_epoch\\\"]\\n        train_acc = metrics[\\\"train/acc_epoch\\\"]\\n        valid_loss = metrics[\\\"valid/loss\\\"]\\n        valid_acc = metrics[\\\"valid/acc\\\"]\\n        trn_res = self.TrainResult(\\n            round(train_loss.data.cpu().numpy().item(), 3),\\n            round(train_acc.data.cpu().numpy().item(), 3),\\n            round(valid_loss.data.cpu().numpy().item(), 3),\\n            round(valid_acc.data.cpu().numpy().item(), 3),\\n        )\\n\\n        curr_epoch = int(trainer.current_epoch)\\n        self.logger.info(f\\\"[{curr_epoch}]: (100.00% done), {trn_res}\\\")\\n\\n    def on_test_epoch_end(self, trainer, pl_module, *args, **kwargs):\\n        metrics = trainer.callback_metrics\\n        test_loss = metrics[\\\"test/loss\\\"]\\n        test_acc = metrics[\\\"test/acc\\\"]\\n        self.logger.info(\\n            f\\\"{self.TestResult(round(test_loss.data.cpu().numpy().item(), 2), round(test_acc.data.cpu().numpy().item(), 2))}\\\"\\n        )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class PrintLogsCallback(pl.Callback):\n",
    "    \"Logs Training logs to console after every epoch\"\n",
    "    TrainResult = namedtuple(\"TrainResult\", [\"loss\", \"acc\", \"valid_loss\", \"valid_acc\"])\n",
    "    TestResult = namedtuple(\"TestResult\", [\"test_loss\", \"test_acc\"])\n",
    "\n",
    "    logger = logging.getLogger(\"src.logs\")\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        train_loss = metrics[\"train/loss_epoch\"]\n",
    "        train_acc = metrics[\"train/acc_epoch\"]\n",
    "        valid_loss = metrics[\"valid/loss\"]\n",
    "        valid_acc = metrics[\"valid/acc\"]\n",
    "        trn_res = self.TrainResult(\n",
    "            round(train_loss.data.cpu().numpy().item(), 3),\n",
    "            round(train_acc.data.cpu().numpy().item(), 3),\n",
    "            round(valid_loss.data.cpu().numpy().item(), 3),\n",
    "            round(valid_acc.data.cpu().numpy().item(), 3),\n",
    "        )\n",
    "\n",
    "        curr_epoch = int(trainer.current_epoch)\n",
    "        self.logger.info(f\"[{curr_epoch}]: (100.00% done), {trn_res}\")\n",
    "\n",
    "    def on_test_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        metrics = trainer.callback_metrics\n",
    "        test_loss = metrics[\"test/loss\"]\n",
    "        test_acc = metrics[\"test/acc\"]\n",
    "        self.logger.info(\n",
    "            f\"{self.TestResult(round(test_loss.data.cpu().numpy().item(), 2), round(test_acc.data.cpu().numpy().item(), 2))}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"import albumentations as A\\nfrom albumentations.pytorch import ToTensorV2\\nimport timm\\nfrom torch import nn\\nfrom src.networks import *\\nfrom omegaconf import OmegaConf\";\n",
       "                var nbb_formatted_code = \"import albumentations as A\\nfrom albumentations.pytorch import ToTensorV2\\nimport timm\\nfrom torch import nn\\nfrom src.networks import *\\nfrom omegaconf import OmegaConf\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "from torch import nn\n",
    "from src.networks import *\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2021-01-22 17:25:07,244\u001b[0m][\u001b[34mLitModel\u001b[0m][\u001b[32mINFO\u001b[0m] - Loss Function : LabelSmoothingCrossEntropy()\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"train_augs = A.Compose(\\n    [\\n        A.RandomResizedCrop(224, 224, p=1.0),\\n        A.RandomBrightness(limit=0.1),\\n        A.HueSaturationValue(20, 20, 20),\\n        A.HorizontalFlip(),\\n        A.Normalize(p=1.0),\\n        ToTensorV2(p=1.0),\\n    ]\\n)\\n\\nvalid_augs = A.Compose(\\n    [A.Resize(224, 224, p=1.0), A.Normalize(p=1.0), ToTensorV2(p=1.0)]\\n)\\n\\ncsv = \\\"../../leaf-disease-classification-kaggle/data/stratified-data-5folds.csv\\\"\\nims = \\\"../../Datasets/cassava/train_images/\\\"\\ndm = CassavaLightningDataModule(\\n    csv,\\n    ims,\\n    curr_fold=0,\\n    train_augs=train_augs,\\n    valid_augs=valid_augs,\\n    bs=8,\\n    num_workers=0,\\n)\\n\\n\\nmodel_hparams = dict(\\n    mixmethod=None,\\n    loss=dict(_target_=\\\"src.losses.LabelSmoothingCrossEntropy\\\", eps=0.1),\\n    learning_rate=1e-03,\\n    lr_mult=100,\\n    optimizer=dict(_target_=\\\"torch.optim.Adam\\\"),\\n    scheduler=dict(\\n        function=dict(_target_=\\\"src.opts.FlatCos\\\", num_epochs=10, pct_start=0.7),\\n        metric_to_track=None,\\n        scheduler_interval=\\\"step\\\",\\n    ),\\n)\\n\\ncfg = OmegaConf.create(model_hparams)\\n\\n\\nencoder = timm.create_model(\\\"resnet18\\\", pretrained=False)\\nmodel = TransferLearningModel(encoder, cut=-2, c=5, act=nn.ReLU(inplace=True))\\nmodel = LightningCassava(model=model, conf=cfg)\";\n",
       "                var nbb_formatted_code = \"train_augs = A.Compose(\\n    [\\n        A.RandomResizedCrop(224, 224, p=1.0),\\n        A.RandomBrightness(limit=0.1),\\n        A.HueSaturationValue(20, 20, 20),\\n        A.HorizontalFlip(),\\n        A.Normalize(p=1.0),\\n        ToTensorV2(p=1.0),\\n    ]\\n)\\n\\nvalid_augs = A.Compose(\\n    [A.Resize(224, 224, p=1.0), A.Normalize(p=1.0), ToTensorV2(p=1.0)]\\n)\\n\\ncsv = \\\"../../leaf-disease-classification-kaggle/data/stratified-data-5folds.csv\\\"\\nims = \\\"../../Datasets/cassava/train_images/\\\"\\ndm = CassavaLightningDataModule(\\n    csv,\\n    ims,\\n    curr_fold=0,\\n    train_augs=train_augs,\\n    valid_augs=valid_augs,\\n    bs=8,\\n    num_workers=0,\\n)\\n\\n\\nmodel_hparams = dict(\\n    mixmethod=None,\\n    loss=dict(_target_=\\\"src.losses.LabelSmoothingCrossEntropy\\\", eps=0.1),\\n    learning_rate=1e-03,\\n    lr_mult=100,\\n    optimizer=dict(_target_=\\\"torch.optim.Adam\\\"),\\n    scheduler=dict(\\n        function=dict(_target_=\\\"src.opts.FlatCos\\\", num_epochs=10, pct_start=0.7),\\n        metric_to_track=None,\\n        scheduler_interval=\\\"step\\\",\\n    ),\\n)\\n\\ncfg = OmegaConf.create(model_hparams)\\n\\n\\nencoder = timm.create_model(\\\"resnet18\\\", pretrained=False)\\nmodel = TransferLearningModel(encoder, cut=-2, c=5, act=nn.ReLU(inplace=True))\\nmodel = LightningCassava(model=model, conf=cfg)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_augs = A.Compose(\n",
    "    [\n",
    "        A.RandomResizedCrop(224, 224, p=1.0),\n",
    "        A.RandomBrightness(limit=0.1),\n",
    "        A.HueSaturationValue(20, 20, 20),\n",
    "        A.HorizontalFlip(),\n",
    "        A.Normalize(p=1.0),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ]\n",
    ")\n",
    "\n",
    "valid_augs = A.Compose(\n",
    "    [A.Resize(224, 224, p=1.0), A.Normalize(p=1.0), ToTensorV2(p=1.0)]\n",
    ")\n",
    "\n",
    "csv = \"../../leaf-disease-classification-kaggle/data/stratified-data-5folds.csv\"\n",
    "ims = \"../../Datasets/cassava/train_images/\"\n",
    "dm = CassavaLightningDataModule(\n",
    "    csv,\n",
    "    ims,\n",
    "    curr_fold=0,\n",
    "    train_augs=train_augs,\n",
    "    valid_augs=valid_augs,\n",
    "    bs=8,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "\n",
    "model_hparams = dict(\n",
    "    mixmethod=None,\n",
    "    loss=dict(_target_=\"src.losses.LabelSmoothingCrossEntropy\", eps=0.1),\n",
    "    learning_rate=1e-03,\n",
    "    lr_mult=100,\n",
    "    optimizer=dict(_target_=\"torch.optim.Adam\"),\n",
    "    scheduler=dict(\n",
    "        function=dict(_target_=\"src.opts.FlatCos\", num_epochs=10, pct_start=0.7),\n",
    "        metric_to_track=None,\n",
    "        scheduler_interval=\"step\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "cfg = OmegaConf.create(model_hparams)\n",
    "\n",
    "\n",
    "encoder = timm.create_model(\"resnet18\", pretrained=False)\n",
    "model = TransferLearningModel(encoder, cut=-2, c=5, act=nn.ReLU(inplace=True))\n",
    "model = LightningCassava(model=model, conf=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "[\u001b[36m2021-01-22 17:25:21,511\u001b[0m][\u001b[34mlightning\u001b[0m][\u001b[32mINFO\u001b[0m] - GPU available: False, used: False\u001b[0m\n",
      "TPU available: False, using: 0 TPU cores\n",
      "[\u001b[36m2021-01-22 17:25:21,512\u001b[0m][\u001b[34mlightning\u001b[0m][\u001b[32mINFO\u001b[0m] - TPU available: False, using: 0 TPU cores\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"trainer = pl.Trainer(\\n    callbacks=[DisableValidationBar(), PrintLogsCallback()],\\n    num_sanity_val_steps=0,\\n    max_epochs=2,\\n    limit_train_batches=1,\\n    limit_val_batches=1,\\n    limit_test_batches=1,\\n    weights_summary=None,\\n    progress_bar_refresh_rate=0,\\n)\";\n",
       "                var nbb_formatted_code = \"trainer = pl.Trainer(\\n    callbacks=[DisableValidationBar(), PrintLogsCallback()],\\n    num_sanity_val_steps=0,\\n    max_epochs=2,\\n    limit_train_batches=1,\\n    limit_val_batches=1,\\n    limit_test_batches=1,\\n    weights_summary=None,\\n    progress_bar_refresh_rate=0,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    callbacks=[DisableValidationBar(), PrintLogsCallback()],\n",
    "    num_sanity_val_steps=0,\n",
    "    max_epochs=2,\n",
    "    limit_train_batches=1,\n",
    "    limit_val_batches=1,\n",
    "    limit_test_batches=1,\n",
    "    weights_summary=None,\n",
    "    progress_bar_refresh_rate=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2021-01-22 17:25:24,338\u001b[0m][\u001b[34mdatamodule\u001b[0m][\u001b[32mINFO\u001b[0m] - Data(fold=0, batch_size=8, im_path='../../Datasets/cassava/train_images')\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:25:24,365\u001b[0m][\u001b[34mLitModel\u001b[0m][\u001b[32mINFO\u001b[0m] - Optimization Parameters: \n",
      "Optimizer(optimizer='Adam', scheduler='FlatCos', lrs=Lrs(encoder_lr=1e-05, fc_lr=0.001), wd=None)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76444e27298e4d32962d9f2ef5c11061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2021-01-22 17:25:27,020\u001b[0m][\u001b[34msrc.logs\u001b[0m][\u001b[32mINFO\u001b[0m] - [0]: (100.00% done), TrainResult(loss=1.83, acc=0.0, valid_loss=1.607, valid_acc=0.125)\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:25:29,448\u001b[0m][\u001b[34msrc.logs\u001b[0m][\u001b[32mINFO\u001b[0m] - [1]: (100.00% done), TrainResult(loss=1.584, acc=0.25, valid_loss=1.581, valid_acc=0.5)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"trainer.fit(model, datamodule=dm)\";\n",
       "                var nbb_formatted_code = \"trainer.fit(model, datamodule=dm)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"# export\\nclass DisableProgressBar(pl.callbacks.ProgressBar):\\n    \\\"Custom Progressbar callback for Lightning Training which disables the validation bar\\\"\\n\\n    def init_sanity_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for the validation sanity run. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Validation sanity check\\\",\\n            position=(2 * self.process_position),\\n            disable=True,\\n            dynamic_ncols=True,\\n        )\\n\\n        return bar\\n\\n    def init_train_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for training. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Training\\\",\\n            initial=self.train_batch_idx,\\n            position=(2 * self.process_position),\\n            disable=True,\\n            dynamic_ncols=True,\\n        )\\n\\n        return bar\\n\\n    def init_validation_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for validation. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Validating\\\",\\n            position=(2 * self.process_position + 1),\\n            disable=True,\\n            dynamic_ncols=False,\\n        )\\n\\n        return bar\\n\\n    def init_test_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for testing. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Testing\\\",\\n            position=(2 * self.process_position),\\n            disable=True,\\n            dynamic_ncols=True,\\n        )\\n\\n        return bar\";\n",
       "                var nbb_formatted_code = \"# export\\nclass DisableProgressBar(pl.callbacks.ProgressBar):\\n    \\\"Custom Progressbar callback for Lightning Training which disables the validation bar\\\"\\n\\n    def init_sanity_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for the validation sanity run. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Validation sanity check\\\",\\n            position=(2 * self.process_position),\\n            disable=True,\\n            dynamic_ncols=True,\\n        )\\n\\n        return bar\\n\\n    def init_train_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for training. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Training\\\",\\n            initial=self.train_batch_idx,\\n            position=(2 * self.process_position),\\n            disable=True,\\n            dynamic_ncols=True,\\n        )\\n\\n        return bar\\n\\n    def init_validation_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for validation. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Validating\\\",\\n            position=(2 * self.process_position + 1),\\n            disable=True,\\n            dynamic_ncols=False,\\n        )\\n\\n        return bar\\n\\n    def init_test_tqdm(self) -> tqdm:\\n        \\\"\\\"\\\" Override this to customize the tqdm bar for testing. \\\"\\\"\\\"\\n        bar = tqdm(\\n            desc=\\\"Testing\\\",\\n            position=(2 * self.process_position),\\n            disable=True,\\n            dynamic_ncols=True,\\n        )\\n\\n        return bar\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class DisableProgressBar(pl.callbacks.ProgressBar):\n",
    "    \"Custom Progressbar callback for Lightning Training which disables the validation bar\"\n",
    "\n",
    "    def init_sanity_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for the validation sanity run. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Validation sanity check\",\n",
    "            position=(2 * self.process_position),\n",
    "            disable=True,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_train_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Training\",\n",
    "            initial=self.train_batch_idx,\n",
    "            position=(2 * self.process_position),\n",
    "            disable=True,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_validation_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for validation. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Validating\",\n",
    "            position=(2 * self.process_position + 1),\n",
    "            disable=True,\n",
    "            dynamic_ncols=False,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_test_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for testing. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Testing\",\n",
    "            position=(2 * self.process_position),\n",
    "            disable=True,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# export\\nclass ConsoleLogger(pl.Callback):\\n    \\\"Fancy logger for console-logging\\\"\\n\\n    tst_res = namedtuple(\\\"TestResult\\\", [\\\"test_loss\\\", \\\"test_acc\\\"])\\n    trn_res = namedtuple(\\\"TrainResult\\\", [\\\"loss\\\", \\\"acc\\\", \\\"val_loss\\\", \\\"val_acc\\\"])\\n    curr_step = 0\\n    has_init = False\\n    logger = logging.getLogger(\\\"src.log\\\")\\n\\n    def __init__(self, print_every: int = 50):\\n        self.print_every = print_every\\n\\n    def on_fit_start(self, trainer, pl_module, *args, **kwargs):\\n        if not self.has_init:\\n            self.log_line()\\n            self.log_msg(f\\\"Model: \\\\n{ModelSummary(trainer.model)}\\\")\\n            self.log_line()\\n            self.log_msg(f\\\"Parameters:\\\")\\n            self.log_msg(f\\\" - max_epochs: {trainer.max_epochs}\\\")\\n            self.log_msg(f\\\" - accumulate_batches: {trainer.accumulate_grad_batches}\\\")\\n            self.log_msg(f\\\" - gradient_clipping: {trainer.gradient_clip_val}\\\")\\n            self.log_line()\\n            self.log_msg(\\n                f\\\"DATASET: 'data: {len(pl_module.train_dataloader())} train + {len(pl_module.val_dataloader())} valid + {len(pl_module.test_dataloader())} test batches'\\\"\\n            )\\n\\n            has_init = True\\n\\n    def on_epoch_start(self, *args, **kwargs):\\n        self.log_line()\\n\\n    def on_train_start(self, trainer, pl_module, *args, **kwargs):\\n        self.log_line()\\n        self.log_msg(\\\"STAGE: TRAIN / VALIDATION\\\")\\n        self.log_line()\\n        self.log_msg(\\n            f\\\"Model training base path: {os.path.relpath(trainer.checkpoint_callback.dirpath)}\\\"\\n        )\\n        self.log_line()\\n        self.log_msg(f\\\"Device: {pl_module.device}\\\")\\n\\n    def on_train_epoch_start(self, *args, **kwargs):\\n        # resets the current step\\n        self.curr_step = 0\\n\\n    def on_train_batch_end(self, trainer, pl_module, *args, **kwargs):\\n        if self.curr_step % self.print_every == 0:\\n            ep = trainer.current_epoch\\n            tots = len(pl_module.train_dataloader())\\n            _stp_metrics = trainer.callback_metrics\\n            _stp_loss = _stp_metrics[\\\"train/loss_step\\\"]\\n            _stp_acc = _stp_metrics[\\\"train/acc_step\\\"]\\n\\n            self.log_msg(\\n                f\\\"epoch - {ep} - iteration {self.curr_step + 1}/{tots+1} - loss {_stp_loss:.3f} - acc {_stp_loss:.3f}\\\"\\n            )\\n\\n        self.curr_step += 1\\n\\n    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):\\n        metrics = trainer.callback_metrics\\n\\n        train_loss = metrics[\\\"train/loss_epoch\\\"]\\n        train_acc = metrics[\\\"train/acc_epoch\\\"]\\n\\n        valid_loss = metrics[\\\"valid/loss\\\"]\\n        valid_acc = metrics[\\\"valid/acc\\\"]\\n\\n        _res = self.trn_res(\\n            round(train_loss.data.cpu().numpy().item(), 3),\\n            round(train_acc.data.cpu().numpy().item(), 3),\\n            round(valid_loss.data.cpu().numpy().item(), 3),\\n            round(valid_acc.data.cpu().numpy().item(), 3),\\n        )\\n\\n        curr_epoch = int(trainer.current_epoch)\\n        self.log_line()\\n        self.log_msg(f\\\"EPOCH {curr_epoch}: {_res}\\\")\\n\\n    def on_fit_end(self, *args, **kwargs):\\n        self.log_line()\\n\\n    def on_test_start(self, trainer, pl_module, *args, **kwargs):\\n        self.log_line()\\n        self.log_msg(\\\"STAGE: TEST\\\")\\n        self.log_line()\\n        self.log_msg(\\n            f\\\"Model testing base path: {os.path.relpath(trainer.checkpoint_callback.dirpath)}\\\"\\n        )\\n        self.log_line()\\n        self.log_msg(f\\\"Device: {pl_module.device}\\\")\\n        self.log_line()\\n\\n    def on_test_epoch_end(self, trainer, pl_module, *args, **kwargs):\\n        metrics = trainer.callback_metrics\\n        test_loss = metrics[\\\"test/loss\\\"]\\n        test_acc = metrics[\\\"test/acc\\\"]\\n        self.log_msg(\\n            f\\\"{self.tst_res(round(test_loss.data.cpu().numpy().item(), 2), round(test_acc.data.cpu().numpy().item(), 2))}\\\"\\n        )\\n\\n    def log_line(self):\\n        self.logger.info(\\\"-\\\" * 70)\\n\\n    def log_msg(self, msg: str):\\n        self.logger.info(msg)\";\n",
       "                var nbb_formatted_code = \"# export\\nclass ConsoleLogger(pl.Callback):\\n    \\\"Fancy logger for console-logging\\\"\\n\\n    tst_res = namedtuple(\\\"TestResult\\\", [\\\"test_loss\\\", \\\"test_acc\\\"])\\n    trn_res = namedtuple(\\\"TrainResult\\\", [\\\"loss\\\", \\\"acc\\\", \\\"val_loss\\\", \\\"val_acc\\\"])\\n    curr_step = 0\\n    has_init = False\\n    logger = logging.getLogger(\\\"src.log\\\")\\n\\n    def __init__(self, print_every: int = 50):\\n        self.print_every = print_every\\n\\n    def on_fit_start(self, trainer, pl_module, *args, **kwargs):\\n        if not self.has_init:\\n            self.log_line()\\n            self.log_msg(f\\\"Model: \\\\n{ModelSummary(trainer.model)}\\\")\\n            self.log_line()\\n            self.log_msg(f\\\"Parameters:\\\")\\n            self.log_msg(f\\\" - max_epochs: {trainer.max_epochs}\\\")\\n            self.log_msg(f\\\" - accumulate_batches: {trainer.accumulate_grad_batches}\\\")\\n            self.log_msg(f\\\" - gradient_clipping: {trainer.gradient_clip_val}\\\")\\n            self.log_line()\\n            self.log_msg(\\n                f\\\"DATASET: 'data: {len(pl_module.train_dataloader())} train + {len(pl_module.val_dataloader())} valid + {len(pl_module.test_dataloader())} test batches'\\\"\\n            )\\n\\n            has_init = True\\n\\n    def on_epoch_start(self, *args, **kwargs):\\n        self.log_line()\\n\\n    def on_train_start(self, trainer, pl_module, *args, **kwargs):\\n        self.log_line()\\n        self.log_msg(\\\"STAGE: TRAIN / VALIDATION\\\")\\n        self.log_line()\\n        self.log_msg(\\n            f\\\"Model training base path: {os.path.relpath(trainer.checkpoint_callback.dirpath)}\\\"\\n        )\\n        self.log_line()\\n        self.log_msg(f\\\"Device: {pl_module.device}\\\")\\n\\n    def on_train_epoch_start(self, *args, **kwargs):\\n        # resets the current step\\n        self.curr_step = 0\\n\\n    def on_train_batch_end(self, trainer, pl_module, *args, **kwargs):\\n        if self.curr_step % self.print_every == 0:\\n            ep = trainer.current_epoch\\n            tots = len(pl_module.train_dataloader())\\n            _stp_metrics = trainer.callback_metrics\\n            _stp_loss = _stp_metrics[\\\"train/loss_step\\\"]\\n            _stp_acc = _stp_metrics[\\\"train/acc_step\\\"]\\n\\n            self.log_msg(\\n                f\\\"epoch - {ep} - iteration {self.curr_step + 1}/{tots+1} - loss {_stp_loss:.3f} - acc {_stp_loss:.3f}\\\"\\n            )\\n\\n        self.curr_step += 1\\n\\n    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):\\n        metrics = trainer.callback_metrics\\n\\n        train_loss = metrics[\\\"train/loss_epoch\\\"]\\n        train_acc = metrics[\\\"train/acc_epoch\\\"]\\n\\n        valid_loss = metrics[\\\"valid/loss\\\"]\\n        valid_acc = metrics[\\\"valid/acc\\\"]\\n\\n        _res = self.trn_res(\\n            round(train_loss.data.cpu().numpy().item(), 3),\\n            round(train_acc.data.cpu().numpy().item(), 3),\\n            round(valid_loss.data.cpu().numpy().item(), 3),\\n            round(valid_acc.data.cpu().numpy().item(), 3),\\n        )\\n\\n        curr_epoch = int(trainer.current_epoch)\\n        self.log_line()\\n        self.log_msg(f\\\"EPOCH {curr_epoch}: {_res}\\\")\\n\\n    def on_fit_end(self, *args, **kwargs):\\n        self.log_line()\\n\\n    def on_test_start(self, trainer, pl_module, *args, **kwargs):\\n        self.log_line()\\n        self.log_msg(\\\"STAGE: TEST\\\")\\n        self.log_line()\\n        self.log_msg(\\n            f\\\"Model testing base path: {os.path.relpath(trainer.checkpoint_callback.dirpath)}\\\"\\n        )\\n        self.log_line()\\n        self.log_msg(f\\\"Device: {pl_module.device}\\\")\\n        self.log_line()\\n\\n    def on_test_epoch_end(self, trainer, pl_module, *args, **kwargs):\\n        metrics = trainer.callback_metrics\\n        test_loss = metrics[\\\"test/loss\\\"]\\n        test_acc = metrics[\\\"test/acc\\\"]\\n        self.log_msg(\\n            f\\\"{self.tst_res(round(test_loss.data.cpu().numpy().item(), 2), round(test_acc.data.cpu().numpy().item(), 2))}\\\"\\n        )\\n\\n    def log_line(self):\\n        self.logger.info(\\\"-\\\" * 70)\\n\\n    def log_msg(self, msg: str):\\n        self.logger.info(msg)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class ConsoleLogger(pl.Callback):\n",
    "    \"Fancy logger for console-logging\"\n",
    "\n",
    "    tst_res = namedtuple(\"TestResult\", [\"test_loss\", \"test_acc\"])\n",
    "    trn_res = namedtuple(\"TrainResult\", [\"loss\", \"acc\", \"val_loss\", \"val_acc\"])\n",
    "    curr_step = 0\n",
    "    has_init = False\n",
    "    logger = logging.getLogger(\"src.log\")\n",
    "\n",
    "    def __init__(self, print_every: int = 50):\n",
    "        self.print_every = print_every\n",
    "\n",
    "    def on_fit_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        if not self.has_init:\n",
    "            self.log_line()\n",
    "            self.log_msg(f\"Model: \\n{ModelSummary(trainer.model)}\")\n",
    "            self.log_line()\n",
    "            self.log_msg(f\"Parameters:\")\n",
    "            self.log_msg(f\" - max_epochs: {trainer.max_epochs}\")\n",
    "            self.log_msg(f\" - accumulate_batches: {trainer.accumulate_grad_batches}\")\n",
    "            self.log_msg(f\" - gradient_clipping: {trainer.gradient_clip_val}\")\n",
    "            self.log_line()\n",
    "            self.log_msg(\n",
    "                f\"DATASET: 'data: {len(pl_module.train_dataloader())} train + {len(pl_module.val_dataloader())} valid + {len(pl_module.test_dataloader())} test batches'\"\n",
    "            )\n",
    "\n",
    "            has_init = True\n",
    "\n",
    "    def on_epoch_start(self, *args, **kwargs):\n",
    "        self.log_line()\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        self.log_line()\n",
    "        self.log_msg(\"STAGE: TRAIN / VALIDATION\")\n",
    "        self.log_line()\n",
    "        self.log_msg(\n",
    "            f\"Model training base path: {os.path.relpath(trainer.checkpoint_callback.dirpath)}\"\n",
    "        )\n",
    "        self.log_line()\n",
    "        self.log_msg(f\"Device: {pl_module.device}\")\n",
    "\n",
    "    def on_train_epoch_start(self, *args, **kwargs):\n",
    "        # resets the current step\n",
    "        self.curr_step = 0\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.curr_step % self.print_every == 0:\n",
    "            ep = trainer.current_epoch\n",
    "            tots = len(pl_module.train_dataloader())\n",
    "            _stp_metrics = trainer.callback_metrics\n",
    "            _stp_loss = _stp_metrics[\"train/loss_step\"]\n",
    "            _stp_acc = _stp_metrics[\"train/acc_step\"]\n",
    "\n",
    "            self.log_msg(\n",
    "                f\"epoch - {ep} - iteration {self.curr_step + 1}/{tots+1} - loss {_stp_loss:.3f} - acc {_stp_loss:.3f}\"\n",
    "            )\n",
    "\n",
    "        self.curr_step += 1\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        metrics = trainer.callback_metrics\n",
    "\n",
    "        train_loss = metrics[\"train/loss_epoch\"]\n",
    "        train_acc = metrics[\"train/acc_epoch\"]\n",
    "\n",
    "        valid_loss = metrics[\"valid/loss\"]\n",
    "        valid_acc = metrics[\"valid/acc\"]\n",
    "\n",
    "        _res = self.trn_res(\n",
    "            round(train_loss.data.cpu().numpy().item(), 3),\n",
    "            round(train_acc.data.cpu().numpy().item(), 3),\n",
    "            round(valid_loss.data.cpu().numpy().item(), 3),\n",
    "            round(valid_acc.data.cpu().numpy().item(), 3),\n",
    "        )\n",
    "\n",
    "        curr_epoch = int(trainer.current_epoch)\n",
    "        self.log_line()\n",
    "        self.log_msg(f\"EPOCH {curr_epoch}: {_res}\")\n",
    "\n",
    "    def on_fit_end(self, *args, **kwargs):\n",
    "        self.log_line()\n",
    "\n",
    "    def on_test_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        self.log_line()\n",
    "        self.log_msg(\"STAGE: TEST\")\n",
    "        self.log_line()\n",
    "        self.log_msg(\n",
    "            f\"Model testing base path: {os.path.relpath(trainer.checkpoint_callback.dirpath)}\"\n",
    "        )\n",
    "        self.log_line()\n",
    "        self.log_msg(f\"Device: {pl_module.device}\")\n",
    "        self.log_line()\n",
    "\n",
    "    def on_test_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        metrics = trainer.callback_metrics\n",
    "        test_loss = metrics[\"test/loss\"]\n",
    "        test_acc = metrics[\"test/acc\"]\n",
    "        self.log_msg(\n",
    "            f\"{self.tst_res(round(test_loss.data.cpu().numpy().item(), 2), round(test_acc.data.cpu().numpy().item(), 2))}\"\n",
    "        )\n",
    "\n",
    "    def log_line(self):\n",
    "        self.logger.info(\"-\" * 70)\n",
    "\n",
    "    def log_msg(self, msg: str):\n",
    "        self.logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2021-01-22 17:26:09,063\u001b[0m][\u001b[34mdatamodule\u001b[0m][\u001b[32mINFO\u001b[0m] - Data(fold=0, batch_size=8, im_path='../../Datasets/cassava/train_images')\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:09,782\u001b[0m][\u001b[34mLitModel\u001b[0m][\u001b[32mINFO\u001b[0m] - Loss Function : LabelSmoothingCrossEntropy()\u001b[0m\n",
      "GPU available: False, used: False\n",
      "[\u001b[36m2021-01-22 17:26:09,784\u001b[0m][\u001b[34mlightning\u001b[0m][\u001b[32mINFO\u001b[0m] - GPU available: False, used: False\u001b[0m\n",
      "TPU available: False, using: 0 TPU cores\n",
      "[\u001b[36m2021-01-22 17:26:09,785\u001b[0m][\u001b[34mlightning\u001b[0m][\u001b[32mINFO\u001b[0m] - TPU available: False, using: 0 TPU cores\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"dm.prepare_data()\\ndm.setup()\\n\\nencoder = timm.create_model(\\\"resnet18\\\", pretrained=False)\\nmodel = TransferLearningModel(encoder, cut=-2, c=5, act=nn.ReLU(inplace=True))\\nmodel = LightningCassava(model=model, conf=cfg)\\ninfo_logger = ConsoleLogger(print_every=2)\\n\\ntrainer = pl.Trainer(\\n    callbacks=[DisableProgressBar(), info_logger],\\n    num_sanity_val_steps=0,\\n    max_epochs=2,\\n    limit_train_batches=4,\\n    limit_val_batches=1,\\n    limit_test_batches=1,\\n    weights_summary=None,\\n    progress_bar_refresh_rate=0,\\n)\";\n",
       "                var nbb_formatted_code = \"dm.prepare_data()\\ndm.setup()\\n\\nencoder = timm.create_model(\\\"resnet18\\\", pretrained=False)\\nmodel = TransferLearningModel(encoder, cut=-2, c=5, act=nn.ReLU(inplace=True))\\nmodel = LightningCassava(model=model, conf=cfg)\\ninfo_logger = ConsoleLogger(print_every=2)\\n\\ntrainer = pl.Trainer(\\n    callbacks=[DisableProgressBar(), info_logger],\\n    num_sanity_val_steps=0,\\n    max_epochs=2,\\n    limit_train_batches=4,\\n    limit_val_batches=1,\\n    limit_test_batches=1,\\n    weights_summary=None,\\n    progress_bar_refresh_rate=0,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "\n",
    "encoder = timm.create_model(\"resnet18\", pretrained=False)\n",
    "model = TransferLearningModel(encoder, cut=-2, c=5, act=nn.ReLU(inplace=True))\n",
    "model = LightningCassava(model=model, conf=cfg)\n",
    "info_logger = ConsoleLogger(print_every=2)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[DisableProgressBar(), info_logger],\n",
    "    num_sanity_val_steps=0,\n",
    "    max_epochs=2,\n",
    "    limit_train_batches=4,\n",
    "    limit_val_batches=1,\n",
    "    limit_test_batches=1,\n",
    "    weights_summary=None,\n",
    "    progress_bar_refresh_rate=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2021-01-22 17:26:13,216\u001b[0m][\u001b[34mLitModel\u001b[0m][\u001b[32mINFO\u001b[0m] - Optimization Parameters: \n",
      "Optimizer(optimizer='Adam', scheduler='FlatCos', lrs=Lrs(encoder_lr=1e-05, fc_lr=0.001), wd=None)\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,217\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,220\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - Model: \n",
      "  | Name      | Type                       | Params\n",
      "---------------------------------------------------------\n",
      "0 | model     | TransferLearningModel      | 11.7 M\n",
      "1 | accuracy  | Accuracy                   | 0     \n",
      "2 | loss_func | LabelSmoothingCrossEntropy | 0     \u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,221\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,222\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - Parameters:\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,223\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] -  - max_epochs: 2\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,223\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] -  - accumulate_batches: 1\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,224\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] -  - gradient_clipping: 0\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,225\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,226\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - DATASET: 'data: 2140 train + 535 valid + 535 test batches'\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,260\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,261\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - STAGE: TRAIN / VALIDATION\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,261\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,262\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - Model training base path: lightning_logs/version_57/checkpoints\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,263\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,264\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - Device: cpu\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:13,266\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:14,962\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - epoch - 0 - iteration 1/2141 - loss 1.689 - acc 1.689\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:17,997\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - epoch - 0 - iteration 3/2141 - loss 1.646 - acc 1.646\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:20,226\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:20,226\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - EPOCH 0: TrainResult(loss=1.685, acc=0.312, val_loss=1.564, val_acc=0.5)\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:20,229\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:21,829\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - epoch - 1 - iteration 1/2141 - loss 1.605 - acc 1.605\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:24,749\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - epoch - 1 - iteration 3/2141 - loss 1.276 - acc 1.276\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:27,067\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:27,068\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - EPOCH 1: TrainResult(loss=1.507, acc=0.375, val_loss=1.494, val_acc=0.5)\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:27,071\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"trainer.fit(model, datamodule=dm)\";\n",
       "                var nbb_formatted_code = \"trainer.fit(model, datamodule=dm)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2021-01-22 17:26:40,863\u001b[0m][\u001b[34mLitModel\u001b[0m][\u001b[32mINFO\u001b[0m] - Optimization Parameters: \n",
      "Optimizer(optimizer='Adam', scheduler='FlatCos', lrs=Lrs(encoder_lr=1e-05, fc_lr=0.001), wd=None)\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,872\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,875\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - Model: \n",
      "  | Name      | Type                       | Params\n",
      "---------------------------------------------------------\n",
      "0 | model     | TransferLearningModel      | 11.7 M\n",
      "1 | accuracy  | Accuracy                   | 0     \n",
      "2 | loss_func | LabelSmoothingCrossEntropy | 0     \u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,876\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,876\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - Parameters:\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,877\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] -  - max_epochs: 2\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,878\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] -  - accumulate_batches: 1\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,879\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] -  - gradient_clipping: 0\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,879\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,881\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - DATASET: 'data: 2140 train + 535 valid + 535 test batches'\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,900\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,901\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - STAGE: TEST\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,902\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,902\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - Model testing base path: lightning_logs/version_57/checkpoints\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,903\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,903\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - Device: cpu\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:40,904\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:41,468\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - TestResult(test_loss=1.49, test_acc=0.5)\u001b[0m\n",
      "[\u001b[36m2021-01-22 17:26:41,470\u001b[0m][\u001b[34msrc.log\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"_ = trainer.test(datamodule=dm, verbose=False)\";\n",
       "                var nbb_formatted_code = \"_ = trainer.test(datamodule=dm, verbose=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = trainer.test(datamodule=dm, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_mixmethods.ipynb.\n",
      "Converted 02_losses.ipynb.\n",
      "Converted 03_layers.ipynb.\n",
      "Converted 03a_networks.ipynb.\n",
      "Converted 04_optimizers_schedules.ipynb.\n",
      "Converted 05_lightning.core.ipynb.\n",
      "Converted 05a_lightning.callbacks.ipynb.\n",
      "Converted 06_fastai.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"# hide\\nfrom nbdev.export import *\\n\\nnotebook2script()\";\n",
       "                var nbb_formatted_code = \"# hide\\nfrom nbdev.export import *\\n\\nnotebook2script()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
