{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp lightning.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import _logger as log\n",
    "\n",
    "from src.lightning.core import *\n",
    "from src.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up python logging\n",
    "logging.basicConfig(format='[%(asctime)s][%(levelname)s]: %(message)s', datefmt=\"%m/%d/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class WandbImageClassificationCallback(pl.Callback):\n",
    "    \"\"\" Custom callback to add some extra functionalites to the wandb logger \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_batches:int = 16, \n",
    "                 log_train_batch: bool = False,\n",
    "                 log_preds: bool = False,\n",
    "                 log_conf_mat: bool = True,):\n",
    "        \n",
    "        # class names for the confusion matrix\n",
    "        self.class_names = list(conf_mat_idx2lbl.values())\n",
    "        \n",
    "        # counter to log training batch images\n",
    "        self.num_bs = num_batches\n",
    "        self.curr_epoch = 0\n",
    "        \n",
    "        self.log_train_batch = log_train_batch\n",
    "        self.log_preds = log_preds\n",
    "        self.log_conf_mat = log_conf_mat\n",
    "        \n",
    "        self.val_imgs, self.val_labels = None, None\n",
    "        \n",
    "    def on_train_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        try:\n",
    "            # log model to the wandb experiment\n",
    "            wandb.watch(models=pl_module.model, criterion=pl_module.loss_func)\n",
    "        except:\n",
    "            log.info(\"Skipping wandb.watch --->\")\n",
    "        \n",
    "    def on_train_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_train_batch:\n",
    "            if pl_module.one_batch is None:\n",
    "                log.info(f\"{self.config_defaults['mixmethod']} samples not available . Skipping --->\")\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                one_batch = pl_module.one_batch[:self.num_bs]\n",
    "                train_ims = one_batch.data.to('cpu')\n",
    "                trainer.logger.experiment.log({\"train_batch\":[wandb.Image(x) for x in train_ims]}, commit=False)\n",
    "        \n",
    "    def on_validation_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_preds:\n",
    "            if self.val_imgs is None and self.val_labels is None:\n",
    "                self.val_imgs, self.val_labels = next(iter(pl_module.val_dataloader()))\n",
    "                self.val_imgs, self.val_labels = self.val_imgs[:self.num_bs], self.val_labels[:self.num_bs]\n",
    "                self.val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "\n",
    "            logits = pl_module(self.val_imgs)\n",
    "            preds  = torch.argmax(logits, 1)\n",
    "            preds  = preds.data.cpu()\n",
    "            \n",
    "            ims = [wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\") for x,pred,y in zip(self.val_imgs,preds,self.val_labels)]\n",
    "            log_dict = {\"predictions\": ims}\n",
    "            wandb.log(ims,commit=False)\n",
    "            \n",
    "    def on_epoch_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        pl_module.val_labels_list = []\n",
    "        pl_module.val_preds_list  = []\n",
    "    \n",
    "    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_conf_mat:\n",
    "            val_preds  = torch.tensor(pl_module.val_preds_list).data.cpu().numpy()\n",
    "            val_labels = torch.tensor(pl_module.val_labels_list).data.cpu().numpy()\n",
    "            log_dict = {'conf_mat': wandb.plot.confusion_matrix(val_preds,val_labels,self.class_names)}\n",
    "            wandb.log(log_dict,commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LitProgressBar(pl.callbacks.ProgressBar):\n",
    "    \"Custom Progressbar callback for Lightning Training\"\n",
    "    \n",
    "    def init_sanity_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for the validation sanity run. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Validation sanity check',\n",
    "            #position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            dynamic_ncols=True,)\n",
    "        \n",
    "        return bar\n",
    "    \n",
    "    def init_train_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Training',\n",
    "            #initial=self.train_batch_idx,\n",
    "            #position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            dynamic_ncols=True,)\n",
    "        \n",
    "        return bar\n",
    "    \n",
    "    def init_validation_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for validation. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Validating',\n",
    "            #position=(2 * self.process_position + 1),\n",
    "            disable=True,\n",
    "            dynamic_ncols=False,)\n",
    "        \n",
    "        return bar\n",
    "    \n",
    "    def init_test_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for testing. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Testing',\n",
    "            #position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            dynamic_ncols=True,)\n",
    "        \n",
    "        return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PrintLogsCallback(pl.Callback):\n",
    "    \"Logs Training logs to console after every epoch\"\n",
    "    def __init__(self, print_str: str = None):\n",
    "        self.print_str = 'eta: {} loss: {:.4f} acc: {:.4f} valid_loss: {:.4f} valid_acc: {:.4f}'\n",
    "        log = logging.getLogger(__name__)\n",
    "        log.setLevel(logging.INFO)\n",
    "        self.logger = log\n",
    "    \n",
    "    def on_epoch_start(self, *args, **kwargs):\n",
    "        self.eta_start = time.time()\n",
    "    \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        train_loss = metrics['train/loss_epoch']\n",
    "        train_acc  = metrics['train/acc_epoch']\n",
    "        valid_loss = metrics['valid/loss']\n",
    "        valid_acc  = metrics['valid/acc']\n",
    "        \n",
    "        end_time = time.time()\n",
    "        self.eta_string = str(datetime.timedelta(seconds=int(end_time-self.eta_start)))\n",
    "        self.curr_epoch = int(trainer.current_epoch)\n",
    "        print_str = self.print_str.format(self.eta_string, train_loss, train_acc, valid_loss, valid_acc)\n",
    "        \n",
    "        self.logger.info(f\"Epoch {self.curr_epoch} \")\n",
    "        self.logger.info(print_str)\n",
    "    \n",
    "    def on_test_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        metrics = trainer.callback_metrics\n",
    "        \n",
    "        train_loss = metrics['train/loss_epoch']\n",
    "        train_acc  = metrics['train/acc_epoch']\n",
    "        \n",
    "        test_loss  = metrics['test/loss']\n",
    "        test_acc   = metrics['test/acc']\n",
    "        \n",
    "        \n",
    "        fmt_str1 = \"[Train] loss: {:.4f} acc: {:.4f}\"\n",
    "        fmt_str2 = \"[Test ] loss: {:.4f} acc: {:.4f}\"\n",
    "        \n",
    "        str1 = fmt_str1.format(train_loss, train_acc)\n",
    "        str2 = fmt_str2.format(test_loss, test_acc)\n",
    "        \n",
    "        self.logger.info(\"Finished !\")\n",
    "        self.logger.info(str1)\n",
    "        self.logger.info(str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "from torch import nn\n",
    "from src.networks import *\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augs = A.Compose([\n",
    "    A.RandomResizedCrop(224, 224, p=1.0),\n",
    "    A.RandomBrightness(limit=0.1),\n",
    "    A.HueSaturationValue(20, 20, 20),\n",
    "    A.HorizontalFlip(),\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensorV2(p=1.0)])\n",
    "\n",
    "valid_augs = A.Compose([\n",
    "    A.Resize(224, 224, p=1.0),\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensorV2(p=1.0)])\n",
    "\n",
    "csv = \"../../leaf-disease-classification-kaggle/data/stratified-data-5folds.csv\"\n",
    "ims = \"../../Datasets/cassava/train_images/\"\n",
    "dm = CassavaLightningDataModule(csv, ims, curr_fold=0, train_augs=train_augs, valid_augs=valid_augs, bs=8, num_workers=0)\n",
    "\n",
    "\n",
    "model_hparams = dict(\n",
    "    mixmethod        = None,\n",
    "    loss             = dict(_target_='src.losses.LabelSmoothingCrossEntropy', eps=0.1),\n",
    "    learning_rate    = 1e-03,\n",
    "    lr_mult          = 100,\n",
    "    optimizer        = dict(_target_='torch.optim.Adam'),\n",
    "    scheduler        = dict(function=dict(_target_='src.opts.FlatCos', num_epochs=10, pct_start=0.7), \n",
    "                            metric_to_track=None, scheduler_interval='step'),\n",
    ")\n",
    "\n",
    "OmegaConf.create(model_hparams)\n",
    "\n",
    "\n",
    "\n",
    "encoder = timm.create_model('resnet18', pretrained=False)\n",
    "model   = TransferLearningModel(encoder, cut=-2, c=5, act=nn.ReLU(inplace=True))\n",
    "model   = LightningCassava(model=model, conf=model_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "[01/17/2021 13:04:54][INFO]: GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "[01/17/2021 13:04:54][INFO]: TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(callbacks=[LitProgressBar(), PrintLogsCallback()], \n",
    "                     num_sanity_val_steps=0, max_epochs=2, limit_train_batches=1, \n",
    "                     limit_val_batches=1, limit_test_batches=1, weights_summary=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0382069fc349db9ff88a3fd4352f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01/17/2021 13:04:57][INFO]: Epoch 0 \n",
      "[01/17/2021 13:04:57][INFO]: eta: 0:00:02 loss: 1.7270 acc: 0.1250 valid_loss: 1.6077 valid_acc: 0.3750\n",
      "[01/17/2021 13:05:00][INFO]: Epoch 1 \n",
      "[01/17/2021 13:05:00][INFO]: eta: 0:00:02 loss: 1.4613 acc: 0.1250 valid_loss: 1.5937 valid_acc: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5d3e5b67bf4294ab18f9d0c6db1b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01/17/2021 13:05:03][INFO]: Finished !\n",
      "[01/17/2021 13:05:03][INFO]: [Train] loss: 1.4613 acc: 0.1250\n",
      "[01/17/2021 13:05:03][INFO]: [Test ] loss: 1.5937 acc: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test/acc': tensor(0.6250),\n",
      " 'test/loss': tensor(1.5937),\n",
      " 'train/acc': tensor(0.1250),\n",
      " 'train/acc_epoch': tensor(0.1250),\n",
      " 'train/acc_step': tensor(0.1250),\n",
      " 'train/loss': tensor(1.4613),\n",
      " 'train/loss_epoch': tensor(1.4613),\n",
      " 'train/loss_step': tensor(1.4613),\n",
      " 'valid/acc': tensor(0.6250),\n",
      " 'valid/loss': tensor(1.5937)}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = trainer.test(model, datamodule=dm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_mixmethods.ipynb.\n",
      "Converted 02_losses.ipynb.\n",
      "Converted 03_layers.ipynb.\n",
      "Converted 03a_networks.ipynb.\n",
      "Converted 04_optimizers_schedules.ipynb.\n",
      "Converted 05_lightning.core.ipynb.\n",
      "Converted 05a_lightning.callbacks.ipynb.\n",
      "Converted 06_fastai.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
