{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp lightning.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import tqdm\n",
    "import wandb\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import _logger as log\n",
    "\n",
    "from src.lightning.core import *\n",
    "from src.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class WandbImageClassificationCallback(pl.Callback):\n",
    "    \"\"\" Custom callback to add some extra functionalites to the wandb logger \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_batches:int = 16, \n",
    "                 log_train_batch: bool = False,\n",
    "                 log_preds: bool = False,\n",
    "                 log_conf_mat: bool = True,):\n",
    "        \n",
    "        # class names for the confusion matrix\n",
    "        self.class_names = list(conf_mat_idx2lbl.values())\n",
    "        \n",
    "        # counter to log training batch images\n",
    "        self.num_bs = num_batches\n",
    "        self.curr_epoch = 0\n",
    "        \n",
    "        self.log_train_batch = log_train_batch\n",
    "        self.log_preds = log_preds\n",
    "        self.log_conf_mat = log_conf_mat\n",
    "        \n",
    "        self.val_imgs, self.val_labels = None, None\n",
    "        \n",
    "    def on_train_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        try:\n",
    "            # log model to the wandb experiment\n",
    "            wandb.watch(models=pl_module.model, criterion=pl_module.loss_func)\n",
    "        except:\n",
    "            log.info(\"Skipping wandb.watch --->\")\n",
    "        \n",
    "    def on_train_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_train_batch:\n",
    "            if pl_module.one_batch is None:\n",
    "                log.info(f\"{self.config_defaults['mixmethod']} samples not available . Skipping --->\")\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                one_batch = pl_module.one_batch[:self.num_bs]\n",
    "                train_ims = one_batch.data.to('cpu')\n",
    "                trainer.logger.experiment.log({\"train_batch\":[wandb.Image(x) for x in train_ims]}, commit=False)\n",
    "        \n",
    "    def on_validation_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_preds:\n",
    "            if self.val_imgs is None and self.val_labels is None:\n",
    "                self.val_imgs, self.val_labels = next(iter(pl_module.val_dataloader()))\n",
    "                self.val_imgs, self.val_labels = self.val_imgs[:self.num_bs], self.val_labels[:self.num_bs]\n",
    "                self.val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "\n",
    "            logits = pl_module(self.val_imgs)\n",
    "            preds  = torch.argmax(logits, 1)\n",
    "            preds  = preds.data.cpu()\n",
    "            \n",
    "            ims = [wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\") for x,pred,y in zip(self.val_imgs,preds,self.val_labels)]\n",
    "            log_dict = {\"predictions\": ims}\n",
    "            wandb.log(ims,commit=False)\n",
    "            \n",
    "    def on_epoch_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        pl_module.val_labels_list = []\n",
    "        pl_module.val_preds_list  = []\n",
    "    \n",
    "    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_conf_mat:\n",
    "            val_preds  = torch.tensor(pl_module.val_preds_list).data.cpu().numpy()\n",
    "            val_labels = torch.tensor(pl_module.val_labels_list).data.cpu().numpy()\n",
    "            log_dict = {'conf_mat': wandb.plot.confusion_matrix(val_preds,val_labels,self.class_names)}\n",
    "            wandb.log(log_dict,commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TqdmLoggingHandler(logging.Handler):\n",
    "    def __init__(self, level=logging.NOTSET):\n",
    "        super().__init__(level)\n",
    "\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            tqdm.tqdm.write(msg)\n",
    "            self.flush()\n",
    "        except (KeyboardInterrupt, SystemExit):\n",
    "            raise\n",
    "        except:\n",
    "            self.handleError(record)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "log.addHandler(TqdmLoggingHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LitProgressBar(pl.callbacks.ProgressBar):\n",
    "    \"Custom Progressbar callback for Lightning Training\"\n",
    "    \n",
    "    def init_sanity_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for the validation sanity run. \"\"\"\n",
    "        bar = tqdm.tqdm(\n",
    "            desc='Validation sanity check',\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            leave=False,\n",
    "            dynamic_ncols=True,\n",
    "            file=sys.stdout,\n",
    "        )\n",
    "        return bar\n",
    "    \n",
    "    def init_train_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\n",
    "        bar = tqdm.tqdm(\n",
    "            desc='Training',\n",
    "            initial=self.train_batch_idx,\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            leave=True,\n",
    "            dynamic_ncols=True,\n",
    "            file=sys.stdout,\n",
    "            smoothing=0,\n",
    "        )\n",
    "        return bar\n",
    "    \n",
    "    def init_validation_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for validation. \"\"\"\n",
    "        bar = tqdm.tqdm(\n",
    "            desc='Validating',\n",
    "            position=(2 * self.process_position + 1),\n",
    "            disable=True,\n",
    "            leave=False,\n",
    "            dynamic_ncols=False,\n",
    "            file=sys.stdout\n",
    "        )\n",
    "        return bar\n",
    "    \n",
    "    def init_test_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for testing. \"\"\"\n",
    "        bar = tqdm.tqdm(\n",
    "            desc='Testing',\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            leave=True,\n",
    "            dynamic_ncols=True,\n",
    "            file=sys.stdout\n",
    "        )\n",
    "        return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PrintLogsCallback(pl.Callback):\n",
    "    \"Logs Training logs to console after every epoch\"\n",
    "    def __init__(self, print_str: str = None):\n",
    "        self.print_str = 'Epoch: [{}] eta: {} loss: {:.4f} acc: {:.4f} valid_loss: {:.4f} valid_acc: {:.4f}'\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def on_epoch_start(self, *args, **kwargs):\n",
    "        self.eta_start = time.time()\n",
    "    \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        train_loss = metrics['train/loss']\n",
    "        train_acc  = metrics['train/acc']\n",
    "        valid_loss = metrics['valid/loss']\n",
    "        valid_acc  = metrics['valid/acc']\n",
    "        \n",
    "        end_time = time.time()\n",
    "        self.eta_string = str(datetime.timedelta(seconds=int(end_time-self.eta_start)))\n",
    "        self.curr_epoch = int(trainer.current_epoch)\n",
    "        print_str = self.print_str.format(self.curr_epoch, self.eta_string, \n",
    "                                        train_loss, train_acc, \n",
    "                                        valid_loss, valid_acc)\n",
    "        self.logger.info(print_str)\n",
    "    \n",
    "    def on_test_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        metrics = trainer.callback_metrics\n",
    "        train_loss = metrics['train/loss']\n",
    "        train_acc  = metrics['train/acc']\n",
    "        valid_loss = metrics['valid/loss']\n",
    "        valid_acc  = metrics['valid/acc']\n",
    "        test_loss  = metrics['test/loss']\n",
    "        test_acc   = metrics['test/acc']\n",
    "        \n",
    "        \n",
    "        fmt_str1 = \"Summary: [Train] loss: {:.4f} acc: {:.4f}\"\n",
    "        fmt_str2 = \"Summary: [Valid] loss: {:.4f} acc: {:.4f}\"\n",
    "        fmt_str3 = \"Summary: [Test]  loss: {:.4f} acc: {:.4f}\"\n",
    "        \n",
    "        str1 = fmt_str1.format(train_loss, train_acc)\n",
    "        str2 = fmt_str2.format(valid_loss, valid_acc)\n",
    "        str3 = fmt_str3.format(test_loss, test_acc)\n",
    "        \n",
    "        self.logger.info(str1)\n",
    "        self.logger.info(str2)\n",
    "        self.logger.info(str3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "from torch import nn\n",
    "from src.networks import *\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2021 09:07:21 - INFO - src.lightning.core - Loss Function : LabelSmoothingCrossEntropy()\n"
     ]
    }
   ],
   "source": [
    "train_augs = A.Compose([\n",
    "    A.RandomResizedCrop(224, 224, p=1.0),\n",
    "    A.RandomBrightness(limit=0.1),\n",
    "    A.HueSaturationValue(20, 20, 20),\n",
    "    A.HorizontalFlip(),\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensorV2(p=1.0)])\n",
    "\n",
    "valid_augs = A.Compose([\n",
    "    A.Resize(224, 224, p=1.0),\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensorV2(p=1.0)])\n",
    "\n",
    "csv = \"../../leaf-disease-classification-kaggle/data/stratified-data-5folds.csv\"\n",
    "ims = \"../../Datasets/cassava/train_images/\"\n",
    "dm = CassavaLightningDataModule(csv, ims, curr_fold=0, train_augs=train_augs, valid_augs=valid_augs, bs=8, num_workers=0)\n",
    "\n",
    "\n",
    "model_hparams = dict(\n",
    "    mixmethod        = None,\n",
    "    loss             = dict(_target_='src.losses.LabelSmoothingCrossEntropy', eps=0.1),\n",
    "    learning_rate    = 1e-03,\n",
    "    lr_mult          = 100,\n",
    "    optimizer        = dict(_target_='src.opts.Ranger', weight_decay=1e-02, betas=[0.95, 0.999], eps=1e-05),\n",
    "    scheduler        = dict(_target_='src.opts.FlatCos', num_epochs=10, pct_start=0.7),\n",
    "    metric_to_track  = None,\n",
    "    scheduler_interval= \"epoch\",\n",
    ")\n",
    "\n",
    "OmegaConf.create(model_hparams)\n",
    "\n",
    "\n",
    "\n",
    "encoder = timm.create_model('resnet18', pretrained=False)\n",
    "model   = TransferLearningModel(encoder, cut=-2, c=5, act=nn.ReLU(inplace=True))\n",
    "model   = LightningCassava(model=model, conf=model_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "01/06/2021 09:07:22 - INFO - lightning - GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "01/06/2021 09:07:22 - INFO - lightning - TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(callbacks=[LitProgressBar(), PrintLogsCallback()], \n",
    "                     num_sanity_val_steps=0, max_epochs=2, limit_train_batches=1, \n",
    "                     limit_val_batches=1, limit_test_batches=1, weights_summary=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2021 09:07:23 - INFO - src.lightning.core - DATA: ../../Datasets/cassava/train_images/\n",
      "01/06/2021 09:07:23 - INFO - src.lightning.core - FOLD: 0  BATCH_SIZE: 8\n",
      "01/06/2021 09:07:23 - INFO - src.lightning.core - Optimizer: Ranger  LR's: (1e-05, 0.001)\n",
      "01/06/2021 09:07:23 - INFO - src.lightning.core - LR Scheculer: FlatCos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Epoch: [0] eta: 0:00:02 loss: 1.7546 acc: 0.1250 valid_loss: 1.6144 valid_acc: 0.0000\n",
      "Epoch 0: 100%|██████████| 2/2 [00:02<00:00,  1.36s/it, loss=1.755, v_num=2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2021 09:07:26 - INFO - __main__ - Epoch: [0] eta: 0:00:02 loss: 1.7546 acc: 0.1250 valid_loss: 1.6144 valid_acc: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1] eta: 0:00:02 loss: 1.6246 acc: 0.2500 valid_loss: 1.6092 valid_acc: 0.1250\n",
      "Epoch 1: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it, loss=1.690, v_num=2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2021 09:07:28 - INFO - __main__ - Epoch: [1] eta: 0:00:02 loss: 1.6246 acc: 0.2500 valid_loss: 1.6092 valid_acc: 0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it, loss=1.690, v_num=2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test/acc': tensor(0.1250),\n",
      " 'test/loss': tensor(1.6092),\n",
      " 'train/acc': tensor(0.2500),\n",
      " 'train/acc_epoch': tensor(0.2500),\n",
      " 'train/acc_step': tensor(0.2500),\n",
      " 'train/loss': tensor(1.6246),\n",
      " 'train/loss_epoch': tensor(1.6246),\n",
      " 'train/loss_step': tensor(1.6246),\n",
      " 'valid/acc': tensor(0.1250),\n",
      " 'valid/loss': tensor(1.6092)}\n",
      "--------------------------------------------------------------------------------\n",
      "Summary: [Train] loss: 1.6246 acc: 0.2500             \n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2021 09:07:37 - INFO - __main__ - Summary: [Train] loss: 1.6246 acc: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: [Valid] loss: 1.6092 acc: 0.1250             \n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2021 09:07:37 - INFO - __main__ - Summary: [Valid] loss: 1.6092 acc: 0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: [Test]  loss: 1.6092 acc: 0.1250             \n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2021 09:07:37 - INFO - __main__ - Summary: [Test]  loss: 1.6092 acc: 0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "_ = trainer.test(model, datamodule=dm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_mixmethods.ipynb.\n",
      "Converted 02_losses.ipynb.\n",
      "Converted 03_layers.ipynb.\n",
      "Converted 03a_networks.ipynb.\n",
      "Converted 04_optimizers_schedules.ipynb.\n",
      "Converted 05_lightning.core.ipynb.\n",
      "Converted 05a_lightning.callbacks.ipynb.\n",
      "Converted 06_fastai.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
