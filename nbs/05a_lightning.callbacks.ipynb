{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp lightning.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import _logger as log\n",
    "from pytorch_lightning.core.memory import ModelSummary, get_human_readable_count\n",
    "\n",
    "from src.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorlog\n",
    "\n",
    "handler = colorlog.StreamHandler()\n",
    "\n",
    "fmt = \"[%(cyan)s%(asctime)s%(reset)s][%(blue)s%(name)s%(reset)s][%(log_color)s%(levelname)s%(reset)s] - %(message)s\"\n",
    "colors = dict(\n",
    "    DEBUG=\"purple\", INFO=\"green\", WARNING=\"yellow\", ERROR=\"red\", CRITICAL=\"red\"\n",
    ")\n",
    "formatter = colorlog.ColoredFormatter(fmt=fmt, log_colors=colors)\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "logging.basicConfig(format=fmt, level=logging.INFO, handlers=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class WandbImageClassificationCallback(pl.Callback):\n",
    "    \"\"\" Custom callback to add some extra functionalites to the wandb logger \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_batches: int = 16,\n",
    "        log_train_batch: bool = False,\n",
    "        log_preds: bool = False,\n",
    "        log_conf_mat: bool = True,\n",
    "    ):\n",
    "\n",
    "        # class names for the confusion matrix\n",
    "        self.class_names = list(conf_mat_idx2lbl.values())\n",
    "\n",
    "        # counter to log training batch images\n",
    "        self.num_bs = num_batches\n",
    "        self.curr_epoch = 0\n",
    "\n",
    "        self.log_train_batch = log_train_batch\n",
    "        self.log_preds = log_preds\n",
    "        self.log_conf_mat = log_conf_mat\n",
    "\n",
    "        self.val_imgs, self.val_labels = None, None\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        try:\n",
    "            # log model to the wandb experiment\n",
    "            wandb.watch(models=pl_module.model, criterion=pl_module.loss_func)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_train_batch:\n",
    "            if pl_module.one_batch is None:\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                one_batch = pl_module.one_batch[: self.num_bs]\n",
    "                train_ims = one_batch.data.to(\"cpu\")\n",
    "                trainer.logger.experiment.log(\n",
    "                    {\"train_batch\": [wandb.Image(x) for x in train_ims]}, commit=False\n",
    "                )\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_preds:\n",
    "            if self.val_imgs is None and self.val_labels is None:\n",
    "                self.val_imgs, self.val_labels = next(iter(pl_module.val_dataloader()))\n",
    "                self.val_imgs, self.val_labels = (\n",
    "                    self.val_imgs[: self.num_bs],\n",
    "                    self.val_labels[: self.num_bs],\n",
    "                )\n",
    "                self.val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "\n",
    "            logits = pl_module(self.val_imgs)\n",
    "            preds = torch.argmax(logits, 1)\n",
    "            preds = preds.data.cpu()\n",
    "\n",
    "            ims = [\n",
    "                wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\")\n",
    "                for x, pred, y in zip(self.val_imgs, preds, self.val_labels)\n",
    "            ]\n",
    "            log_dict = {\"predictions\": ims}\n",
    "            wandb.log(ims, commit=False)\n",
    "\n",
    "    def on_epoch_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        pl_module.val_labels_list = []\n",
    "        pl_module.val_preds_list = []\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_conf_mat:\n",
    "            val_preds = torch.tensor(pl_module.val_preds_list).data.cpu().numpy()\n",
    "            val_labels = torch.tensor(pl_module.val_labels_list).data.cpu().numpy()\n",
    "            log_dict = {\n",
    "                \"conf_mat\": wandb.plot.confusion_matrix(\n",
    "                    val_preds, val_labels, self.class_names\n",
    "                )\n",
    "            }\n",
    "            wandb.log(log_dict, commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DisableValidationBar(pl.callbacks.ProgressBar):\n",
    "    \"Custom Progressbar callback for Lightning Training which disables the validation bar\"\n",
    "\n",
    "    def init_sanity_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for the validation sanity run. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Validation sanity check\",\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_train_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Training\",\n",
    "            initial=self.train_batch_idx,\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_validation_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for validation. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Validating\",\n",
    "            position=(2 * self.process_position + 1),\n",
    "            disable=True,\n",
    "            dynamic_ncols=False,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_test_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for testing. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Testing\",\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PrintLogsCallback(pl.Callback):\n",
    "    \"Logs Training logs to console after every epoch\"\n",
    "    TrainResult = namedtuple(\"TrainOutput\", [\"loss\", \"acc\", \"val_loss\", \"val_acc\"])\n",
    "    TestResult = namedtuple(\"TestOutput\", [\"test_loss\", \"test_acc\"])\n",
    "\n",
    "    logger = logging.getLogger(\"Trainer\")\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        train_loss = metrics[\"train/loss_epoch\"]\n",
    "        train_acc = metrics[\"train/acc_epoch\"]\n",
    "        valid_loss = metrics[\"valid/loss\"]\n",
    "        valid_acc = metrics[\"valid/acc\"]\n",
    "        trn_res = self.TrainResult(\n",
    "            round(train_loss.data.cpu().numpy().item(), 3),\n",
    "            round(train_acc.data.cpu().numpy().item(), 3),\n",
    "            round(valid_loss.data.cpu().numpy().item(), 3),\n",
    "            round(valid_acc.data.cpu().numpy().item(), 3),\n",
    "        )\n",
    "\n",
    "        curr_epoch = int(trainer.current_epoch)\n",
    "        self.logger.info(f\"EPOCH {curr_epoch}: {trn_res}\")\n",
    "\n",
    "    def on_test_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        metrics = trainer.callback_metrics\n",
    "        test_loss = metrics[\"test/loss\"]\n",
    "        test_acc = metrics[\"test/acc\"]\n",
    "        self.logger.info(\n",
    "            f\"{self.TestResult(round(test_loss.data.cpu().numpy().item(), 2), round(test_acc.data.cpu().numpy().item(), 2))}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "from torch import nn\n",
    "from src.networks import *\n",
    "\n",
    "from hydra.experimental import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overrides = [\n",
    "    \"image_dims=120\",\n",
    "    \"datamodule.bs=5\",\n",
    "    \"datamodule.num_workers=0\",\n",
    "    \"general=default\",\n",
    "    \"trainer=fast-dev-cpu\",\n",
    "    \"mixmethod=cutmix\",\n",
    "    \"network=transferlearning\"\n",
    "]\n",
    "\n",
    "with initialize(config_path=os.path.relpath(\"../conf/\")):\n",
    "    cfg = compose(config_name=\"effnet-base\", overrides=overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_augs = A.Compose([instantiate(a) for a in cfg.augmentations.train])\n",
    "val_augs = A.Compose([instantiate(a) for a in cfg.augmentations.valid])\n",
    "\n",
    "dm = instantiate(cfg.datamodule, train_augs=trn_augs, valid_augs=val_augs,)\n",
    "\n",
    "encoder = timm.create_model(cfg.encoder, pretrained=False)\n",
    "model = TransferLearningModel(encoder, cut=-2, c=5, act=nn.ReLU(inplace=True))\n",
    "model = LightningCassava(model=model, conf=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "[\u001b[36m2021-01-23 22:00:31,198\u001b[0m][\u001b[34mlightning\u001b[0m][\u001b[32mINFO\u001b[0m] - GPU available: False, used: False\u001b[0m\n",
      "TPU available: False, using: 0 TPU cores\n",
      "[\u001b[36m2021-01-23 22:00:31,199\u001b[0m][\u001b[34mlightning\u001b[0m][\u001b[32mINFO\u001b[0m] - TPU available: False, using: 0 TPU cores\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    callbacks=[DisableValidationBar(), PrintLogsCallback()],\n",
    "    num_sanity_val_steps=0,\n",
    "    max_epochs=2,\n",
    "    limit_train_batches=1,\n",
    "    limit_val_batches=1,\n",
    "    limit_test_batches=1,\n",
    "    weights_summary=None,\n",
    "    progress_bar_refresh_rate=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad2cf709a5e4adf91df7095fad3396f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2021-01-23 22:00:33,112\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - EPOCH 0: TrainOutput(loss=0.694, acc=0.4, val_loss=0.643, val_acc=0.0)\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:34,092\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - EPOCH 1: TrainOutput(loss=0.548, acc=0.4, val_loss=0.639, val_acc=0.0)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc377f5429a4b97a2b3c80d6e1a7cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2021-01-23 22:00:39,833\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - TestOutput(test_loss=0.64, test_acc=0.0)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_outputs = trainer.test(datamodule=dm, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DisableProgressBar(pl.callbacks.ProgressBar):\n",
    "    \"Custom Progressbar callback for Lightning Training which disables the validation bar\"\n",
    "\n",
    "    def init_sanity_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for the validation sanity run. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Validation sanity check\",\n",
    "            position=(2 * self.process_position),\n",
    "            disable=True,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_train_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Training\",\n",
    "            initial=self.train_batch_idx,\n",
    "            position=(2 * self.process_position),\n",
    "            disable=True,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_validation_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for validation. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Validating\",\n",
    "            position=(2 * self.process_position + 1),\n",
    "            disable=True,\n",
    "            dynamic_ncols=False,\n",
    "        )\n",
    "\n",
    "        return bar\n",
    "\n",
    "    def init_test_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for testing. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc=\"Testing\",\n",
    "            position=(2 * self.process_position),\n",
    "            disable=True,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ConsoleLogger(pl.Callback):\n",
    "    \"Fancy logger for console-logging\"\n",
    "    trn_res = namedtuple(\"TrainOutput\", [\"loss\", \"acc\", \"val_loss\", \"val_acc\"])\n",
    "    tst_res = namedtuple(\"TestOutput\", [\"test_loss\", \"test_acc\"])\n",
    "    curr_step = 0\n",
    "    has_init = False\n",
    "    logger = logging.getLogger(\"Trainer\")\n",
    "\n",
    "    def __init__(self, print_every: int = 50):\n",
    "        self.print_every = print_every\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        if not self.has_init:\n",
    "            cfg = pl_module.hparams\n",
    "\n",
    "            self.log_line()\n",
    "            self.log_msg(f\"Model:\")\n",
    "            self.log_msg(\n",
    "                f\" - model_class: {str(cfg.network.transfer_learning_model._target_).split('.')[-1]}\"\n",
    "            )\n",
    "            self.log_msg(f\" - base_model: {cfg.encoder}\")\n",
    "            summary = ModelSummary(pl_module)\n",
    "            self.log_msg(\n",
    "                f\" - total_parameters: {get_human_readable_count(summary.param_nums[0])}\"\n",
    "            )\n",
    "            self.log_line()\n",
    "\n",
    "            self.log_msg(f\"Dataset:\")\n",
    "            self.log_msg(f\" - path: {os.path.relpath(cfg.datamodule.im_dir)}\")\n",
    "            self.log_msg(f\" - validation_fold: {str(cfg.datamodule.curr_fold)}\")\n",
    "            self.log_msg(\n",
    "                f\" - {len(pl_module.train_dataloader())} train + {len(pl_module.val_dataloader())} valid + {len(pl_module.test_dataloader())} test batches\"\n",
    "            )\n",
    "            self.log_line()\n",
    "\n",
    "            self.log_msg(f\"Parameters:\")\n",
    "            self.log_msg(f\" - input_dimensions: {(cfg.image_dims, cfg.image_dims, 3)}\")\n",
    "            self.log_msg(f\" - max_epochs: {trainer.max_epochs}\")\n",
    "            self.log_msg(f\" - mini_batch_size: {str(cfg.datamodule.bs)}\")\n",
    "            self.log_msg(f\" - accumulate_batches: {trainer.accumulate_grad_batches}\")\n",
    "            self.log_msg(f\" - optimizer: {str(cfg.optimizer._target_).split('.')[-1]}\")\n",
    "            self.log_msg(f\" - learning_rates: {str(pl_module.lr_list)}\")\n",
    "            self.log_msg(f\" - weight_decay: {str(cfg.optimizer.weight_decay)}\")\n",
    "            self.log_msg(\n",
    "                f\" - lr scheduler: {str(cfg.scheduler.function._target_).split('.')[-1]}\"\n",
    "            )\n",
    "            self.log_msg(f\" - gradient_clipping: {trainer.gradient_clip_val}\")\n",
    "            self.log_msg(f\" - loss_function: {pl_module.loss_func}\")\n",
    "            self.log_msg(f\" - mix_method: {pl_module.mix_fn}\")\n",
    "\n",
    "            has_init = True\n",
    "\n",
    "        self.log_line()\n",
    "        self.log_msg(\"STAGE: TRAIN / VALIDATION\")\n",
    "        self.log_line()\n",
    "        self.log_msg(\n",
    "            f\"Model training base path: {os.path.relpath(trainer.checkpoint_callback.dirpath)}\"\n",
    "        )\n",
    "        self.log_line()\n",
    "        self.log_msg(f\"Device: {pl_module.device}\")\n",
    "\n",
    "    def on_epoch_start(self, *args, **kwargs):\n",
    "        self.log_line()\n",
    "\n",
    "    def on_train_epoch_start(self, *args, **kwargs):\n",
    "        # resets the current step\n",
    "        self.curr_step = 0\n",
    "        self.batch_time = 0\n",
    "        self.seen_batches = 0\n",
    "        self._stp_loss = 0\n",
    "        self._stp_acc = 0\n",
    "\n",
    "    def on_train_batch_start(self, *args, **kwargs):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        ep = trainer.current_epoch\n",
    "        tots = len(pl_module.train_dataloader())\n",
    "\n",
    "        mini_batch_size = pl_module.hparams.datamodule.bs\n",
    "\n",
    "        _stp_metrics = trainer.callback_metrics\n",
    "\n",
    "        self.seen_batches += 1\n",
    "        self.batch_time += time.time() - self.start_time\n",
    "\n",
    "        self._stp_loss += _stp_metrics[\"train/loss_step\"]\n",
    "        self._stp_acc += _stp_metrics[\"train/acc_step\"]\n",
    "\n",
    "        if self.curr_step % self.print_every == 0:\n",
    "            seen_samples = mini_batch_size * self.seen_batches\n",
    "            self.log_msg(\n",
    "                f\"epoch - {ep} - iter {self.curr_step}/{tots} - loss \"\n",
    "                f\"{self._stp_loss / self.seen_batches: .3f} - acc {self._stp_acc / self.seen_batches: .3f} \"\n",
    "                f\"- samples/sec: {seen_samples / self.batch_time: .2f}\"\n",
    "            )\n",
    "\n",
    "        self.curr_step += 1\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        metrics = trainer.callback_metrics\n",
    "\n",
    "        train_loss = metrics[\"train/loss_epoch\"]\n",
    "        train_acc = metrics[\"train/acc_epoch\"]\n",
    "\n",
    "        valid_loss = metrics[\"valid/loss\"]\n",
    "        valid_acc = metrics[\"valid/acc\"]\n",
    "\n",
    "        _res = self.trn_res(\n",
    "            round(train_loss.data.cpu().numpy().item(), 3),\n",
    "            round(train_acc.data.cpu().numpy().item(), 3),\n",
    "            round(valid_loss.data.cpu().numpy().item(), 3),\n",
    "            round(valid_acc.data.cpu().numpy().item(), 3),\n",
    "        )\n",
    "\n",
    "        curr_epoch = int(trainer.current_epoch)\n",
    "        self.log_line()\n",
    "        self.log_msg(f\"EPOCH {curr_epoch}: {_res}\")\n",
    "\n",
    "    def on_test_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        self.has_init = False\n",
    "        self.log_line()\n",
    "        self.log_msg(\"STAGE: TEST\")\n",
    "        self.log_line()\n",
    "        self.log_msg(\n",
    "            f\"Model testing base path: {os.path.relpath(trainer.checkpoint_callback.dirpath)}\"\n",
    "        )\n",
    "        self.log_line()\n",
    "        self.log_msg(f\"Device: {pl_module.device}\")\n",
    "        self.log_line()\n",
    "\n",
    "    def on_test_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        metrics = trainer.callback_metrics\n",
    "        test_loss = metrics[\"test/loss\"]\n",
    "        test_acc = metrics[\"test/acc\"]\n",
    "        self.log_msg(\n",
    "            f\"{self.tst_res(round(test_loss.data.cpu().numpy().item(), 2), round(test_acc.data.cpu().numpy().item(), 2))}\"\n",
    "        )\n",
    "\n",
    "    def on_fit_end(self, *args, **kwargs):\n",
    "        self.log_line()\n",
    "\n",
    "    def log_line(self):\n",
    "        self.logger.info(\"-\" * 70)\n",
    "\n",
    "    def log_msg(self, msg: str):\n",
    "        self.logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "[\u001b[36m2021-01-23 22:00:45,951\u001b[0m][\u001b[34mlightning\u001b[0m][\u001b[32mINFO\u001b[0m] - GPU available: False, used: False\u001b[0m\n",
      "TPU available: False, using: 0 TPU cores\n",
      "[\u001b[36m2021-01-23 22:00:45,952\u001b[0m][\u001b[34mlightning\u001b[0m][\u001b[32mINFO\u001b[0m] - TPU available: False, using: 0 TPU cores\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dm = instantiate(cfg.datamodule, train_augs=trn_augs, valid_augs=val_augs,)\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "\n",
    "encoder = timm.create_model(\"resnet18\", pretrained=False)\n",
    "model = TransferLearningModel(encoder, cut=-2, c=5, act=nn.ReLU(inplace=True))\n",
    "model = LightningCassava(model=model, conf=cfg)\n",
    "trainer = instantiate(\n",
    "    cfg.trainer,\n",
    "    callbacks=[DisableProgressBar(), ConsoleLogger(print_every=5)],\n",
    "    max_epochs=2,\n",
    "    limit_train_batches=20,\n",
    "    limit_val_batches=6,\n",
    "    limit_test_batches=6,\n",
    "    weights_summary=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2021-01-23 22:00:46,829\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,830\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - Model:\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,830\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - model_class: TransferLearningModel\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,831\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - base_model: tf_efficientnet_b0_ns\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,834\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - total_parameters: 11.7 M\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,835\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,835\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - Dataset:\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,837\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - path: ../../Datasets/cassava/train_images\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,838\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - validation_fold: 0\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,839\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - 3424 train + 856 valid + 856 test batches\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,839\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,840\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - Parameters:\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,841\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - input_dimensions: (120, 120, 3)\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,841\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - max_epochs: 2\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,842\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - mini_batch_size: 5\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,843\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - accumulate_batches: 1\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,844\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - optimizer: AdamW\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,845\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - learning_rates: Lrs(encoder_lr=0.0001, fc_lr=0.001)\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,846\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - weight_decay: 0.1\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,847\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - lr scheduler: LinearSchedulerWithWarmup\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,847\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - gradient_clipping: 0\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,848\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - loss_function: BiTemperedLogisticLoss()\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,849\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] -  - mix_method: Cutmix(probability=1.0, alpha=1.0, iters=100000000)\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,849\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,849\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - STAGE: TRAIN / VALIDATION\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,850\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,850\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - Model training base path: lightning_logs/version_14/checkpoints\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,851\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,851\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - Device: cpu\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:46,852\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:47,535\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - epoch - 0 - iter 0/3424 - loss  0.907 - acc  0.000 - samples/sec:  9.44\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:50,045\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - epoch - 0 - iter 5/3424 - loss  0.851 - acc  0.133 - samples/sec:  11.25\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:52,466\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - epoch - 0 - iter 10/3424 - loss  0.813 - acc  0.145 - samples/sec:  11.65\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:54,871\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - epoch - 0 - iter 15/3424 - loss  0.808 - acc  0.150 - samples/sec:  11.83\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:58,077\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:58,078\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - EPOCH 0: TrainOutput(loss=0.789, acc=0.16, val_loss=0.694, val_acc=0.1)\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:58,080\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:00:58,658\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - epoch - 1 - iter 0/3424 - loss  0.873 - acc  0.000 - samples/sec:  11.82\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:01:01,088\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - epoch - 1 - iter 5/3424 - loss  0.811 - acc  0.133 - samples/sec:  12.05\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:01:03,532\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - epoch - 1 - iter 10/3424 - loss  0.845 - acc  0.091 - samples/sec:  12.02\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:01:05,979\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - epoch - 1 - iter 15/3424 - loss  0.806 - acc  0.150 - samples/sec:  11.98\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:01:09,042\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:01:09,043\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - EPOCH 1: TrainOutput(loss=0.811, acc=0.14, val_loss=0.736, val_acc=0.1)\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:01:09,044\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2021-01-23 22:01:12,726\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:01:12,727\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - STAGE: TEST\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:01:12,727\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:01:12,728\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - Model testing base path: lightning_logs/version_14/checkpoints\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:01:12,729\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:01:12,729\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - Device: cpu\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:01:12,729\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:01:13,583\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - TestOutput(test_loss=0.74, test_acc=0.1)\u001b[0m\n",
      "[\u001b[36m2021-01-23 22:01:13,585\u001b[0m][\u001b[34mTrainer\u001b[0m][\u001b[32mINFO\u001b[0m] - ----------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = trainer.test(datamodule=dm, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
