{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp lightning.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import _logger as log\n",
    "\n",
    "from src.lightning.core import *\n",
    "from src.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class WandbImageClassificationCallback(pl.Callback):\n",
    "    \"\"\" Custom callback to add some extra functionalites to the wandb logger \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_batches:int = 16, \n",
    "                 log_train_batch: bool = False,\n",
    "                 log_preds: bool = False,\n",
    "                 log_conf_mat: bool = True,):\n",
    "        \n",
    "        # class names for the confusion matrix\n",
    "        self.class_names = list(conf_mat_idx2lbl.values())\n",
    "        \n",
    "        # counter to log training batch images\n",
    "        self.num_bs = num_batches\n",
    "        self.curr_epoch = 0\n",
    "        \n",
    "        self.log_train_batch = log_train_batch\n",
    "        self.log_preds = log_preds\n",
    "        self.log_conf_mat = log_conf_mat\n",
    "        \n",
    "        self.val_imgs, self.val_labels = None, None\n",
    "        \n",
    "    def on_train_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        try:\n",
    "            # log model to the wandb experiment\n",
    "            wandb.watch(models=pl_module.model, criterion=pl_module.loss_func)\n",
    "        except:\n",
    "            log.info(\"Skipping wandb.watch --->\")\n",
    "        \n",
    "    def on_train_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_train_batch:\n",
    "            if pl_module.one_batch is None:\n",
    "                log.info(f\"{self.config_defaults['mixmethod']} samples not available . Skipping --->\")\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                one_batch = pl_module.one_batch[:self.num_bs]\n",
    "                train_ims = one_batch.data.to('cpu')\n",
    "                trainer.logger.experiment.log({\"train_batch\":[wandb.Image(x) for x in train_ims]}, commit=False)\n",
    "        \n",
    "    def on_validation_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_preds:\n",
    "            if self.val_imgs is None and self.val_labels is None:\n",
    "                self.val_imgs, self.val_labels = next(iter(pl_module.val_dataloader()))\n",
    "                self.val_imgs, self.val_labels = self.val_imgs[:self.num_bs], self.val_labels[:self.num_bs]\n",
    "                self.val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "\n",
    "            logits = pl_module(self.val_imgs)\n",
    "            preds  = torch.argmax(logits, 1)\n",
    "            preds  = preds.data.cpu()\n",
    "            \n",
    "            ims = [wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\") for x,pred,y in zip(self.val_imgs,preds,self.val_labels)]\n",
    "            log_dict = {\"predictions\": ims}\n",
    "            wandb.log(ims,commit=False)\n",
    "            \n",
    "    def on_epoch_start(self, trainer, pl_module, *args, **kwargs):\n",
    "        pl_module.val_labels_list = []\n",
    "        pl_module.val_preds_list  = []\n",
    "    \n",
    "    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        if self.log_conf_mat:\n",
    "            val_preds  = torch.tensor(pl_module.val_preds_list).data.cpu().numpy()\n",
    "            val_labels = torch.tensor(pl_module.val_labels_list).data.cpu().numpy()\n",
    "            log_dict = {'conf_mat': wandb.plot.confusion_matrix(val_preds,val_labels,self.class_names)}\n",
    "            wandb.log(log_dict,commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LitProgressBar(pl.callbacks.ProgressBar):\n",
    "    \"Custom Progressbar callback for Lightning Training\"\n",
    "    \n",
    "    def init_sanity_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for the validation sanity run. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Validation sanity check',\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            leave=False,\n",
    "            dynamic_ncols=True,)\n",
    "        \n",
    "        return bar\n",
    "    \n",
    "    def init_train_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Training',\n",
    "            initial=self.train_batch_idx,\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            leave=False,\n",
    "            dynamic_ncols=True,)\n",
    "        \n",
    "        return bar\n",
    "    \n",
    "    def init_validation_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for validation. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Validating',\n",
    "            position=(2 * self.process_position + 1),\n",
    "            disable=True,\n",
    "            leave=False,\n",
    "            dynamic_ncols=False,)\n",
    "        \n",
    "        return bar\n",
    "    \n",
    "    def init_test_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for testing. \"\"\"\n",
    "        bar = tqdm(\n",
    "            desc='Testing',\n",
    "            position=(2 * self.process_position),\n",
    "            disable=self.is_disabled,\n",
    "            leave=False,\n",
    "            dynamic_ncols=True,)\n",
    "        \n",
    "        return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PrintLogsCallback(pl.Callback):\n",
    "    \"Logs Training logs to console after every epoch\"\n",
    "    def __init__(self, print_str: str = None):\n",
    "        self.print_str = 'eta: {} loss: {:.4f} acc: {:.4f} valid_loss: {:.4f} valid_acc: {:.4f}'\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def on_epoch_start(self, *args, **kwargs):\n",
    "        self.eta_start = time.time()\n",
    "    \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        train_loss = metrics['train/loss_epoch']\n",
    "        train_acc  = metrics['train/acc_epoch']\n",
    "        valid_loss = metrics['valid/loss']\n",
    "        valid_acc  = metrics['valid/acc']\n",
    "        \n",
    "        end_time = time.time()\n",
    "        self.eta_string = str(datetime.timedelta(seconds=int(end_time-self.eta_start)))\n",
    "        self.curr_epoch = int(trainer.current_epoch)\n",
    "        print_str = self.print_str.format(self.eta_string, train_loss, train_acc, valid_loss, valid_acc)\n",
    "        \n",
    "        self.logger.info(f\"Epoch {self.curr_epoch} \")\n",
    "        self.logger.info(print_str)\n",
    "    \n",
    "    def on_test_epoch_end(self, trainer, pl_module, *args, **kwargs):\n",
    "        metrics = trainer.callback_metrics\n",
    "        train_loss = metrics['train/loss']\n",
    "        train_acc  = metrics['train/acc']\n",
    "        valid_loss = metrics['valid/loss']\n",
    "        valid_acc  = metrics['valid/acc']\n",
    "        test_loss  = metrics['test/loss']\n",
    "        test_acc   = metrics['test/acc']\n",
    "        \n",
    "        \n",
    "        fmt_str1 = \"Summary: [Train] loss: {:.4f} acc: {:.4f}\"\n",
    "        fmt_str2 = \"Summary: [Valid] loss: {:.4f} acc: {:.4f}\"\n",
    "        fmt_str3 = \"Summary: [Test]  loss: {:.4f} acc: {:.4f}\"\n",
    "        \n",
    "        str1 = fmt_str1.format(train_loss, train_acc)\n",
    "        str2 = fmt_str2.format(valid_loss, valid_acc)\n",
    "        str3 = fmt_str3.format(test_loss, test_acc)\n",
    "        \n",
    "        self.logger.info(\"Finished !\")\n",
    "        self.logger.info(str1)\n",
    "        self.logger.info(str2)\n",
    "        self.logger.info(str3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "from torch import nn\n",
    "from src.networks import *\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2021 13:15:56 - INFO - Loss Function : LabelSmoothingCrossEntropy()\n"
     ]
    }
   ],
   "source": [
    "train_augs = A.Compose([\n",
    "    A.RandomResizedCrop(224, 224, p=1.0),\n",
    "    A.RandomBrightness(limit=0.1),\n",
    "    A.HueSaturationValue(20, 20, 20),\n",
    "    A.HorizontalFlip(),\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensorV2(p=1.0)])\n",
    "\n",
    "valid_augs = A.Compose([\n",
    "    A.Resize(224, 224, p=1.0),\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensorV2(p=1.0)])\n",
    "\n",
    "csv = \"../../leaf-disease-classification-kaggle/data/stratified-data-5folds.csv\"\n",
    "ims = \"../../Datasets/cassava/train_images/\"\n",
    "dm = CassavaLightningDataModule(csv, ims, curr_fold=0, train_augs=train_augs, valid_augs=valid_augs, bs=8, num_workers=0)\n",
    "\n",
    "\n",
    "model_hparams = dict(\n",
    "    mixmethod        = None,\n",
    "    loss             = dict(_target_='src.losses.LabelSmoothingCrossEntropy', eps=0.1),\n",
    "    learning_rate    = 1e-03,\n",
    "    lr_mult          = 100,\n",
    "    optimizer        = dict(_target_='src.opts.Ranger', weight_decay=1e-02, betas=[0.95, 0.999], eps=1e-05),\n",
    "    scheduler        = dict(function=dict(_target_='src.opts.FlatCos', num_epochs=10, pct_start=0.7), \n",
    "                            metric_to_track=None, scheduler_interval='step'),\n",
    ")\n",
    "\n",
    "OmegaConf.create(model_hparams)\n",
    "\n",
    "\n",
    "\n",
    "encoder = timm.create_model('resnet18', pretrained=False)\n",
    "model   = TransferLearningModel(encoder, cut=-2, c=5, act=nn.ReLU(inplace=True))\n",
    "model   = LightningCassava(model=model, conf=model_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "01/06/2021 13:15:58 - INFO - GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "01/06/2021 13:15:58 - INFO - TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(callbacks=[LitProgressBar(), PrintLogsCallback()], \n",
    "                     num_sanity_val_steps=0, max_epochs=2, limit_train_batches=1, \n",
    "                     limit_val_batches=1, limit_test_batches=1, weights_summary=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2021 13:15:59 - INFO - DATA: ../../Datasets/cassava/train_images/\n",
      "01/06/2021 13:15:59 - INFO - FOLD: 0  BATCH_SIZE: 8\n",
      "01/06/2021 13:15:59 - INFO - Optimizer: Ranger  LR's: (1e-05, 0.001)\n",
      "01/06/2021 13:15:59 - INFO - LR Scheculer: FlatCos\n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:02<00:00,  1.62s/it, loss=1.405, v_num=10]01/06/2021 13:16:02 - INFO - Epoch 0 \n",
      "01/06/2021 13:16:02 - INFO - eta: 0:00:02 loss: 1.4050 acc: 0.2500 valid_loss: 1.6109 valid_acc: 0.2500\n",
      "Epoch 1: 100%|██████████| 2/2 [00:02<00:00,  1.60s/it, loss=1.622, v_num=10]01/06/2021 13:16:04 - INFO - Epoch 1 \n",
      "01/06/2021 13:16:04 - INFO - eta: 0:00:02 loss: 1.8385 acc: 0.0000 valid_loss: 1.6056 valid_acc: 0.1250\n",
      "                                                                            \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]01/06/2021 13:16:10 - INFO - Finished !\n",
      "01/06/2021 13:16:10 - INFO - Summary: [Train] loss: 1.8385 acc: 0.0000\n",
      "01/06/2021 13:16:10 - INFO - Summary: [Valid] loss: 1.6056 acc: 0.1250\n",
      "01/06/2021 13:16:10 - INFO - Summary: [Test]  loss: 1.6056 acc: 0.1250\n",
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test/acc': tensor(0.1250),\n",
      " 'test/loss': tensor(1.6056),\n",
      " 'train/acc': tensor(0.),\n",
      " 'train/acc_epoch': tensor(0.),\n",
      " 'train/acc_step': tensor(0.),\n",
      " 'train/loss': tensor(1.8385),\n",
      " 'train/loss_epoch': tensor(1.8385),\n",
      " 'train/loss_step': tensor(1.8385),\n",
      " 'valid/acc': tensor(0.1250),\n",
      " 'valid/loss': tensor(1.6056)}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "_ = trainer.test(model, datamodule=dm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  2.07it/s]01/06/2021 13:16:28 - INFO - Finished !\n",
      "01/06/2021 13:16:28 - INFO - Summary: [Train] loss: 1.8385 acc: 0.0000\n",
      "01/06/2021 13:16:28 - INFO - Summary: [Valid] loss: 1.6056 acc: 0.1250\n",
      "01/06/2021 13:16:28 - INFO - Summary: [Test]  loss: 1.6056 acc: 0.1250\n",
      "                                                      \r"
     ]
    }
   ],
   "source": [
    "_ = trainer.test(model, datamodule=dm, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_mixmethods.ipynb.\n",
      "Converted 02_losses.ipynb.\n",
      "Converted 03_layers.ipynb.\n",
      "Converted 03a_networks.ipynb.\n",
      "Converted 04_optimizers_schedules.ipynb.\n",
      "Converted 05_lightning.core.ipynb.\n",
      "Converted 05a_lightning.callbacks.ipynb.\n",
      "Converted 06_fastai.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
