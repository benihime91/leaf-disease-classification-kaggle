{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from fvcore.common import registry\n",
    "from omegaconf import DictConfig\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "from src import _logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Optimizer Registery -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "OPTIM_REGISTERY = registry.Registry(\"Optimizers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# register all default optimizers\n",
    "OPTIM_REGISTERY.register(torch.optim.SGD)\n",
    "OPTIM_REGISTERY.register(torch.optim.RMSprop)\n",
    "OPTIM_REGISTERY.register(torch.optim.Adam)\n",
    "OPTIM_REGISTERY.register(torch.optim.AdamW)\n",
    "OPTIM_REGISTERY.register(torch.optim.Adamax)\n",
    "OPTIM_REGISTERY.register(torch.optim.Adadelta)\n",
    "OPTIM_REGISTERY.register(torch.optim.Adagrad)\n",
    "OPTIM_REGISTERY.register(torch.optim.SparseAdam)\n",
    "OPTIM_REGISTERY.register(torch.optim.ASGD)\n",
    "OPTIM_REGISTERY.register(torch.optim.LBFGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranger Optimizer - \n",
    "> from https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger2020.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@OPTIM_REGISTERY.register()\n",
    "class Ranger(Optimizer):\n",
    "    \"from - https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py\"\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, k=6, N_sma_threshhold=5,\n",
    "                 betas=(.95, 0.999), eps=1e-5, weight_decay=0,\n",
    "                 use_gc=True, gc_conv_only=False):\n",
    "\n",
    "        # parameter checks\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "        if not lr > 0:\n",
    "            raise ValueError(f'Invalid Learning Rate: {lr}')\n",
    "        if not eps > 0:\n",
    "            raise ValueError(f'Invalid eps: {eps}')\n",
    "\n",
    "        # parameter comments:\n",
    "        # beta1 (momentum) of .95 seems to work better than .90...\n",
    "        # N_sma_threshold of 5 seems better in testing than 4.\n",
    "        # In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n",
    "\n",
    "        # prep defaults and init torch.optim base\n",
    "        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas,\n",
    "                        N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "        # adjustable threshold\n",
    "        self.N_sma_threshhold = N_sma_threshhold\n",
    "\n",
    "        # look ahead params\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "        # radam buffer for state\n",
    "        self.radam_buffer = [[None, None, None] for ind in range(10)]\n",
    "\n",
    "        # gc on or off\n",
    "        self.use_gc = use_gc\n",
    "\n",
    "        # level of gradient centralization\n",
    "        self.gc_gradient_threshold = 3 if gc_conv_only else 1\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Ranger, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        \n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        # Evaluate averages and grad, update param tensors\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        'Ranger optimizer does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]  # get state dict for this param\n",
    "\n",
    "                if len(state) == 0:  # if first time to run...init dictionary with our desired entries\n",
    "                    # if self.first_run_check==0:\n",
    "                    # self.first_run_check=1\n",
    "                    #print(\"Initializing slow buffer...should not see this at load from saved model!\")\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "\n",
    "                    # look ahead weight storage now in state dict\n",
    "                    state['slow_buffer'] = torch.empty_like(p.data)\n",
    "                    state['slow_buffer'].copy_(p.data)\n",
    "\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n",
    "                        p_data_fp32)\n",
    "\n",
    "                # begin computations\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                # GC operation for Conv layers and FC layers\n",
    "                if grad.dim() > self.gc_gradient_threshold:\n",
    "                    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # compute variance mov avg\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                # compute mean moving avg\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                buffered = self.radam_buffer[int(state['step'] % 10)]\n",
    "\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * \\\n",
    "                        state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "                    if N_sma > self.N_sma_threshhold:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (\n",
    "                            N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay']\n",
    "                                     * group['lr'], p_data_fp32)\n",
    "\n",
    "                # apply lr\n",
    "                if N_sma > self.N_sma_threshhold:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size *\n",
    "                                         group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "                # integrated look ahead...\n",
    "                # we do it at the param level instead of group level\n",
    "                if state['step'] % group['k'] == 0:\n",
    "                    # get access to slow param tensor\n",
    "                    slow_p = state['slow_buffer']\n",
    "                    # (fast weights - slow weights) * alpha\n",
    "                    slow_p.add_(self.alpha, p.data - slow_p)\n",
    "                    # copy interpolated weights to RAdam param tensor\n",
    "                    p.data.copy_(slow_p)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registry of Optimizers:\n",
      "╒════════════╤══════════════════════════════════════════════╕\n",
      "│ Names      │ Objects                                      │\n",
      "╞════════════╪══════════════════════════════════════════════╡\n",
      "│ SGD        │ <class 'torch.optim.sgd.SGD'>                │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ RMSprop    │ <class 'torch.optim.rmsprop.RMSprop'>        │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ Adam       │ <class 'torch.optim.adam.Adam'>              │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ AdamW      │ <class 'torch.optim.adamw.AdamW'>            │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ Adamax     │ <class 'torch.optim.adamax.Adamax'>          │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ Adadelta   │ <class 'torch.optim.adadelta.Adadelta'>      │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ Adagrad    │ <class 'torch.optim.adagrad.Adagrad'>        │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ SparseAdam │ <class 'torch.optim.sparse_adam.SparseAdam'> │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ ASGD       │ <class 'torch.optim.asgd.ASGD'>              │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ LBFGS      │ <class 'torch.optim.lbfgs.LBFGS'>            │\n",
      "├────────────┼──────────────────────────────────────────────┤\n",
      "│ Ranger     │ <class '__main__.Ranger'>                    │\n",
      "╘════════════╧══════════════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "print(OPTIM_REGISTERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_optimizer(cfg: DictConfig, params):\n",
    "    \"loaded a optimizer in OPTIM_REGISTERY from `cfg`\"\n",
    "    optimizer = OPTIM_REGISTERY.get(cfg.name)(params=params, **cfg.params)\n",
    "    _logger.info(f\"{cfg.name} loaded from OPTIM_REGISTERY\")\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "model  = torch.nn.Linear(24, 2, bias=True)\n",
    "params = model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[32m01/31 20:33:46\u001b[0m \u001b[35m__main__\u001b[0m]: Ranger loaded from OPTIM_REGISTERY\n"
     ]
    }
   ],
   "source": [
    "# cfg = OmegaConf.load(\"../conf/optimizer/adam.yaml\")\n",
    "cfg = OmegaConf.load(\"../conf/optimizer/ranger.yaml\")\n",
    "# cfg = OmegaConf.load(\"../conf/optimizer/adamw.yaml\")\n",
    "# cfg = OmegaConf.load(\"../conf/optimizer/sgd.yaml\")\n",
    "\n",
    "optimizer = create_optimizer(cfg, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01a_data.datasets.ipynb.\n",
      "Converted 01b_data.datasests_factory.ipynb.\n",
      "Converted 01c_data.mixmethods.ipynb.\n",
      "Converted 02_losses.ipynb.\n",
      "Converted 03a_optimizers.ipynb.\n",
      "Converted 03b_schedulers.ipynb.\n",
      "Converted 04a_models.utils.ipynb.\n",
      "Converted 04b_models.layers.ipynb.\n",
      "Converted 04c_models.classifiers.ipynb.\n",
      "Converted 04d_models.builder.ipynb.\n",
      "Converted 04e_models.task.ipynb.\n",
      "Converted 05_callbacks.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
