{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom List of Lightning Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import wandb\n",
    "from pytorch_lightning import Callback, Trainer\n",
    "from timm.utils import AverageMeter\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "from src import _logger\n",
    "from src.core import conf_mat_idx2lbl, idx2lbl\n",
    "from src.models import Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Wandb Callback -\n",
    "> meant to be used in conjunction with `pl.loggers.WandbLogger`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class WandbTask(Callback):\n",
    "    \"\"\" Custom callback to add some extra functionalites to the wandb logger \n",
    "    Does the following:\n",
    "        1. Logs the model graph to wandb.\n",
    "        2. Logs confusion matrix of preds/labels for each validation epoch.\n",
    "        3. Logs confusion matrix of preds/labels after testing.\n",
    "    \"\"\"\n",
    "    class_names = list(conf_mat_idx2lbl.values())\n",
    "    \n",
    "    def on_train_start(self, trainer: Trainer, pl_module: Task, *args, **kwrags) -> None:\n",
    "        try   : wandb.watch(models=pl_module.model, criterion=pl_module.criterion)\n",
    "        except: pass\n",
    "        \n",
    "    def on_validation_epoch_start(self, trainer: Trainer, pl_module: Task, *args, **kwrags) -> None:\n",
    "        self.labels, self.predictions = [], []\n",
    "    \n",
    "    def on_validation_batch_end(self, trainer: Trainer, pl_module: Task, *args, **kwrags) -> None:\n",
    "        self.labels = self.labels + pl_module.labels\n",
    "        self.predictions = self.predictions + pl_module.preds\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer: Trainer, pl_module: Task, *args, **kwrags) -> None:\n",
    "        preds   = torch.tensor(self.predictions).data.cpu().numpy()\n",
    "        vlabels = torch.tensor(self.labels).data.cpu().numpy()\n",
    "        \n",
    "        matrix = wandb.plot.confusion_matrix(preds, labels, self.class_names)\n",
    "        wandb.log(dict(valid_confusion_matrix=matrix), commit=False)\n",
    "        \n",
    "    def on_test_epoch_start(self, trainer: Trainer, pl_module: Task, *args, **kwrags) -> None:\n",
    "        self.labels, self.predictions = [], []\n",
    "        \n",
    "    def on_test_batch_end(self, trainer: Trainer, pl_module: Task, *args, **kwrags) -> None:\n",
    "        self.labels = self.labels + pl_module.labels\n",
    "        self.predictions = self.predictions + pl_module.preds\n",
    "        \n",
    "    def on_test_epoch_end(self, trainer: Trainer, pl_module: Task, *args, **kwrags) -> None:\n",
    "        preds   = torch.tensor(self.predictions).data.cpu().numpy()\n",
    "        vlabels = torch.tensor(self.labels).data.cpu().numpy()\n",
    "        \n",
    "        matrix = wandb.plot.confusion_matrix(preds, labels, self.class_names)\n",
    "        wandb.log(dict(test_confusion_matrix=matrix), commit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Progress Bar Callback -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DisableValidationBar(pl.callbacks.ProgressBar):\n",
    "    \"Custom Progressbar callback for Lightning Training which disables the validation bar\"\n",
    "\n",
    "    def init_sanity_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for the validation sanity run. \"\"\"\n",
    "        bar = tqdm(desc=\"Validation sanity check\", dynamic_ncols=True,)\n",
    "        return bar\n",
    "\n",
    "    def init_train_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\n",
    "        bar = tqdm(desc=\"Training\", disable=self.is_disabled, dynamic_ncols=True,)\n",
    "        return bar\n",
    "\n",
    "    def init_validation_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for validation. \"\"\"\n",
    "        bar = tqdm(desc=\"Validating\", disable=True, dynamic_ncols=False,)\n",
    "        return bar\n",
    "\n",
    "    def init_test_tqdm(self) -> tqdm:\n",
    "        \"\"\" Override this to customize the tqdm bar for testing. \"\"\"\n",
    "        bar = tqdm(desc=\"Testing\", disable=self.is_disabled, dynamic_ncols=True,)\n",
    "        return bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Logger Callback -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LogInformationCallback(pl.Callback):\n",
    "    \"Logs Training loss/metric to console after every epoch\"\n",
    "    TrainResult = namedtuple(\"TrainOutput\", [\"loss\", \"acc\", \"val_loss\", \"val_acc\"])\n",
    "    TestResult  = namedtuple(\"TestOutput\",  [\"test_loss\", \"test_acc\"])\n",
    "    \n",
    "    def on_train_epoch_start(self, trainer: Trainer, pl_module: Task, *args, **kwrags) -> None:\n",
    "        self.batch_time=  AverageMeter()\n",
    "        self.end = time.time()\n",
    "        \n",
    "    def on_train_batch_end(self, trainer: Trainer, pl_module: Task, *args, **kwrags) -> None:\n",
    "        self.batch_time.update(time.time() - self.end)\n",
    "\n",
    "    def on_epoch_end(self, trainer: Trainer, pl_module: Task, *args, **kwrags) -> None:\n",
    "        metrics = copy.copy(trainer.callback_metrics)\n",
    "        \n",
    "        train_loss = metrics[\"train/loss_epoch\"]\n",
    "        train_acc  = metrics[\"train/acc_epoch\"]\n",
    "        valid_loss = metrics[\"valid/loss\"]\n",
    "        valid_acc  = metrics[\"valid/acc\"]\n",
    "        \n",
    "        res = self.TrainResult(\n",
    "            round(train_loss.data.cpu().numpy().item(), 4),\n",
    "            round(train_acc.data.cpu().numpy().item(),  4),\n",
    "            round(valid_loss.data.cpu().numpy().item(), 4),\n",
    "            round(valid_acc.data.cpu().numpy().item(),  4),\n",
    "        )\n",
    "\n",
    "        curr_epoch = int(pl_module.current_epoch)\n",
    "        total_epoch = int(trainer.max_epochs)\n",
    "        _logger.info(f\"Train: [ {curr_epoch}/{total_epoch}] Time: {self.batch_time.val:.3f} ({self.batch_time.avg:.3f}) {res}\")\n",
    "        \n",
    "    def on_test_epoch_start(self, trainer: Trainer, pl_module: Task, *args, **kwrags) -> None:\n",
    "        self.batch_time = AverageMeter()\n",
    "        self.end = time.time()\n",
    "        \n",
    "    def on_test_batch_end(self, trainer: Trainer, pl_module: Task, *args, **kwrags) -> None:\n",
    "        self.batch_time.update(time.time() - self.end)\n",
    "\n",
    "    def on_test_epoch_end(self, trainer: Trainer, pl_module: Task, *args, **kwrags) -> None:\n",
    "        metrics = trainer.callback_metrics\n",
    "        \n",
    "        test_loss = metrics[\"test/loss\"]\n",
    "        test_acc  = metrics[\"test/acc\"]\n",
    "        \n",
    "        res = self.TestResult(\n",
    "            round(test_loss.data.cpu().numpy().item(), 4), \n",
    "            round(test_acc.data.cpu().numpy().item(),  4))\n",
    "        \n",
    "        _logger.info(f\"Test: Time: {self.batch_time.val:.3f} ({self.batch_time.avg:.3f}) {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hydra.experimental import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "from pl_bolts.callbacks import BatchGradientVerificationCallback, TrainingDataMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overrides = [\n",
    "    \"input.input_size=120\",\n",
    "    \"training.batch_size=5\",\n",
    "    \"augmentations=tfms-v0\",\n",
    "    \"data.dataloader.num_workers=0\",\n",
    "    \"general=default\",\n",
    "    \"trainer=fast-dev-cpu\",\n",
    "    \"mixmethod=snapmix\",\n",
    "    \"training.mix_epochs=3\",\n",
    "    \"training.batch_size=64\",\n",
    "    \"model=v0\",\n",
    "    \"model.base_model.activation=mish\",\n",
    "    \"model.head.params.act_layer=mish\",\n",
    "    \"training.accumulate_grad_batches=1\",\n",
    "    \"loss=crossentropy\",\n",
    "]\n",
    "\n",
    "with initialize(config_path=os.path.relpath(\"../conf/\")):\n",
    "    cfg = compose(config_name=\"effnet-base\", overrides=overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[32m01/31 20:56:16\u001b[0m \u001b[35msrc.models.builder\u001b[0m]: Configuration for the current model :\n",
      "[\u001b[32m01/31 20:56:16\u001b[0m \u001b[35msrc.models.builder\u001b[0m]:  feature_extractor: tf_efficientnet_b3_ns\n",
      "[\u001b[32m01/31 20:56:16\u001b[0m \u001b[35msrc.models.builder\u001b[0m]:  activation: mish\n",
      "[\u001b[32m01/31 20:56:16\u001b[0m \u001b[35msrc.models.builder\u001b[0m]:  params: {'drop_path_rate': 0.25}\n",
      "[\u001b[32m01/31 20:56:16\u001b[0m \u001b[35msrc.models.builder\u001b[0m]:  head: CnnHeadV0\n",
      "[\u001b[32m01/31 20:56:16\u001b[0m \u001b[35msrc.models.builder\u001b[0m]:  params: {'n_out': 5, 'pool_type': 'avg', 'use_conv': False, 'act_layer': 'mish'}\n",
      "[\u001b[32m01/31 20:56:17\u001b[0m \u001b[35msrc.models.task\u001b[0m]: LossFunction: CrossEntropyLoss()\n",
      "[\u001b[32m01/31 20:56:17\u001b[0m \u001b[35msrc.models.task\u001b[0m]: Training with Snapmix(alpha=5.0, conf_prob=0.5, num_iters=3).\n",
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "[\u001b[32m01/31 20:56:17\u001b[0m \u001b[35msrc.data.dataset_factory\u001b[0m]: Generating Datasets for FOLD :0\n",
      "[\u001b[32m01/31 20:56:18\u001b[0m \u001b[35msrc.data.dataset_factory\u001b[0m]: Train Dataset has 17117, Validation Dataset has 4280 instances.\n",
      "[\u001b[32m01/31 20:56:18\u001b[0m \u001b[35msrc.optimizers\u001b[0m]: Ranger loaded from OPTIM_REGISTERY\n",
      "[\u001b[32m01/31 20:56:18\u001b[0m \u001b[35msrc.schedulers\u001b[0m]: FlatCosScheduler loaded from SCHEDULER_REGISTERY\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | trn_metric | Accuracy         | 0     \n",
      "1 | val_metric | Accuracy         | 0     \n",
      "2 | tst_metric | Accuracy         | 0     \n",
      "3 | model      | Net              | 10.7 M\n",
      "4 | criterion  | CrossEntropyLoss | 0     \n",
      "------------------------------------------------\n",
      "10.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.7 M    Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bae9e4c8b4a45caa55f5703ca39c77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[32m01/31 20:58:56\u001b[0m \u001b[35m__main__\u001b[0m]: Train: [ 0/8] Time: 138.052 (75.587) TrainOutput(loss=1.7081, acc=0.4422, val_loss=1.6205, val_acc=0.0938)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Task(cfg)\n",
    "\n",
    "cbs = [LogInformationCallback(), \n",
    "       DisableValidationBar(), \n",
    "       pl.callbacks.LearningRateMonitor(\"step\"),\n",
    "       TrainingDataMonitor()]\n",
    "\n",
    "trainer = instantiate(cfg.trainer, callbacks=cbs, max_epochs=8, \n",
    "                      limit_train_batches=10, limit_val_batches=5, \n",
    "                      limit_test_batches=5, weights_summary=\"top\", \n",
    "                      profiler=False, terminate_on_nan=True)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[32m01/31 20:59:58\u001b[0m \u001b[35msrc.data.dataset_factory\u001b[0m]: Generating Datasets for FOLD :0\n",
      "[\u001b[32m01/31 20:59:59\u001b[0m \u001b[35msrc.data.dataset_factory\u001b[0m]: Train Dataset has 17117, Validation Dataset has 4280 instances.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63391f8d29746b98ce9e490e9dade7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[32m01/31 21:00:19\u001b[0m \u001b[35m__main__\u001b[0m]: Test: Time: 18.367 (11.300) TestOutput(test_loss=1.6166, test_acc=0.1094)\n"
     ]
    }
   ],
   "source": [
    "_ = trainer.test(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01a_data.datasets.ipynb.\n",
      "Converted 01b_data.datasests_factory.ipynb.\n",
      "Converted 01c_data.mixmethods.ipynb.\n",
      "Converted 02_losses.ipynb.\n",
      "Converted 03a_optimizers.ipynb.\n",
      "Converted 03b_schedulers.ipynb.\n",
      "Converted 04a_models.utils.ipynb.\n",
      "Converted 04b_models.layers.ipynb.\n",
      "Converted 04c_models.classifiers.ipynb.\n",
      "Converted 04d_models.builder.ipynb.\n",
      "Converted 04e_models.task.ipynb.\n",
      "Converted 05_callbacks.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
