{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _WeightedLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LabelSmoothingCrossEntropy(_WeightedLoss):\n",
    "    \"label smoothing loss\"\n",
    "\n",
    "    def __init__(self, eps: float = 0.1, reduction='mean', weight=None):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.eps, self.reduction = eps, reduction\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[1]\n",
    "        log_preds = F.log_softmax(output, dim=1)\n",
    "        if self.reduction == 'sum':\n",
    "            loss = -log_preds.sum()\n",
    "        else:\n",
    "            # We divide by that size at the return line so sum and not mean\n",
    "            loss = -log_preds.sum(dim=1)\n",
    "            if self.reduction == 'mean':\n",
    "                loss = loss.mean()\n",
    "        return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2480)\n"
     ]
    }
   ],
   "source": [
    "output = torch.tensor([[0.1, 0.2, 0.5], [0.3, 0.4, 0.0]])\n",
    "target = torch.tensor([1, 2])\n",
    "loss = LabelSmoothingCrossEntropy()(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "###########################################################################\n",
    "# From : https://github.com/mlpanda/bi-tempered-loss-pytorch/blob/master/bi_tempered_loss.py\n",
    "###########################################################################\n",
    "def log_t(u, t):\n",
    "    \"\"\"Compute log_t for `u`.\"\"\"\n",
    "\n",
    "    if t == 1.0:\n",
    "        return torch.log(u)\n",
    "    else:\n",
    "        return (u ** (1.0 - t) - 1.0) / (1.0 - t)\n",
    "\n",
    "\n",
    "def exp_t(u, t):\n",
    "    \"\"\"Compute exp_t for `u`.\"\"\"\n",
    "\n",
    "    if t == 1.0:\n",
    "        return torch.exp(u)\n",
    "    else:\n",
    "        return torch.relu(1.0 + (1.0 - t) * u) ** (1.0 / (1.0 - t))\n",
    "\n",
    "\n",
    "def compute_normalization_fixed_point(activations, t, num_iters=5):\n",
    "    \"\"\"Returns the normalization value for each example (t > 1.0).\n",
    "    Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature 2 (> 1.0 for tail heaviness).\n",
    "    num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "\n",
    "    mu = torch.max(activations, dim=-1).values.view(-1, 1)\n",
    "    normalized_activations_step_0 = activations - mu\n",
    "\n",
    "    normalized_activations = normalized_activations_step_0\n",
    "    i = 0\n",
    "    while i < num_iters:\n",
    "        i += 1\n",
    "        logt_partition = torch.sum(\n",
    "            exp_t(normalized_activations, t), dim=-1).view(-1, 1)\n",
    "        normalized_activations = normalized_activations_step_0 * \\\n",
    "            (logt_partition ** (1.0 - t))\n",
    "\n",
    "    logt_partition = torch.sum(\n",
    "        exp_t(normalized_activations, t), dim=-1).view(-1, 1)\n",
    "\n",
    "    return -log_t(1.0 / logt_partition, t) + mu\n",
    "\n",
    "\n",
    "def compute_normalization(activations, t, num_iters=5):\n",
    "    \"\"\"Returns the normalization value for each example.\n",
    "    Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature 2 (< 1.0 for finite support, > 1.0 for tail heaviness).\n",
    "    num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "\n",
    "    if t < 1.0:\n",
    "        # not implemented as these values do not occur in the authors experiments...\n",
    "        return None\n",
    "    else:\n",
    "        return compute_normalization_fixed_point(activations, t, num_iters)\n",
    "\n",
    "\n",
    "def tempered_softmax(activations, t, num_iters=5):\n",
    "    \"\"\"Tempered softmax function.\n",
    "    Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature tensor > 0.0.\n",
    "    num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "    A probabilities tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    if t == 1.0:\n",
    "        normalization_constants = torch.log(\n",
    "            torch.sum(torch.exp(activations), dim=-1))\n",
    "    else:\n",
    "        normalization_constants = compute_normalization(\n",
    "            activations, t, num_iters)\n",
    "\n",
    "    return exp_t(activations - normalization_constants, t)\n",
    "\n",
    "\n",
    "def bi_tempered_logistic_loss(activations, labels, t1, t2, label_smoothing=0.0, num_iters=5, reduction='mean'):\n",
    "    \"\"\"Bi-Tempered Logistic Loss.\"\"\"\n",
    "\n",
    "    if len(labels.shape) < len(activations.shape):  # not one-hot\n",
    "        labels_onehot = torch.zeros_like(activations)\n",
    "        labels_onehot.scatter_(1, labels[..., None], 1)\n",
    "    else:\n",
    "        labels_onehot = labels\n",
    "\n",
    "    if label_smoothing > 0:\n",
    "        num_classes = labels_onehot.shape[-1]\n",
    "        labels_onehot = (1 - label_smoothing * num_classes / (num_classes - 1)) \\\n",
    "            * labels_onehot + \\\n",
    "            label_smoothing / (num_classes - 1)\n",
    "\n",
    "    probabilities = tempered_softmax(activations, t2, num_iters)\n",
    "\n",
    "    loss_values = labels_onehot * log_t(labels_onehot + 1e-10, t1) \\\n",
    "        - labels_onehot * log_t(probabilities, t1) \\\n",
    "        - labels_onehot.pow(2.0 - t1) / (2.0 - t1) \\\n",
    "        + probabilities.pow(2.0 - t1) / (2.0 - t1)\n",
    "    loss_values = loss_values.sum(dim=-1)  # sum over classes\n",
    "\n",
    "    if reduction == 'none':\n",
    "        return loss_values\n",
    "    if reduction == 'sum':\n",
    "        return loss_values.sum()\n",
    "    if reduction == 'mean':\n",
    "        return loss_values.mean()\n",
    "\n",
    "\n",
    "class BiTemperedLogisticLoss(_WeightedLoss):\n",
    "    \"Bi-tempered-logistic-loss\"\n",
    "\n",
    "    def __init__(self, eps: float = 0.1, t1=0.6, t2=1.4, num_iters=5, reduction='mean'):\n",
    "        super().__init__(reduction=reduction)\n",
    "        self.eps = eps\n",
    "        self.t1, self.t2 = t1, t2\n",
    "        self.num_iters = num_iters\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = bi_tempered_logistic_loss(output, target, self.t1, self.t2,\n",
    "                                         self.eps, self.num_iters, self.reduction)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5005)\n"
     ]
    }
   ],
   "source": [
    "output = torch.tensor([[0.1, 0.2, 0.5],[0.3, 0.4, 0.0]])\n",
    "target = torch.tensor([1, 2])\n",
    "loss = BiTemperedLogisticLoss()(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# from : https://www.kaggle.com/yerramvarun/cassava-taylorce-loss-label-smoothing-combo\n",
    "class TaylorSoftmax(torch.nn.Module):\n",
    "    def __init__(self, dim=1, n=2):\n",
    "        super(TaylorSoftmax, self).__init__()\n",
    "        assert n % 2 == 0\n",
    "        self.dim = dim\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        fn = torch.ones_like(x)\n",
    "        denor = 1.\n",
    "        for i in range(1, self.n+1):\n",
    "            denor *= i\n",
    "            fn = fn + x.pow(i) / denor\n",
    "        out = fn / fn.sum(dim=self.dim, keepdims=True)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TaylorCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, n=2, reduction='mean', eps: float = 0.2):\n",
    "        super(TaylorCrossEntropyLoss, self).__init__(reduction=reduction)\n",
    "        assert n % 2 == 0\n",
    "        self.taylor_softmax = TaylorSoftmax(dim=1, n=n)\n",
    "        self.label_smoothing_loss = LabelSmoothingCrossEntropy(\n",
    "            eps=eps, reduction=self.reduction)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        log_probs = self.taylor_softmax(output).log()\n",
    "        loss = self.label_smoothing_loss(log_probs, target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2290)\n"
     ]
    }
   ],
   "source": [
    "output = torch.tensor([[0.1, 0.2, 0.5],[0.3, 0.4, 0.0]])\n",
    "target = torch.tensor([1, 2])\n",
    "loss = TaylorCrossEntropyLoss()(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_mixmethods.ipynb.\n",
      "Converted 02_losses.ipynb.\n",
      "Converted 03_layers.ipynb.\n",
      "Converted 03a_networks.ipynb.\n",
      "Converted 04_optimizers_schedules.ipynb.\n",
      "Converted 05_lightning.data.ipynb.\n",
      "Converted 05a_lightning.core.ipynb.\n",
      "Converted 05b_lightning.callbacks.ipynb.\n",
      "Converted 06_fastai.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
