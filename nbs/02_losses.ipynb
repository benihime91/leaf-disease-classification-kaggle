{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _WeightedLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LabelSmoothingCrossEntropy(_WeightedLoss):\n",
    "    \"label smoothing loss\"\n",
    "    def __init__(self, eps:float=0.1, reduction='mean', weight=None):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.eps,self.reduction = eps,reduction\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        if self.reduction=='sum': \n",
    "            loss = -log_preds.sum()\n",
    "        else:\n",
    "            loss = -log_preds.sum(dim=-1) #We divide by that size at the return line so sum and not mean\n",
    "            if self.reduction=='mean':  loss = loss.mean()\n",
    "        return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.2480)\n"
     ]
    }
   ],
   "source": [
    "output = torch.tensor([[0.1, 0.2, 0.5],[0.3, 0.4, 0.0]])\n",
    "target = torch.tensor([1, 2])\n",
    "loss = LabelSmoothingCrossEntropy()(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "###########################################################################\n",
    "#From : https://github.com/mlpanda/bi-tempered-loss-pytorch/blob/master/bi_tempered_loss.py\n",
    "###########################################################################\n",
    "def log_t(u, t):\n",
    "    \"\"\"Compute log_t for `u`.\"\"\"\n",
    "\n",
    "    if t == 1.0:\n",
    "        return torch.log(u)\n",
    "    else:\n",
    "        return (u ** (1.0 - t) - 1.0) / (1.0 - t)\n",
    "\n",
    "\n",
    "def exp_t(u, t):\n",
    "    \"\"\"Compute exp_t for `u`.\"\"\"\n",
    "\n",
    "    if t == 1.0:\n",
    "        return torch.exp(u)\n",
    "    else:\n",
    "        return torch.relu(1.0 + (1.0 - t) * u) ** (1.0 / (1.0 - t))\n",
    "\n",
    "\n",
    "def compute_normalization_fixed_point(activations, t, num_iters=5):\n",
    "    \"\"\"Returns the normalization value for each example (t > 1.0).\n",
    "    Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature 2 (> 1.0 for tail heaviness).\n",
    "    num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "\n",
    "    mu = torch.max(activations, dim=-1).values.view(-1, 1)\n",
    "    normalized_activations_step_0 = activations - mu\n",
    "\n",
    "    normalized_activations = normalized_activations_step_0\n",
    "    i = 0\n",
    "    while i < num_iters:\n",
    "        i += 1\n",
    "        logt_partition = torch.sum(exp_t(normalized_activations, t), dim=-1).view(-1, 1)\n",
    "        normalized_activations = normalized_activations_step_0 * (logt_partition ** (1.0 - t))\n",
    "\n",
    "    logt_partition = torch.sum(exp_t(normalized_activations, t), dim=-1).view(-1, 1)\n",
    "\n",
    "    return -log_t(1.0 / logt_partition, t) + mu\n",
    "\n",
    "\n",
    "def compute_normalization(activations, t, num_iters=5):\n",
    "    \"\"\"Returns the normalization value for each example.\n",
    "    Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature 2 (< 1.0 for finite support, > 1.0 for tail heaviness).\n",
    "    num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "\n",
    "    if t < 1.0:\n",
    "        return None # not implemented as these values do not occur in the authors experiments...\n",
    "    else:\n",
    "        return compute_normalization_fixed_point(activations, t, num_iters)\n",
    "\n",
    "\n",
    "def tempered_softmax(activations, t, num_iters=5):\n",
    "    \"\"\"Tempered softmax function.\n",
    "    Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature tensor > 0.0.\n",
    "    num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "    A probabilities tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    if t == 1.0:\n",
    "        normalization_constants = torch.log(torch.sum(torch.exp(activations), dim=-1))\n",
    "    else:\n",
    "        normalization_constants = compute_normalization(activations, t, num_iters)\n",
    "\n",
    "    return exp_t(activations - normalization_constants, t)\n",
    "\n",
    "def bi_tempered_logistic_loss(activations, labels, t1, t2, label_smoothing=0.0, num_iters=5, reduction = 'mean'):\n",
    "    \"\"\"Bi-Tempered Logistic Loss.\"\"\"\n",
    "\n",
    "    if len(labels.shape)<len(activations.shape): #not one-hot\n",
    "        labels_onehot = torch.zeros_like(activations)\n",
    "        labels_onehot.scatter_(1, labels[..., None], 1)\n",
    "    else:\n",
    "        labels_onehot = labels\n",
    "\n",
    "    if label_smoothing > 0:\n",
    "        num_classes = labels_onehot.shape[-1]\n",
    "        labels_onehot = ( 1 - label_smoothing * num_classes / (num_classes - 1) ) \\\n",
    "                * labels_onehot + \\\n",
    "                label_smoothing / (num_classes - 1)\n",
    "\n",
    "    probabilities = tempered_softmax(activations, t2, num_iters)\n",
    "\n",
    "    loss_values = labels_onehot * log_t(labels_onehot + 1e-10, t1) \\\n",
    "            - labels_onehot * log_t(probabilities, t1) \\\n",
    "            - labels_onehot.pow(2.0 - t1) / (2.0 - t1) \\\n",
    "            + probabilities.pow(2.0 - t1) / (2.0 - t1)\n",
    "    loss_values = loss_values.sum(dim = -1) #sum over classes\n",
    "\n",
    "    if reduction == 'none':\n",
    "        return loss_values\n",
    "    if reduction == 'sum':\n",
    "        return loss_values.sum()\n",
    "    if reduction == 'mean':\n",
    "        return loss_values.mean()\n",
    "\n",
    "\n",
    "class BiTemperedLogisticLoss(_WeightedLoss):\n",
    "    \"Bi-tempered-logistic-loss\"\n",
    "    def __init__(self, eps:float=0.1, t1=0.6, t2=1.4, num_iters=5, reduction='mean'):\n",
    "        super().__init__(reduction=reduction)\n",
    "        self.eps = eps\n",
    "        self.t1, self.t2 = t1, t2\n",
    "        self.num_iters = num_iters\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss=bi_tempered_logistic_loss(output, target, self.t1, self.t2, \n",
    "                                      self.eps, self.num_iters, self.reduction)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.5005)\n"
     ]
    }
   ],
   "source": [
    "output = torch.tensor([[0.1, 0.2, 0.5],[0.3, 0.4, 0.0]])\n",
    "target = torch.tensor([1, 2])\n",
    "loss = BiTemperedLogisticLoss()(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Converted 00_core.ipynb.\nConverted 01_mixmethods.ipynb.\nConverted 02_losses.ipynb.\nConverted 03_layers.ipynb.\nConverted 03a_networks.ipynb.\nConverted 04_optimizers_schedules.ipynb.\nConverted 05_lightning.core.ipynb.\nConverted 05a_lightning.callbacks.ipynb.\nConverted 06_fastai.core.ipynb.\nConverted example-training-notebook.ipynb.\nConverted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}