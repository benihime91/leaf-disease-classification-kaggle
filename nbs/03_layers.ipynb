{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# export\\nimport torch\\nfrom torch import nn\\nimport torch.nn.functional as F\\nfrom functools import partial\\n\\nnorm_types = (\\n    nn.BatchNorm1d,\\n    nn.BatchNorm2d,\\n    nn.BatchNorm3d,\\n    nn.InstanceNorm1d,\\n    nn.InstanceNorm2d,\\n    nn.InstanceNorm3d,\\n    nn.LayerNorm,\\n)\";\n",
       "                var nbb_formatted_code = \"# export\\nimport torch\\nfrom torch import nn\\nimport torch.nn.functional as F\\nfrom functools import partial\\n\\nnorm_types = (\\n    nn.BatchNorm1d,\\n    nn.BatchNorm2d,\\n    nn.BatchNorm3d,\\n    nn.InstanceNorm1d,\\n    nn.InstanceNorm2d,\\n    nn.InstanceNorm3d,\\n    nn.LayerNorm,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "norm_types = (\n",
    "    nn.BatchNorm1d,\n",
    "    nn.BatchNorm2d,\n",
    "    nn.BatchNorm3d,\n",
    "    nn.InstanceNorm1d,\n",
    "    nn.InstanceNorm2d,\n",
    "    nn.InstanceNorm3d,\n",
    "    nn.LayerNorm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# export\\ndef requires_grad(m):\\n    \\\"Check if the first parameter of `m` requires grad or not\\\"\\n    ps = list(m.parameters())\\n    return ps[0].requires_grad if len(ps) > 0 else False\\n\\n\\ndef init_default(m, func=nn.init.kaiming_normal_):\\n    \\\"Initialize `m` weights with `func` and set `bias` to 0.\\\"\\n    if func:\\n        if hasattr(m, \\\"weight\\\"):\\n            func(m.weight)\\n        if hasattr(m, \\\"bias\\\") and hasattr(m.bias, \\\"data\\\"):\\n            m.bias.data.fill_(0.0)\\n    return m\\n\\n\\ndef cond_init(m, func):\\n    \\\"Apply `init_default` to `m` unless it's a batchnorm module\\\"\\n    if (not isinstance(m, norm_types)) and requires_grad(m):\\n        init_default(m, func)\\n\\n\\ndef apply_leaf(m, f):\\n    \\\"Apply `f` to children of `m`.\\\"\\n    c = m.children()\\n    if isinstance(m, nn.Module):\\n        f(m)\\n    for l in c:\\n        apply_leaf(l, f)\\n\\n\\ndef apply_init(m, func=nn.init.kaiming_normal_):\\n    \\\"Initialize all non-batchnorm layers of `m` with `func`.\\\"\\n    apply_leaf(m, partial(cond_init, func=func))\";\n",
       "                var nbb_formatted_code = \"# export\\ndef requires_grad(m):\\n    \\\"Check if the first parameter of `m` requires grad or not\\\"\\n    ps = list(m.parameters())\\n    return ps[0].requires_grad if len(ps) > 0 else False\\n\\n\\ndef init_default(m, func=nn.init.kaiming_normal_):\\n    \\\"Initialize `m` weights with `func` and set `bias` to 0.\\\"\\n    if func:\\n        if hasattr(m, \\\"weight\\\"):\\n            func(m.weight)\\n        if hasattr(m, \\\"bias\\\") and hasattr(m.bias, \\\"data\\\"):\\n            m.bias.data.fill_(0.0)\\n    return m\\n\\n\\ndef cond_init(m, func):\\n    \\\"Apply `init_default` to `m` unless it's a batchnorm module\\\"\\n    if (not isinstance(m, norm_types)) and requires_grad(m):\\n        init_default(m, func)\\n\\n\\ndef apply_leaf(m, f):\\n    \\\"Apply `f` to children of `m`.\\\"\\n    c = m.children()\\n    if isinstance(m, nn.Module):\\n        f(m)\\n    for l in c:\\n        apply_leaf(l, f)\\n\\n\\ndef apply_init(m, func=nn.init.kaiming_normal_):\\n    \\\"Initialize all non-batchnorm layers of `m` with `func`.\\\"\\n    apply_leaf(m, partial(cond_init, func=func))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "def requires_grad(m):\n",
    "    \"Check if the first parameter of `m` requires grad or not\"\n",
    "    ps = list(m.parameters())\n",
    "    return ps[0].requires_grad if len(ps) > 0 else False\n",
    "\n",
    "\n",
    "def init_default(m, func=nn.init.kaiming_normal_):\n",
    "    \"Initialize `m` weights with `func` and set `bias` to 0.\"\n",
    "    if func:\n",
    "        if hasattr(m, \"weight\"):\n",
    "            func(m.weight)\n",
    "        if hasattr(m, \"bias\") and hasattr(m.bias, \"data\"):\n",
    "            m.bias.data.fill_(0.0)\n",
    "    return m\n",
    "\n",
    "\n",
    "def cond_init(m, func):\n",
    "    \"Apply `init_default` to `m` unless it's a batchnorm module\"\n",
    "    if (not isinstance(m, norm_types)) and requires_grad(m):\n",
    "        init_default(m, func)\n",
    "\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    \"Apply `f` to children of `m`.\"\n",
    "    c = m.children()\n",
    "    if isinstance(m, nn.Module):\n",
    "        f(m)\n",
    "    for l in c:\n",
    "        apply_leaf(l, f)\n",
    "\n",
    "\n",
    "def apply_init(m, func=nn.init.kaiming_normal_):\n",
    "    \"Initialize all non-batchnorm layers of `m` with `func`.\"\n",
    "    apply_leaf(m, partial(cond_init, func=func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# export\\nclass AdaptiveConcatPool2d(nn.Module):\\n    \\\"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d` from FastAI\\\"\\n\\n    def __init__(self, size=None):\\n        super(AdaptiveConcatPool2d, self).__init__()\\n        self.size = size or 1\\n        self.ap = nn.AdaptiveAvgPool2d(self.size)\\n        self.mp = nn.AdaptiveMaxPool2d(self.size)\\n\\n    def forward(self, x):\\n        return torch.cat([self.mp(x), self.ap(x)], 1)\";\n",
       "                var nbb_formatted_code = \"# export\\nclass AdaptiveConcatPool2d(nn.Module):\\n    \\\"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d` from FastAI\\\"\\n\\n    def __init__(self, size=None):\\n        super(AdaptiveConcatPool2d, self).__init__()\\n        self.size = size or 1\\n        self.ap = nn.AdaptiveAvgPool2d(self.size)\\n        self.mp = nn.AdaptiveMaxPool2d(self.size)\\n\\n    def forward(self, x):\\n        return torch.cat([self.mp(x), self.ap(x)], 1)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d` from FastAI\"\n",
    "\n",
    "    def __init__(self, size=None):\n",
    "        super(AdaptiveConcatPool2d, self).__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool2d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool2d(self.size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.mp(x), self.ap(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# export\\n# mish activation from : https://github.com/fastai/fastai/blob/master/fastai/layers.py#L549\\n@torch.jit.script\\ndef _mish_jit_fwd(x):\\n    return x.mul(torch.tanh(F.softplus(x)))\\n\\n\\n@torch.jit.script\\ndef _mish_jit_bwd(x, grad_output):\\n    x_sigmoid = torch.sigmoid(x)\\n    x_tanh_sp = F.softplus(x).tanh()\\n    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\\n\\n\\nclass MishJitAutoFn(torch.autograd.Function):\\n    @staticmethod\\n    def forward(ctx, x):\\n        ctx.save_for_backward(x)\\n        return _mish_jit_fwd(x)\\n\\n    @staticmethod\\n    def backward(ctx, grad_output):\\n        x = ctx.saved_variables[0]\\n        return _mish_jit_bwd(x, grad_output)\\n\\n\\ndef mish(x):\\n    return MishJitAutoFn.apply(x)\\n\\n\\n# Cell\\nclass Mish(nn.Module):\\n    def __init__(self, inplace=False):\\n        super(Mish, self).__init__()\\n\\n    def forward(self, x):\\n        return MishJitAutoFn.apply(x)\";\n",
       "                var nbb_formatted_code = \"# export\\n# mish activation from : https://github.com/fastai/fastai/blob/master/fastai/layers.py#L549\\n@torch.jit.script\\ndef _mish_jit_fwd(x):\\n    return x.mul(torch.tanh(F.softplus(x)))\\n\\n\\n@torch.jit.script\\ndef _mish_jit_bwd(x, grad_output):\\n    x_sigmoid = torch.sigmoid(x)\\n    x_tanh_sp = F.softplus(x).tanh()\\n    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\\n\\n\\nclass MishJitAutoFn(torch.autograd.Function):\\n    @staticmethod\\n    def forward(ctx, x):\\n        ctx.save_for_backward(x)\\n        return _mish_jit_fwd(x)\\n\\n    @staticmethod\\n    def backward(ctx, grad_output):\\n        x = ctx.saved_variables[0]\\n        return _mish_jit_bwd(x, grad_output)\\n\\n\\ndef mish(x):\\n    return MishJitAutoFn.apply(x)\\n\\n\\n# Cell\\nclass Mish(nn.Module):\\n    def __init__(self, inplace=False):\\n        super(Mish, self).__init__()\\n\\n    def forward(self, x):\\n        return MishJitAutoFn.apply(x)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "# mish activation from : https://github.com/fastai/fastai/blob/master/fastai/layers.py#L549\n",
    "@torch.jit.script\n",
    "def _mish_jit_fwd(x):\n",
    "    return x.mul(torch.tanh(F.softplus(x)))\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def _mish_jit_bwd(x, grad_output):\n",
    "    x_sigmoid = torch.sigmoid(x)\n",
    "    x_tanh_sp = F.softplus(x).tanh()\n",
    "    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n",
    "\n",
    "\n",
    "class MishJitAutoFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return _mish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_variables[0]\n",
    "        return _mish_jit_bwd(x, grad_output)\n",
    "\n",
    "\n",
    "def mish(x):\n",
    "    return MishJitAutoFn.apply(x)\n",
    "\n",
    "\n",
    "# Cell\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super(Mish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return MishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# export\\ndef cut_model(model: nn.Module, n: int = -2):\\n    \\\"cuts `model` layers upto `n`\\\"\\n    ls = list(model.children())[:n]\\n    encoder = nn.Sequential(*ls)\\n    return encoder\\n\\n\\ndef num_features_model(m: nn.Module, in_chs: int = 3):\\n    \\\"Return the number of output features for `m`.\\\"\\n    m.to(\\\"cpu\\\")\\n    dummy_inp = torch.zeros((32, in_chs, 120, 120))\\n    dummy_out = m(dummy_inp)\\n    return dummy_out.size()[1]\";\n",
       "                var nbb_formatted_code = \"# export\\ndef cut_model(model: nn.Module, n: int = -2):\\n    \\\"cuts `model` layers upto `n`\\\"\\n    ls = list(model.children())[:n]\\n    encoder = nn.Sequential(*ls)\\n    return encoder\\n\\n\\ndef num_features_model(m: nn.Module, in_chs: int = 3):\\n    \\\"Return the number of output features for `m`.\\\"\\n    m.to(\\\"cpu\\\")\\n    dummy_inp = torch.zeros((32, in_chs, 120, 120))\\n    dummy_out = m(dummy_inp)\\n    return dummy_out.size()[1]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "def cut_model(model: nn.Module, n: int = -2):\n",
    "    \"cuts `model` layers upto `n`\"\n",
    "    ls = list(model.children())[:n]\n",
    "    encoder = nn.Sequential(*ls)\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def num_features_model(m: nn.Module, in_chs: int = 3):\n",
    "    \"Return the number of output features for `m`.\"\n",
    "    m.to(\"cpu\")\n",
    "    dummy_inp = torch.zeros((32, in_chs, 120, 120))\n",
    "    dummy_out = m(dummy_inp)\n",
    "    return dummy_out.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"import torchvision\\n\\norig_model = torchvision.models.resnet18(pretrained=False)\\nmodel = cut_model(orig_model, -2)\";\n",
       "                var nbb_formatted_code = \"import torchvision\\n\\norig_model = torchvision.models.resnet18(pretrained=False)\\nmodel = cut_model(orig_model, -2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "orig_model = torchvision.models.resnet18(pretrained=False)\n",
    "model = cut_model(orig_model, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"num_features_model(model), num_features_model(orig_model)\";\n",
       "                var nbb_formatted_code = \"num_features_model(model), num_features_model(orig_model)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_features_model(model), num_features_model(orig_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"# export\\ndef create_head(\\n    nf: int, n_out: int, lin_ftrs: int = 512, act: nn.Module = nn.ReLU(inplace=True)\\n):\\n    \\\"create a custom head for a classifier from FastAI\\\"\\n    lin_ftrs = [nf, lin_ftrs, n_out]\\n\\n    pool = AdaptiveConcatPool2d()\\n\\n    layers = [pool, nn.Flatten()]\\n\\n    layers += [\\n        nn.BatchNorm1d(lin_ftrs[0]),\\n        nn.Dropout(0.25),\\n        act,\\n        nn.Linear(lin_ftrs[0], lin_ftrs[1], bias=False),\\n        nn.BatchNorm1d(lin_ftrs[1]),\\n        nn.Dropout(0.5),\\n        act,\\n        nn.Linear(lin_ftrs[1], lin_ftrs[2], bias=False),\\n    ]\\n    return nn.Sequential(*layers)\";\n",
       "                var nbb_formatted_code = \"# export\\ndef create_head(\\n    nf: int, n_out: int, lin_ftrs: int = 512, act: nn.Module = nn.ReLU(inplace=True)\\n):\\n    \\\"create a custom head for a classifier from FastAI\\\"\\n    lin_ftrs = [nf, lin_ftrs, n_out]\\n\\n    pool = AdaptiveConcatPool2d()\\n\\n    layers = [pool, nn.Flatten()]\\n\\n    layers += [\\n        nn.BatchNorm1d(lin_ftrs[0]),\\n        nn.Dropout(0.25),\\n        act,\\n        nn.Linear(lin_ftrs[0], lin_ftrs[1], bias=False),\\n        nn.BatchNorm1d(lin_ftrs[1]),\\n        nn.Dropout(0.5),\\n        act,\\n        nn.Linear(lin_ftrs[1], lin_ftrs[2], bias=False),\\n    ]\\n    return nn.Sequential(*layers)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "def create_head(\n",
    "    nf: int, n_out: int, lin_ftrs: int = 512, act: nn.Module = nn.ReLU(inplace=True)\n",
    "):\n",
    "    \"create a custom head for a classifier from FastAI\"\n",
    "    lin_ftrs = [nf, lin_ftrs, n_out]\n",
    "\n",
    "    pool = AdaptiveConcatPool2d()\n",
    "\n",
    "    layers = [pool, nn.Flatten()]\n",
    "\n",
    "    layers += [\n",
    "        nn.BatchNorm1d(lin_ftrs[0]),\n",
    "        nn.Dropout(0.25),\n",
    "        act,\n",
    "        nn.Linear(lin_ftrs[0], lin_ftrs[1], bias=False),\n",
    "        nn.BatchNorm1d(lin_ftrs[1]),\n",
    "        nn.Dropout(0.5),\n",
    "        act,\n",
    "        nn.Linear(lin_ftrs[1], lin_ftrs[2], bias=False),\n",
    "    ]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): AdaptiveConcatPool2d(\n",
       "    (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "    (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "  )\n",
       "  (1): Flatten(start_dim=1, end_dim=-1)\n",
       "  (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Dropout(p=0.25, inplace=False)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Dropout(p=0.5, inplace=False)\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): Linear(in_features=512, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"head = create_head(num_features_model(model), n_out=3)\\nhead\";\n",
       "                var nbb_formatted_code = \"head = create_head(num_features_model(model), n_out=3)\\nhead\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head = create_head(num_features_model(model), n_out=3)\n",
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_mixmethods.ipynb.\n",
      "Converted 02_losses.ipynb.\n",
      "Converted 03_layers.ipynb.\n",
      "Converted 03a_networks.ipynb.\n",
      "Converted 04_optimizers_schedules.ipynb.\n",
      "Converted 05_lightning.core.ipynb.\n",
      "Converted 05a_lightning.callbacks.ipynb.\n",
      "Converted 06_fastai.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# hide\\nfrom nbdev.export import *\\n\\nnotebook2script()\";\n",
       "                var nbb_formatted_code = \"# hide\\nfrom nbdev.export import *\\n\\nnotebook2script()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
