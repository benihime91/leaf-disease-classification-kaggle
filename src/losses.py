# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_losses.ipynb (unless otherwise specified).

__all__ = ['LabelSmoothingCrossEntropy']

# Cell
import torch
import torch.nn.functional as F
from torch.nn.modules.loss import _WeightedLoss

# Cell
class LabelSmoothingCrossEntropy(_WeightedLoss):
    "label smoothing loss"
    def __init__(self, eps:float=0.1, reduction='mean', weight=None):
        super().__init__(weight=weight, reduction=reduction)
        self.eps,self.reduction = eps,reduction
        self.weight = weight

    def forward(self, output, target):
        c = output.size()[-1]
        log_preds = F.log_softmax(output, dim=-1)
        if self.reduction=='sum':
            loss = -log_preds.sum()
        else:
            loss = -log_preds.sum(dim=-1) #We divide by that size at the return line so sum and not mean
            if self.reduction=='mean':  loss = loss.mean()
        return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), reduction=self.reduction)