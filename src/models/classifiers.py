# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04c_models.classifiers.ipynb (unless otherwise specified).

__all__ = ["CLASSIFIER_REGISTERY", "CnnHeadV0", "CnnHeadV1", "CnnHeadV2"]

# Cell
import torch.nn.functional as F
from fastcore.all import L
from fvcore.common import registry
from torch import nn

from .layers import *
from .utils import *

CLASSIFIER_REGISTERY = registry.Registry("Classifiers")

# Cell
@CLASSIFIER_REGISTERY.register()
def CnnHeadV0(nf, n_out, concat_pool=True, use_conv=False, drop=0.5, **kwargs):
    "create a classifier from timm lib"
    pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)
    global_pool = nn.Sequential(pool, nn.Flatten())
    if concat_pool:
        nf *= 2
    fc = nn.Linear(nf, n_out, bias=True)
    if drop > 0:
        head = nn.Sequential(nn.Dropout(drop), global_pool, fc)
    else:
        head = nn.Sequential(global_pool, fc)
    return head


# Cell
@CLASSIFIER_REGISTERY.register()
def CnnHeadV1(nf, n_out, lin_ftrs=None, ps=0.5, concat_pool=True, first_bn=True, lin_first=False,
              act_layer="default", bn_final=False, **kwargs,):
    "Model head that takes `nf` features, runs through `lin_ftrs`, and out `n_out` classes."
    if concat_pool:
        nf *= 2
    lin_ftrs = [nf, 512, n_out] if lin_ftrs is None else [nf] + lin_ftrs + [n_out]
    bns = [first_bn] + [True] * len(lin_ftrs[1:])
    ps = L(ps)

    if len(ps) == 1:
        ps = [ps[0] / 2] * (len(lin_ftrs) - 2) + ps

    actns = [ACTIVATIONS[act_layer](inplace=True)] * (len(lin_ftrs) - 2) + [None]

    pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)

    layers = [pool, nn.Flatten()]

    if lin_first:
        layers.append(nn.Dropout(ps.pop(0)))

    for ni, no, bn, p, actn in zip(lin_ftrs[:-1], lin_ftrs[1:], bns, ps, actns):
        layers += LinBnDrop(ni, no, bn=bn, p=p, act=actn, lin_first=lin_first)
    if lin_first:
        layers.append(nn.Linear(lin_ftrs[-2], n_out))
    if bn_final:
        layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))
    return nn.Sequential(*layers)


# Cell
@CLASSIFIER_REGISTERY.register()
class CnnHeadV2(nn.Module):
    def __init__(self, nf, n_out, dropout=0.5, act_layer="mish", **kwargs):
        super().__init__()
        self.dropout = dropout
        self.act1 = ACTIVATIONS[act_layer](inplace=True)
        self.conv = nn.Conv2d(nf, nf, 1, 1)
        self.norm = nn.BatchNorm2d(nf)
        self.pool = GeM()

        self.fc1 = nn.Linear(nf, nf // 2)
        self.act2 = ACTIVATIONS[act_layer](inplace=True)
        self.rms_norm = RMSNorm(nf // 2)
        self.fc2 = nn.Linear(nf // 2, n_out)

    def forward(self, x):
        x = self.act1(x)
        x = self.conv(x)
        x = self.norm(x)
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = F.dropout(x, p=self.dropout)
        x = self.fc1(x)

        x = self.act2(x)
        x = self.rms_norm(x)
        x = F.dropout(x, p=self.dropout / 2)
        x = self.fc2(x)
        return x
