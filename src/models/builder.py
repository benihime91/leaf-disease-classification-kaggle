# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03a_models.builder.ipynb (unless otherwise specified).

__all__ = ['build_head', 'Net']

# Cell
import timm
import torch
from fastcore.all import ifnone
from omegaconf import DictConfig
from timm.models.layers import create_classifier
from torch import nn

from src import _logger
from ..core import *
from .classifiers import *
from .layers import *
from .utils import apply_init, cut_model, num_features_model

# Cell
def build_head(cfg: DictConfig, nf, verbose=True):
    "builds a classifier for model with output `nf`"
    _logger.info("Configuration for model head : ")
    _logger.info(f"\t class_name: {cfg.name}")

    if verbose:
        for n,v in cfg.params.items():
            _logger.info(f"\t {n}: {str(v)}")

    head = CLASSIFIER_REGISTERY.get(cfg.name)(nf=nf, **cfg.params)
    return head

# Cell
class Net(nn.Module):
    "Creates a model using the Global Config"

    def __init__(self, cfg: DictConfig, verbose=True):
        super(Net, self).__init__()

        # configuration for the encoder
        base_cfg = cfg.model.base_model

        # build the encoder
        if verbose:
            _logger.info("Configuration for the current model :")
            _logger.info(f"\t base_network: {base_cfg.name}")

            if base_cfg.activation is not None:
                _logger.info(f"\t activation: {base_cfg.activation}")

            for n, v in base_cfg.params.items():
                _logger.info(f"\t {n}: {v}")

        if base_cfg.activation is not None:
            act = ACTIVATIONS[base_cfg.activation]
        else:
            act = None
        self.encoder = timm.create_model(base_cfg.name, act_layer=act, **base_cfg.params)
        self.encoder = cut_model(self.encoder, -2)

        # configuration for the classifier
        self.head_conf = cfg.model.head
        nf = num_features_model(self.encoder)
        # build the classifer
        self.head = build_head(self.head_conf, nf, verbose)


    def init_classifier(self):
        if self.clf_conf.act_layer == "default":
            apply_init(self.classifier, torch.nn.init.kaiming_normal_)
        else:
            apply_init(self.classifier, torch.nn.init.kaiming_uniform_)

    def get_head(self):
        return self.head

    def get_classifier(self):
        try:
            return self.head[-1]
        except:
            return self.head.fc2

    def forward_features(self, x):
        return self.encoder(x)

    def forward(self):
        return self.head(self.forward_features(x))

    def get_param_list(self):
        return [params(self.encoder), params(self.classifier)]