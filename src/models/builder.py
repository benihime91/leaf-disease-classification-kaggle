# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04d_models.builder.ipynb (unless otherwise specified).

__all__ = ["build_head", "Net"]

# Cell
import timm
import torch
from omegaconf import DictConfig, OmegaConf
from torch import nn

from src import _logger as logger
from ..core import *
from .classifiers import *
from .layers import *
from .utils import apply_init, cut_model, num_features_model

# Cell
def build_head(cfg: DictConfig, nf):
    "builds a classifier for model with output `nf` and `cfg`"
    head = CLASSIFIER_REGISTERY.get(cfg.name)(nf=nf, **cfg.params)
    return head


bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)

# From : https://github.com/fastai/fastai/blob/66a03da8a11cd85188f4c6f063b98c4a209492e8/fastai/callback/training.py#L43
def set_bn_eval(m: nn.Module, use_eval=True) -> None:
    "Set bn layers in eval mode for all recursive children of `m`."
    for l in m.children():
        if isinstance(l, bn_types) and not next(l.parameters()).requires_grad:
            if use_eval:
                l.eval()
            else:
                l.train()
        set_bn_eval(l)


# Cell
class Net(nn.Module):
    "Creates a model using the Global Config"

    def __init__(self, cfg: DictConfig, verbose=True):
        super(Net, self).__init__()
        self.global_conf = cfg
        self.base_conf   = cfg.model.base_model
        self.head_conf   = cfg.model.head

        # build the encoder
        if verbose:
            # configuration for the Whole Model
            m_conf = OmegaConf.to_yaml(cfg.model, resolve=True)
            # Log configuration for the Model
            logger.info(f"Configuration for current model:\n {m_conf}")

        # configure activation of the model
        # none for default layer or ReLU/SiLU for the model
        if self.base_conf.activation is not None: 
            self.act = ACTIVATIONS[self.base_conf.activation]
        else : 
            self.act = None

        # build encoder
        self.encoder = timm.create_model(self.base_conf.name, act_layer=self.act, **self.base_conf.params)
        self.encoder = cut_model(self.encoder, -2)
        
        # build the head of the model
        nf = num_features_model(self.encoder)
        self.head = build_head(self.head_conf, nf)

        # batch_norm layers + initialize the `head`
        self._make_trainable()
        self._init_head()

    def _make_trainable(self):
        "make all the layers trainable and optinally freeze the BN layers of the encoder"
        # make all layers trainable in encoder
        for param in self.encoder.parameters():
            param.requires_grad = True
        self.encoder.train()
        
        # freeze the batchnorm layers of the encoder
        if self.global_conf.training.bn_freeze:
            set_bn_eval(self.encoder)

        # make the custom head trainable
        for param in self.head.parameters():
            param.requires_grad = True
        self.head.train()

    def _init_head(self):
        "initializes the weights of the head of the model"
        if self.head_conf.params.act_layer == "default":
            apply_init(self.head, torch.nn.init.kaiming_normal_)
        elif self.head_conf.name == "CnnHeadV0":
            apply_init(self.head, torch.nn.init.kaiming_normal_)
        else:
            apply_init(self.head, torch.nn.init.kaiming_uniform_)

    def get_classifier(self):
        "returns the classification layer (final layer) of the head"
        return self.head[-1]

    def forward_features(self, x: torch.Tensor):
        "generates the feature maps from the encoder"
        return self.encoder(x)

    def forward(self, x: torch.Tensor):
        "forward for the model feature_extraction -> generate logits"
        return self.head(self.forward_features(x))

    def get_param_list(self):
        "splits the parameters of the Model"
        return [trainable_params(self.encoder), params(self.head)]
