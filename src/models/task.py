# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03b_models.task.ipynb (unless otherwise specified).

__all__ = ['Task']

# Cell
from typing import Any, Callable, Dict, List, Optional, Union
from collections import OrderedDict, namedtuple

import torch
from hydra.utils import instantiate
from omegaconf import DictConfig
from pytorch_lightning import LightningModule
from pytorch_lightning.metrics import Accuracy
from torch.utils.data import DataLoader

from src import _logger
from ..data import CassavaClassificationModule
from .builder import Net
from ..optimizers import create_optimizer
from ..schedulers import create_scheduler
from fastcore.all import ifnone

# Cell
class Task(LightningModule):
    "A general Task for Cassave Leaf Disease Classification"

    def __init__(self, conf: DictConfig):
        super().__init__()
        self.model = Net(conf)
        self.trn_metric = Accuracy()
        self.val_metric = Accuracy()
        self.tst_metric = Accuracy()
        self.save_hyperparameters(conf)

        self.datamodule = CassavaClassificationModule(conf)
        self.datamodule.prepare_data()
        self.datamodule.setup(stage=None, verbose=False)
        self.lrs = namedtuple("LearningRates", ["lr1", "lr2"])

        self.criterion   = instantiate(self.hparams.loss)
        self.mixfunction = instantiate(self.hparams.mixmethod)

        _logger.info(f"LossFunction: {self.criterion}")
        _logger.info(f"MixFunction : {self.mixfunction}")

    def forward(self, x: Any) -> Any:
        return self.model(x)

    def step(self, batch: Any, batch_idx: int, stage="train") -> Any:
        """
        The training/validation/test step. Override for custom behavior.
        """
        x, y = batch
        self.preds  = None
        self.labels = list(y.data.cpu().numpy())

        if stage == "train":
            if self.mixfunction is not None and (self.current_epoch < self.hparams.training.mix_epochs):
                x = mixfunction(x, y, model=self.model)
                logits = self.forward(x)
                loss   = self.mixfunction.lf(logits, loss_func=self.criterion)

            elif self.mixfunction is None or (self.current_epoch >= hparams.training.mix_epochs):
                logits = self.forward(x)
                loss   = self.criterion(logits, y)

            metric = self.trn_metric(logits, y)

        elif stage == "valid" or "test":
            logits = self.forward(x)
            loss   = self.criterion(logits, y)

            if stage == "valid":
                metric = self.val_metric(logits, y)
            else:
                metric = self.tst_metric(logits, y)
        else:
            raise NameError

        preds = torch.argmax(logits, 1)
        self.preds = list(preds)

        return dict(loss=loss, metric=metric)

    def training_step(self, batch: Any, batch_idx: int) -> Any:
        logs = self.step(batch, batch_idx, stage="train")

        self.log_dict(OrderedDict(train_loss=logs['loss'], train_acc=logs['metric']),
                      on_step=True, on_epoch=True, prog_bar=True)

        return logs['loss']

    def validation_step(self, batch: Any, batch_idx: int) -> None:
        logs = self.step(batch, batch_idx, stage="valid")

        self.log_dict(OrderedDict(val_loss=logs['loss'], val_acc=logs['metric']),
                      on_step=False, on_epoch=True, prog_bar=True)

    def test_step(self, batch: Any, batch_idx: int) -> None:
        logs = self.step(batch, batch_idx, stage="test")

        self.log_dict(OrderedDict(test_loss=logs['loss'], test_acc=logs['metric']),
                      on_step=False, on_epoch=True, prog_bar=False)

    def configure_optimizers(self) -> (List[torch.optim.Optimizer], List[Dict]):
        params = self.model.get_param_list()
        lrs    = (self.hparams.training.learning_rate/self.hparams.training.lr_mult,
                  self.hparams.training.learning_rate)
        self.lrs = self.lrs(lrs[0], lrs[1])

        epochs = self.hparams.training.num_epochs
        try   : steps = int(self.num_training_steps / epochs)
        except: steps = len(self.train_dataloader())/ self.hparams.training.accumulate_grad_batches
        params = [dict(params=params[0], lr=lrs[0]), dict(params=params[1], lr=lrs[1])]

        optim = create_optimizer(self.hparams.optimizer, params)
        sched = create_scheduler(self.hparams.scheduler, optim, steps, epochs)
        return [optim], [sched]

    def train_dataloader(self, *args, **kwargs) -> DataLoader:
        "returns a PyTorch DataLoader for Training"
        if self.current_epoch == self.hparams.training.mix_epochs:
            if self.mixfunction is not None:
                self.mixfunction.stop()
            # reset transformations in the datamoule
            self.datamodule.train_ds.reload_transforms(self.trainer.datamodule.augs_final)
        dataloader = self.datamodule.train_dataloader()
        return dataloader

    def val_dataloader(self, *args, **kwargs) -> DataLoader:
        "returns a PyTorch DataLoader for Validation"
        return self.datamodule.val_dataloader()

    def test_dataloader(self, *args, **kwargs) -> DataLoader:
        "returns a PyTorch DataLoader for Testing"
        return self.datamodule.test_dataloader()

    @property
    def num_training_steps(self) -> int:
        """Total training steps inferred from datamodule and devices."""
        dataset = self.train_dataloader()
        if self.trainer.max_steps:
            return self.trainer.max_steps

        dataset_size = (
            self.trainer.limit_train_batches
            if self.trainer.limit_train_batches != 0
            else len(dataset)
        )

        num_devices = max(1, self.trainer.num_gpus, self.trainer.num_processes)
        if self.trainer.tpu_cores:
            num_devices = max(num_devices, self.trainer.tpu_cores)

        effective_batch_size = dataset.batch_size * self.trainer.accumulate_grad_batches * num_devices
        return (dataset_size // effective_batch_size) * self.trainer.max_epochs