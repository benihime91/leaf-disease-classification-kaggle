# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01a_lightning.callbacks.ipynb (unless otherwise specified).

__all__ = ['WandbImageClassificationCallback', 'LitProgressBar', 'PrintLogsCallback']

# Cell
import sys
import time
import datetime
from tqdm import tqdm
import wandb

import torch
import pytorch_lightning as pl
from pytorch_lightning import _logger as log

from .core import *
from ..core import *

# Cell
class WandbImageClassificationCallback(pl.Callback):
    """ Custom callback to add some extra functionalites to the wandb logger """

    def __init__(self,
                 dm: pl.LightningDataModule,
                 default_config: dict = None,
                 num_batches:int = 16,
                 log_train_batch: bool = False,
                 log_preds: bool = False,
                 log_conf_mat: bool = True,):

        # class names for the confusion matrix
        self.class_names = list(conf_mat_idx2lbl.values())

        # counter to log training batch images
        self.dm = dm
        self.num_bs = num_batches
        self.curr_epoch = 0
        self.log_train_batch = log_train_batch
        self.log_preds = log_preds
        self.val_imgs, self.val_labels = None, None
        self.log_conf_mat = log_conf_mat
        self.default_config = default_config

    def on_train_start(self, trainer, pl_module, *args, **kwargs):
        try:
            # log model to the wandb experiment
            wandb.watch(models=pl_module.model, criterion=pl_module.loss_func)
        except:
            log.info("Skipping wandb.watch --->")

        train_augs, valid_augs = self.dm.train_augs, self.dm.valid_augs
        self.train_config = dict(train_augments=train_augs, valid_augments=valid_augs)

        if self.default_config is not None:
            try:
                wandb.config.update(self.default_config)
                wandb.config.update(self.train_config)
                log.info("wandb config updated -->")
            except:
                log.info("Skipping update wandb config -->")

    def on_train_epoch_end(self, trainer, pl_module, *args, **kwargs):
        if self.log_train_batch:
            if pl_module.one_batch_of_image is None:
                log.info(f"{self.config_defaults['mixmethod']} samples not available . Skipping --->")
                pass

            else:
                one_batch = pl_module.one_batch_of_image[:self.num_bs]
                train_ims = one_batch.data.to('cpu')
                trainer.logger.experiment.log({"train_batch":[wandb.Image(x) for x in train_ims]}, commit=False)

    def on_validation_epoch_end(self, trainer, pl_module, *args, **kwargs):
        if self.log_preds:
            if self.val_imgs is None and self.val_labels is None:
                self.val_imgs, self.val_labels = next(iter(self.dm.val_dataloader()))
                self.val_imgs, self.val_labels = self.val_imgs[:self.num_bs], self.val_labels[:self.num_bs]
                self.val_imgs = self.val_imgs.to(device=pl_module.device)

            logits = pl_module(self.val_imgs)
            preds  = torch.argmax(logits, 1)
            preds  = preds.data.cpu()
            trainer.logger.experiment.log({"predictions": [wandb.Image(x, caption=f"Pred:{pred}, Label:{y}") for x, pred, y in zip(self.val_imgs, preds, self.val_labels)]},
                                          commit=False)

    def on_epoch_start(self, trainer, pl_module, *args, **kwargs):
        pl_module.val_labels_list = []
        pl_module.val_preds_list  = []

    def on_epoch_end(self, trainer, pl_module, *args, **kwargs):
        if self.log_conf_mat:
            val_preds  = torch.tensor(pl_module.val_preds_list).data.cpu().numpy()
            val_labels = torch.tensor(pl_module.val_labels_list).data.cpu().numpy()
            log_dict = {'conf_mat': wandb.plot.confusion_matrix(val_preds,val_labels,self.class_names)}
            wandb.log(log_dict,commit=False)

# Cell
class LitProgressBar(pl.callbacks.ProgressBar):
    "Custom Progressbar callback for Lightning Training"
    def init_sanity_tqdm(self) -> tqdm:
        """ Override this to customize the tqdm bar for the validation sanity run. """
        bar = tqdm(
            desc='Validation sanity check',
            position=(2 * self.process_position),
            disable=self.is_disabled,
            leave=False,
            dynamic_ncols=True,
            file=sys.stdout,
        )
        return bar

    def init_train_tqdm(self) -> tqdm:
        """ Override this to customize the tqdm bar for training. """
        bar = tqdm(
            desc='Training',
            initial=self.train_batch_idx,
            position=(2 * self.process_position),
            disable=self.is_disabled,
            leave=False,
            dynamic_ncols=True,
            file=sys.stdout,
            smoothing=0,
        )
        return bar

    def init_validation_tqdm(self) -> tqdm:
        """ Override this to customize the tqdm bar for validation. """
        bar = tqdm(
            desc='Validating',
            position=(2 * self.process_position + 1),
            disable=True,
            leave=False,
            dynamic_ncols=True,
            file=sys.stdout
        )
        return bar

    def init_test_tqdm(self) -> tqdm:
        """ Override this to customize the tqdm bar for testing. """
        bar = tqdm(
            desc='Testing',
            position=(2 * self.process_position),
            disable=self.is_disabled,
            leave=False,
            dynamic_ncols=True,
            file=sys.stdout
        )
        return bar

# Cell
class PrintLogsCallback(pl.Callback):
    "Logs Training logs to console after every epoch"
    def __init__(self, print_str: str = None, logger=None):
        self.print_str = 'Epoch: [{}] eta: {} loss: {:.4f} acc: {:.4f} valid_loss: {:.4f} valid_acc: {:.4f}'

        if logger is None: self.logger = log
        else             : self.logger = logger

    def on_epoch_start(self, *args, **kwargs):
        self.eta_start = time.time()

    def on_epoch_end(self, trainer, pl_module):
        metrics = trainer.callback_metrics
        train_loss = metrics['train/loss']
        train_acc  = metrics['train/acc']
        valid_loss = metrics['valid/loss']
        valid_acc  = metrics['valid/acc']

        end_time = time.time()
        self.eta_string = str(datetime.timedelta(seconds=int(end_time-self.eta_start)))
        self.curr_epoch = int(trainer.current_epoch)
        print_str = self.print_str.format(self.curr_epoch, self.eta_string,
                                        train_loss, train_acc,
                                        valid_loss, valid_acc)
        self.logger.info(print_str)

    def on_test_epoch_end(self, trainer, pl_module, *args, **kwargs):
        metrics = trainer.callback_metrics
        train_loss = metrics['train/loss']
        train_acc  = metrics['train/acc']
        valid_loss = metrics['valid/loss']
        valid_acc  = metrics['valid/acc']
        test_loss  = metrics['test/loss']
        test_acc   = metrics['test/acc']


        fmt_str1 = "Summary: [Train] loss: {:.4f} acc: {:.4f}"
        fmt_str2 = "Summary: [Valid] loss: {:.4f} acc: {:.4f}"
        fmt_str3 = "Summary: [Test]  loss: {:.4f} acc: {:.4f}"

        str1 = fmt_str1.format(train_loss, train_acc)
        str2 = fmt_str2.format(valid_loss, valid_acc)
        str3 = fmt_str3.format(test_loss, test_acc)

        self.logger.info(str1)
        self.logger.info(str2)
        self.logger.info(str3)