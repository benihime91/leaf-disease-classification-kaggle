# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/05a_lightning.core.ipynb (unless otherwise specified).

__all__ = ['fetch_scheduler', 'LightningCassava', 'LightningVisionTransformer']

# Cell
import os
import logging
from typing import Dict, List
from collections import namedtuple

import torch
from torch import nn
import torch.nn.functional as F

from hydra.utils import instantiate, call
from omegaconf import OmegaConf, DictConfig

import pytorch_lightning as pl

from ..core import *
from ..layers import *
from ..networks import *
from ..opts import *
from ..losses import *
from ..mixmethods import *
from ..networks import *

# Cell
def fetch_scheduler(optim, conf: DictConfig, litm: pl.LightningModule) -> Dict:
    "instantiates an `scheduler` for `optim` from `conf`"
    steps = len(litm.train_dataloader()) // litm.trainer.accumulate_grad_batches

    if conf.scheduler.function._target_ == "torch.optim.lr_scheduler.OneCycleLR":
        lrs = litm.lr_list
        lr_list = [lrs.encoder_lr, lrs.fc_lr]
        kwargs = dict(optimizer=optim, max_lr=lr_list, steps_per_epoch=steps)
        sch: torch.optim.lr_scheduler.OneCycleLR = instantiate(
            conf.scheduler.function, **kwargs
        )

    elif conf.scheduler.function._target_ == "src.opts.FlatCos":
        kwargs = dict(optimizer=optim, steps_per_epoch=steps)
        sch: FlatCos = instantiate(conf.scheduler.function, **kwargs)

    elif conf.scheduler.function._target_ == "src.opts.CosineAnnealingWarmupScheduler":
        kwargs = dict(optimizer=optim, steps_per_epoch=steps)
        sch: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts = instantiate(
            conf.scheduler.function, **kwargs
        )

    elif conf.scheduler.function._target_ == "src.opts.LinearSchedulerWithWarmup":
        kwargs = dict(optimizer=optim, steps_per_epoch=steps, warmup_steps=steps)
        sch: LinearSchedulerWithWarmup = instantiate(conf.scheduler.function, **kwargs)
    else:
        sch: torch.optim.lr_scheduler._LRScheduler = instantiate(
            conf.scheduler.function, optimizer=opt
        )

    # convert scheduler to lightning format
    sch = {
        "scheduler": sch,
        "monitor": conf.scheduler.metric_to_track,
        "interval": conf.scheduler.scheduler_interval,
        "frequency": 1,
    }

    return sch

# Cell
# TODO: add midlevel classification branch in learning.
class LightningCassava(pl.LightningModule):
    """LightningModule wrapper for `TransferLearningModel`"""

    _log = logging.getLogger(__name__)
    _log.setLevel(logging.INFO)

    def __init__(self, model, conf: DictConfig):
        super().__init__()

        # set up init args
        self.save_hyperparameters(conf)
        self.model = model
        self.accuracy = pl.metrics.Accuracy()

        if isinstance(self.model, VisionTransformer):
            self._log.warning("Use class src.lightning.core.LightningVisionTransformer")

        try:
            mixmethod = instantiate(self.hparams["mixmethod"])
        except:
            mixmethod = None

        if mixmethod is not None:
            if isinstance(mixmethod, SnapMix):
                assert isinstance(self.model, SnapMixTransferLearningModel)

        self.mix_fn = mixmethod
        self.loss_func = instantiate(self.hparams["loss"])
        self.val_labels_list = []
        self.val_preds_list = []
        self.one_batch = None

    def forward(self, xb):
        "forward method"
        return self.model(xb)

    def training_step(self, batch, batch_idx):
        "training step for one-batch"
        x, y = batch

        if self.mix_fn is not None:
            x = self.mix_fn(x, y, self.model)
            y_hat = self(x)
            loss = self.mix_fn.loss(self.loss_func, y_hat)

        else:
            y_hat = self(x)
            loss = self.loss_func(y_hat, y)

        self.one_batch = x

        train_acc = self.accuracy(y_hat, y)

        self.log("train/loss", loss, on_epoch=True)
        self.log("train/acc", train_acc, on_epoch=True)
        return loss

    def validation_step(self, batch, batch_idx):
        "Validation step for one-batch"
        x, y = batch
        y_hat = self(x)
        loss = self.loss_func(y_hat, y)
        acc = self.accuracy(y_hat, y)

        # For confusion matrix purposes
        preds = torch.argmax(y_hat, 1)
        val_labels = y.data.cpu().numpy()
        val_preds = preds.data.cpu().numpy()

        self.val_preds_list = self.val_preds_list + list(val_preds)
        self.val_labels_list = self.val_labels_list + list(val_labels)

        metrics = {"valid/loss": loss, "valid/acc": acc}

        self.log_dict(metrics)

    def test_step(self, batch, batch_idx):
        "test step for one-batch"
        x, y = batch
        y_hat = self(x)

        loss = self.loss_func(y_hat, y)
        acc = self.accuracy(y_hat, y)

        metrics = {"test/loss": loss, "test/acc": acc}
        self.log_dict(metrics)

    def configure_optimizers(self):
        param_list = [
            {"params": self.param_list[0], "lr": self.lr_list.encoder_lr},
            {"params": self.param_list[1], "lr": self.lr_list.fc_lr},
        ]

        opt = instantiate(self.hparams.optimizer, params=param_list)
        sch = fetch_scheduler(opt, self.hparams, self)
        return [opt], [sch]

    @property
    def lr_list(self) -> namedtuple:
        "returns lrs for encoder and fc of the model"
        lrs = namedtuple("Lrs", ["encoder_lr", "fc_lr"])
        encoder_lr = self.hparams.learning_rate / self.hparams.lr_mult
        fc_lr = self.hparams.learning_rate
        return lrs(encoder_lr, fc_lr)

    @property
    def param_list(self) -> List:
        "returns the list of parameters [params of encoder, params of fc]"
        param_list = [params(self.model.encoder), params(self.model.fc)]
        return param_list

    def load_state_from_checkpoint(self, path: str):
        "loads in the weights of the LightningModule from given checkpoint"
        self._log.info(f"Attempting to load checkpoint {os.path.relpath(path)}")
        checkpoint = torch.load(path, map_location=self.device)
        self._log.info(f"Successfully loaded checkpoint {os.path.relpath(path)}")

        self.load_state_dict(checkpoint["state_dict"])
        self._log.info(f"Successfully loaded weights from checkpoint {path}")

    def save_model_weights(self, path: str):
        "saves weights of self.model"
        self._log.info(f"Attempting to save weights to {os.path.relpath(path)}")
        state = self.model.state_dict()
        torch.save(state, path)
        self._log.info(f"Successfully saved weights to {os.path.relpath(path)}")

    def load_model_weights(self, path: str):
        "loads weights of self.model"
        self._log.info(f"Attempting to load weights from {os.path.relpath(path)}")
        state_dict = torch.load(path)
        self.model.load_state_dict(state_dict)
        self._log.info(f"Successfully loaded weights from {os.path.relpath(path)}")

# Cell
# @TODO: Add Snapmix support
class LightningVisionTransformer(pl.LightningModule):
    """LightningModule wrapper for `VisionTransfer`"""

    _log = logging.getLogger(__name__)
    _log.setLevel(logging.INFO)

    def __init__(self, model: VisionTransformer, conf: DictConfig = None):
        super().__init__()
        self.save_hyperparameters(conf)
        self.model = model
        self.accuracy = pl.metrics.Accuracy()

        try:
            mixmethod = instantiate(self.hparams["mixmethod"])
        except:
            mixmethod = None

        if mixmethod is not None:
            assert not isinstance(
                mixmethod, SnapMix
            ), "Snapmix not supported in Vision Transformer"

        self.mix_fn = mixmethod
        self.loss_func = instantiate(self.hparams["loss"])
        self.val_labels_list = []
        self.val_preds_list = []
        self.one_batch = None

    def forward(self, xb):
        return self.model(xb)

    def training_step(self, batch, batch_idx):
        x, y = batch

        if self.mix_fn is not None:
            x = self.mix_fn(x, y, self.model)
            y_hat = self(x)
            loss = self.mix_fn.loss(self.loss_func, y_hat)

        else:
            y_hat = self(x)
            loss = self.loss_func(y_hat, y)

        self.one_batch = x

        train_acc = self.accuracy(y_hat, y)

        self.log("train/loss", loss, on_epoch=True)
        self.log("train/acc", train_acc, on_epoch=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_func(y_hat, y)
        acc = self.accuracy(y_hat, y)

        # For confusion matrix purposes
        preds = torch.argmax(y_hat, 1)
        val_labels = y.data.cpu().numpy()
        val_preds = preds.data.cpu().numpy()

        self.val_preds_list = self.val_preds_list + list(val_preds)
        self.val_labels_list = self.val_labels_list + list(val_labels)

        metrics = {"valid/loss": loss, "valid/acc": acc}
        self.log_dict(metrics)

    def test_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)

        loss = self.loss_func(y_hat, y)
        acc = self.accuracy(y_hat, y)

        metrics = {"test/loss": loss, "test/acc": acc}
        self.log_dict(metrics)

    def configure_optimizers(self):
        param_list = [
            {"params": self.param_list[0], "lr": self.lr_list.encoder_lr},
            {"params": self.param_list[1], "lr": self.lr_list.fc_lr},
        ]

        opt = instantiate(self.hparams.optimizer, params=param_list)
        sch = fetch_scheduler(opt, self.hparams, self)
        return [opt], [sch]

    @property
    def lr_list(self) -> namedtuple:
        "returns lrs for encoder and fc of the model"
        lrs = namedtuple("Lrs", ["encoder_lr", "fc_lr"])
        encoder_lr = self.hparams.learning_rate / self.hparams.lr_mult
        fc_lr = self.hparams.learning_rate
        return lrs(encoder_lr, fc_lr)

    @property
    def param_list(self):
        "returns the list of parameters [(params of the model - params of head), params of head]"
        model_params = params(self.model)[:-2]
        head_params = params(self.model.model.head)
        param_list = [model_params, head_params]
        return param_list

    def load_state_from_checkpoint(self, path: str):
        "loads in the weights of the LightningModule from given checkpoint"
        self._log.info(f"Attempting to load checkpoint {os.path.relpath(path)}")
        checkpoint = torch.load(path, map_location=self.device)
        self._log.info(f"Successfully loaded checkpoint {os.path.relpath(path)}")

        self.load_state_dict(checkpoint["state_dict"])
        self._log.info(f"Successfully loaded weights from checkpoint {path}")

    def save_model_weights(self, path: str):
        "saves weights of self.model"
        self._log.info(f"Attempting to save weights to {os.path.relpath(path)}")
        state = self.model.state_dict()
        torch.save(state, path)
        self._log.info(f"Successfully saved weights to {os.path.relpath(path)}")

    def load_model_weights(self, path: str):
        "loads weights of self.model"
        self._log.info(f"Attempting to load weights from {os.path.relpath(path)}")
        state_dict = torch.load(path)
        self.model.load_state_dict(state_dict)
        self._log.info(f"Successfully loaded weights from {os.path.relpath(path)}")