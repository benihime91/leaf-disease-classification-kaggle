# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01b_data.datasests_factory.ipynb (unless otherwise specified).

__all__ = ['create_transform', 'CassavaClassificationModule']

# Cell
import albumentations as A
import torchvision.transforms as T
from hydra.utils import instantiate
from omegaconf import DictConfig, OmegaConf
from pytorch_lightning import LightningDataModule
from torch.utils.data import DataLoader, Dataset

from src import _logger
from .datasets import CassavaDataset, load_data

# Cell
def create_transform(cfg: DictConfig, verbose=False):
    "creates transoformations to be used in datasets"
    train_augs_initial = [instantiate(t) for t in cfg.train.before_mix]
    train_augs_final   = [instantiate(t) for t in cfg.train.after_mix]
    valid_augs = [instantiate(t) for t in cfg.valid]

    if cfg.backend == "torchvision":
        compose_func = T.Compose
    elif cfg.backend == "albumentations":
        compose_func = A.Compose

    train_augs_initial = compose_func(train_augs_initial)
    train_augs_final   = compose_func(train_augs_final)
    valid_augs = compose_func(valid_augs)
    return train_augs_initial, train_augs_final, valid_augs

# Cell
class CassavaClassificationModule(LightningDataModule):
    "Lightning datamodule for CassavaImageClassification Task"

    def __init__(self, cfg: DictConfig):
        "Note: `cfg` has to be the global hydra config"
        super().__init__()
        self.cfg = cfg
        # split the configuration files
        self.dset_cfg = cfg.data.dataset
        self.tfms_cfg = cfg.augmentations
        self.dls_conf = cfg.data.dataloader
        self.fold = self.dset_cfg.fold

    def prepare_data(self):
        # loads the data correspoind to the current fold
        self.data = load_data(self.dset_cfg.csv, self.dset_cfg.image_dir, self.fold, shuffle=True)

        self.train_data = self.data.loc[self.data["is_valid"] == False]
        self.valid_data = self.data.loc[self.data["is_valid"] == True]

        self.train_data = self.train_data.sample(frac=1).reset_index(inplace=False, drop=True)

        self.valid_data = self.valid_data.sample(frac=1).reset_index(inplace=False, drop=True)

    def setup(self, stage=None):
        _logger.info("Data processing configuration for current dataset:")
        _logger.info(f"\tinput_size: {cfg.input.input_size}")
        _logger.info(f"\tmean: {tuple(cfg.input.mean)}")
        _logger.info(f"\tstd: {tuple(cfg.input.std)}")
        _logger.info(f"\tinterpolation: {cfg.input.interpolation}")

        self.augs_initial, self.augs_final, self.augs_valid = create_transform(self.tfms_cfg)

        # Assign train/val datasets for use in dataloaders

        if self.tfms_cfg.backend == "torchvision":
            self.train_ds = CassavaDataset.from_torchvision_tfms(self.train_data,
                                                                 fn_col="filePath",
                                                                 label_col="label",
                                                                 transform=self.augs_initial)

            self.valid_ds = CassavaDataset.from_torchvision_tfms(self.valid_data,
                                                                 fn_col="filePath",
                                                                 label_col="label",
                                                                 transform=self.augs_valid)

        elif tfm_config.backend == "albumentations":
            self.train_ds = CassavaDataset.from_albu_tfms(self.train_data,
                                                          fn_col="filePath",
                                                          label_col="label",
                                                          transform=self.augs_initial)

            self.valid_ds = CassavaDataset.from_albu_tfms(self.train_data,
                                                          fn_col="filePath",
                                                          label_col="label",
                                                          transform=self.augs_initial)

        # use the same dataset for validation and test
        self.test_ds = self.valid_ds

    def train_dataloader(self):
        return DataLoader(self.train_ds, **self.dls_conf)

    def val_dataloader(self):
        return DataLoader(self.valid_ds, **self.dls_conf)

    def test_dataloader(self):
        return DataLoader(self.test_ds, **self.dls_conf)