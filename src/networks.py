# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03a_networks.ipynb (unless otherwise specified).

__all__ = ['activation_map', 'TransferLearningModel', 'SnapMixTransferLearningModel', 'VisionTransformer']

# Cell
from typing import List

import torch
from torch import nn
import torch.nn.functional as F

from .core import *
from .layers import *

# Cell
activation_map = dict(relu=nn.ReLU, mish=Mish, silu=nn.SiLU)

# Cell
class TransferLearningModel(nn.Module):
    "Transfer Learning with `encoder`"

    def __init__(
        self,
        encoder: nn.Module,
        c: int,
        cut: int = -2,
        act: nn.Module = nn.ReLU(inplace=True),
        lin_ftrs: int = 512,
    ):
        """
        Args:
            encoder (nn.Module) : the classifer to extract features.
            c (int) : number of output classes.
            cut (int) : number of layers to cut/keep from the encoder.
            act (nn.Module) : activation function for the head.
            lin_ftrs (int): linear features for the fc layer
        """
        super(TransferLearningModel, self).__init__()
        self.c = c
        self.encoder = encoder
        self.encoder = cut_model(self.encoder, cut)

        # create the custom head for the model
        feats = num_features_model(self.encoder, in_chs=3) * 2
        self.fc = create_head(feats, n_out=self.c, lin_ftrs=lin_ftrs, act=act)

        if isinstance(act, nn.ReLU):
            apply_init(self.fc, torch.nn.init.kaiming_normal_)
        else:
            apply_init(self.fc, torch.nn.init.kaiming_uniform_)

    def forward(self, xb):
        return self.fc(self.encoder(xb))

# Cell
# TODO: add midlevel classification branch in learning.
class SnapMixTransferLearningModel(nn.Module):
    "Transfer Learning with model to be comaptible with Snapmix"

    def __init__(self, encoder: nn.Module, c: int, cut: int = -2, **kwargs):
        """
        Args:
            encoder (nn.Module): the classifer to extract features
            c (int) : number of output classes
            cut (int) : number of layers to cut/keep from the encoder
        """
        super(SnapMixTransferLearningModel, self).__init__()
        self.c = c

        try:
            # for timm models
            try:
                feats = encoder.fc.in_features
            except:
                feats = encoder.classifier.in_features

        except:
            # for models from pretrainedmodels
            feats = encoder.last_linear.in_features

        # build model
        self.encoder = cut_model(encoder, cut)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.flat_layer = nn.Flatten()
        self.fc = nn.Linear(feats, self.c)

        apply_init(self.fc, torch.nn.init.kaiming_normal_)

    def forward(self, xb):
        fmps = self.encoder(xb)
        x = self.pool(fmps)
        x = self.flat_layer(x)
        return self.fc(x)

# Cell
# from : https://www.kaggle.com/abhinand05/vision-transformer-vit-tutorial-baseline/output#Vision-Transformers:-A-gentle-introduction
class VisionTransformer(nn.Module):
    def __init__(self, encoder: nn.Module, c: int, **kwargs):
        super(VisionTransformer, self).__init__()
        self.model = encoder
        self.model.head = nn.Linear(self.model.head.in_features, c)
        apply_init(self.model.head, torch.nn.init.kaiming_normal_)

    def forward(self, xb):
        return self.model(xb)