# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_mixmethods.ipynb (unless otherwise specified).

__all__ = ['NoneReduce', 'Mixup', 'Cutmix', 'SnapMix', 'MixupAndCutmix']

# Cell
import numpy as np
from functools import partial
from collections import namedtuple

import torch
import torch.nn.functional as F
from torch.nn import Module

from .core import *

# Cell
# from : https://github.com/fastai/fastai/blob/493023513ddd5157647bd10e9cebbbbdc043474c/fastai/layers.py#L582
class NoneReduce:
    "A context manager to evaluate `loss_func` with none reduce."

    def __init__(self, loss_func):
        self.loss_func, self.old_red = loss_func, None

    def __enter__(self):
        if hasattr(self.loss_func, "reduction"):
            self.old_red = self.loss_func.reduction
            self.loss_func.reduction = "none"
            return self.loss_func
        else:
            return partial(self.loss_func, reduction="none")

    def __exit__(self, type, value, traceback):
        if self.old_red is not None:
            self.loss_func.reduction = self.old_red

# Cell
# modified from : https://github.com/facebookresearch/mixup-cifar10/blob/master/train.py
class Mixup:
    """Implements mixup from https://arxiv.org/abs/1710.09412

    Args:
        alpha (float): mixup alpha value.
        conf_prob (float): probability of applying mixup per batch or elements
        steps (int): number of batches or elements to apply mixup
    """

    def __init__(self, alpha: float = 0.4, conf_prob: float = 1.0, steps: int = None):
        self.distrib = alpha
        self.conf_prob = conf_prob

        if steps is None:
            self.steps = 100000000
        else:
            self.steps = steps

        self.device = None
        self.is_active = False
        self.curr_step = 0

    def __str__(self):
        mix = namedtuple("Mixup", ["probability", "alpha", "iters"])
        return str(mix(self.conf_prob, self.distrib, self.steps))

    def __call__(self, xb: torch.Tensor, yb: torch.Tensor, model: Module = None):
        if self.curr_step < self.steps:
            xb = self.call(xb=xb, yb=yb, model=model)
        else:
            self.yb1 = yb
            self.is_active = False

        self.curr_step += 1
        return xb

    def call(self, xb: torch.Tensor, yb: torch.Tensor, model: Module = None):
        r = np.random.rand(1)

        if r < self.conf_prob:
            self.is_active = True
            bs = xb.size()[0]

            if self.device is None:
                self.device = xb.device

            self.lam = np.random.beta(self.distrib, self.distrib)
            self.lam = torch.tensor(self.lam, device=self.device)

            index = torch.randperm(bs, device=self.device)
            xb = self.lam * xb + (1 - self.lam) * xb[index, :]
            self.yb1, self.yb2 = yb, yb[index]

        else:
            self.yb1 = yb
            self.is_active = False
        return xb

    def loss(self, lf, pred, reduction="mean"):
        self.old_lf = lf
        if self.is_active:
            with NoneReduce(self.old_lf) as lf:
                loss = torch.lerp(lf(pred, self.yb2), lf(pred, self.yb1), self.lam)

            loss = loss.mean() if reduction == "mean" else loss.sum() if reduction == "sum" else loss

        if not self.is_active:
            loss = self.old_lf(pred, self.yb1)

        return loss

# Cell
# modified from : https://github.com/clovaai/CutMix-PyTorch/blob/master/train.py
class Cutmix:
    """Implementation of `https://arxiv.org/abs/1905.04899`

    Args:
        alpha (float): cutmix alpha value.
        conf_prob (float): probability of applying cutmix per batch or elements
        steps (int): number of batches or elements to apply cutmix
    """

    def __init__(self, alpha: float = 0.4, conf_prob: float = 1.0, steps: int = None):
        self.distrib = alpha
        self.conf_prob = conf_prob

        if steps is None:
            self.steps = 100000000
        else:
            self.steps = steps

        self.device = None
        self.is_active = False
        self.curr_step = 0

    def rand_bbox(self, W, H, lam):
        cut_rat = torch.sqrt(1.0 - lam)
        cut_w = torch.round(W * cut_rat).type(torch.long).to(self.device)
        cut_h = torch.round(H * cut_rat).type(torch.long).to(self.device)

        # uniform
        cx = torch.randint(0, W, (1,), device=self.device)
        cy = torch.randint(0, H, (1,), device=self.device)
        x1 = torch.clamp(cx - cut_w // 2, 0, W)
        y1 = torch.clamp(cy - cut_h // 2, 0, H)
        x2 = torch.clamp(cx + cut_w // 2, 0, W)
        y2 = torch.clamp(cy + cut_h // 2, 0, H)
        return (
            x1.to(self.device),
            y1.to(self.device),
            x2.to(self.device),
            y2.to(self.device),
        )

    def __call__(self, xb: torch.Tensor, yb: torch.Tensor, model: Module = None):
        if self.curr_step < self.steps:
            xb = self.call(xb=xb, yb=yb, model=model)
        else:
            self.yb1 = yb
            self.is_active = False

        self.curr_step += 1
        return xb

    def call(self, xb: torch.Tensor, yb: torch.Tensor, model: Module = None):
        r = np.random.rand(1)

        if r < self.conf_prob:
            self.is_active = True

            bs, _, H, W = xb.size()

            if self.device is None:
                self.device = xb.device

            self.lam = np.random.beta(self.distrib, self.distrib)
            self.lam = torch.tensor(self.lam, device=self.device)

            index = torch.randperm(bs, device=self.device)

            self.yb1, self.yb2 = yb, yb[index]
            x1, y1, x2, y2 = self.rand_bbox(W, H, self.lam)
            xb[:, :, x1:x2, y1:y2] = xb[index, :, x1:x2, y1:y2]

            self.lam = (1 - ((x2 - x1) * (y2 - y1)) / float(W * H)).item()

        else:
            self.yb1 = yb
            self.is_active = False

        return xb

    def __str__(self):
        mix = namedtuple("Cutmix", ["probability", "alpha", "iters"])
        return str(mix(self.conf_prob, self.distrib, self.steps))

    def loss(self, lf, pred, reduction="mean"):
        self.old_lf = lf

        if self.is_active:
            with NoneReduce(self.old_lf) as lf:
                loss = torch.lerp(lf(pred, self.yb2), lf(pred, self.yb1), self.lam)
            loss = loss.mean() if reduction == "mean" else loss.sum() if reduction == "sum" else loss

        if not self.is_active:
            loss = self.old_lf(pred, self.yb1)

        return loss

# Cell
# @TODO: add midlevel classification branch in learning.
class SnapMix:
    """Implementation of https://arxiv.org/abs/2012.04846

    Args:
        alpha (float): snapmix alpha value.
        conf_prob (float): probability of applying snapmix per batch or elements
        steps (int): number of batches or elements to apply snapmix

    """

    def __init__(self, alpha: float = 5.0, conf_prob: float = 1.0, steps: int = None):
        self.distrib = alpha
        self.conf_prob = conf_prob

        if steps is None:
            self.steps = 100000000
        else:
            self.steps = steps

        self.device = None
        self.is_active = False
        self.curr_step = 0

    def rand_bbox(self, W, H, lam):
        cut_rat = torch.sqrt(1.0 - lam)
        cut_w = torch.round(W * cut_rat).type(torch.long).to(self.device)
        cut_h = torch.round(H * cut_rat).type(torch.long).to(self.device)

        # uniform
        cx = torch.randint(0, W, (1,), device=self.device)
        cy = torch.randint(0, H, (1,), device=self.device)

        x1 = torch.clamp(cx - cut_w // 2, 0, W)
        y1 = torch.clamp(cy - cut_h // 2, 0, H)
        x2 = torch.clamp(cx + cut_w // 2, 0, W)
        y2 = torch.clamp(cy + cut_h // 2, 0, H)

        return (
            x1.to(self.device),
            y1.to(self.device),
            x2.to(self.device),
            y2.to(self.device),
        )

    def get_spm(
        self, input: torch.Tensor, target: torch.Tensor, model: Module,
    ):

        bs, _, H, W = input.size()
        img_size = (H, W)
        bs = input.size(0)

        with torch.no_grad():
            fms = model.encoder(input)

            # grab the classification layer of the model
            clsw = model.fc

            weight = clsw.weight.data
            weight = weight.view(weight.size(0), weight.size(1), 1, 1)
            bias = clsw.bias.data

            fms = F.relu(fms)
            poolfea = F.adaptive_avg_pool2d(fms, (1, 1)).squeeze()
            clslogit = F.softmax(clsw.forward(poolfea))

            logitlist = []

            for i in range(bs):
                logitlist.append(clslogit[i, target[i]])

            clslogit = torch.stack(logitlist)

            out = F.conv2d(fms, weight, bias=bias)

            outmaps = []
            for i in range(bs):
                evimap = out[i, target[i]]
                outmaps.append(evimap)

            if img_size is not None:
                outmaps = torch.stack(outmaps)
                outmaps = outmaps.view(
                    outmaps.size(0), 1, outmaps.size(1), outmaps.size(2)
                )
                outmaps = F.interpolate(
                    outmaps, img_size, mode="bilinear", align_corners=False
                )

            outmaps = outmaps.squeeze()

            for i in range(bs):
                outmaps[i] -= outmaps[i].min()
                outmaps[i] /= outmaps[i].sum()

        return outmaps, clslogit

    def __call__(self, xb: torch.Tensor, yb: torch.Tensor, model: Module = None):
        if self.curr_step < self.steps:
            xb = self.call(xb=xb, yb=yb, model=model)
        else:
            self.yb1 = yb
            self.is_active = False

        self.curr_step += 1
        return xb

    def call(self, xb: torch.Tensor, yb: torch.Tensor, model: Module):
        r = np.random.rand(1)

        if r < self.conf_prob:
            self.is_active = True

            bs, _, H, W = xb.size()

            self.img_size = (H, W)

            if self.device is None:
                self.device = xb.device

            lam_a = torch.ones(xb.size(0), device=self.device)
            lam_b = 1 - lam_a

            self.yb = yb
            self.yb1 = yb.clone()

            rand_index = torch.randperm(bs, device=self.device)

            wfmaps, _ = self.get_spm(xb, yb, model)

            lam = np.random.beta(self.distrib, self.distrib)
            lam = torch.tensor(lam, device=self.device)

            lam1 = np.random.beta(self.distrib, self.distrib)
            lam1 = torch.tensor(lam1, device=self.device)

            rand_index = torch.randperm(bs, device=self.device)

            wfmaps_b = wfmaps[rand_index, :, :]
            self.yb1 = self.yb[rand_index]

            same_label = self.yb == self.yb1

            bbx1, bby1, bbx2, bby2 = self.rand_bbox(W, H, lam)
            bbx1_1, bby1_1, bbx2_1, bby2_1 = self.rand_bbox(W, H, lam1)

            area = (bby2 - bby1) * (bbx2 - bbx1)
            area1 = (bby2_1 - bby1_1) * (bbx2_1 - bbx1_1)

            if area1 > 0 and area > 0:
                ncont = xb[rand_index, :, bbx1_1:bbx2_1, bby1_1:bby2_1].clone()
                ncont = F.interpolate(
                    ncont,
                    size=(bbx2 - bbx1, bby2 - bby1),
                    mode="bilinear",
                    align_corners=True,
                )
                xb[:, :, bbx1:bbx2, bby1:bby2] = ncont

                lam_a = 1 - wfmaps[:, bbx1:bbx2, bby1:bby2].sum(2).sum(1) / (
                    wfmaps.sum(2).sum(1) + 1e-8
                )
                lam_b = wfmaps_b[:, bbx1_1:bbx2_1, bby1_1:bby2_1].sum(2).sum(1) / (
                    wfmaps_b.sum(2).sum(1) + 1e-8
                )

                tmp = lam_a.clone()

                lam_a[same_label] += lam_b[same_label]
                lam_b[same_label] += tmp[same_label]

                lam = 1 - (
                    (bbx2 - bbx1) * (bby2 - bby1) / (xb.size()[-1] * xb.size()[-2])
                )
                lam_a[torch.isnan(lam_a)] = lam
                lam_b[torch.isnan(lam_b)] = 1 - lam

            self.yb1, self.yb2 = self.yb, self.yb1
            self.lam_a, self.lam_b = lam_a.to(self.device), lam_b.to(self.device)

        else:
            self.yb1 = yb
            self.is_active = False

        return xb

    def __str__(self):
        mix = namedtuple("SnapMix", ["probability", "alpha", "iters"])
        return str(mix(self.conf_prob, self.distrib, self.steps))

    def loss(self, lf, pred, *args, **kwargs):
        if self.is_active:
            loss_a = lf(pred, self.yb1)
            loss_b = lf(pred, self.yb2)
            loss = torch.mean(loss_a * self.lam_a + loss_b * self.lam_b)

        if not self.is_active:
            loss = lf(pred, self.yb1)

        return loss

# Cell
class MixupAndCutmix:
    """ Applies Mixup or Cumtix to a batch or elements.

     Args:
         cutmix_alpha (float): cutmix alpha value, cutmix is active if > 0.
         mixup_alpha (float): mixup alpha value, mixup is active if > 0.
         switch_prob (float): probability of switching to cutmix instead of mixup when both are active.
         conf_prob (float): probability of applying mixup or cutmix per batch or element.
         steps (int): number of batches or elements to apply snapmix.
     """

    def __init__(
        self,
        cutmix_alpha: float = 1.0,
        mixup_alpha: float = 0.4,
        switch_prob: float = 0.5,
        conf_prob: float = 0.5,
        steps: int = None,
    ):

        self.cutmix = Cutmix(cutmix_alpha, conf_prob=1.0)
        self.mixup = Mixup(mixup_alpha, conf_prob=1.0)

        self.switch_prob, self.conf_prob = switch_prob, conf_prob
        self.cx_alpha, self.mx_aplha = cutmix_alpha, mixup_alpha

        self.is_active = False
        self.curr_method = None

        if steps is None:
            self.steps = 100000000
        else:
            self.steps = steps

        self.curr_step = 0

    def __str__(self):
        mix = namedtuple(
            "MixupCutmix", ["probability", "alphas", "switch_prob", "iters"]
        )
        return str(
            mix(
                self.conf_prob,
                [self.mx_aplha, self.cx_alpha],
                self.switch_prob,
                self.steps,
            )
        )

    def __call__(self, xb: torch.Tensor, yb: torch.Tensor, model: Module = None):
        if self.curr_step < self.steps:
            xb = self.call(xb=xb, yb=yb, model=model)
        else:
            self.yb1 = yb
            self.is_active = False

        self.curr_step += 1
        return xb

    def call(self, xb: torch.Tensor, yb: torch.Tensor, model: Module = None):
        r = np.random.rand(1)

        if r < self.conf_prob:
            q = np.random.rand(1)

            if q < self.switch_prob:
                self.curr_method = self.cutmix
            else:
                self.curr_method = self.mixup

            self.is_active = True

            xb = self.curr_method(xb, yb, model)
            self.yb1 = self.curr_method.yb1

        else:
            self.yb1 = yb
            self.is_active = False

        return xb

    def loss(self, lf, pred, *args, **kwargs):
        if not self.is_active:
            loss = lf(pred, self.yb1)

        if self.is_active:
            loss = self.curr_method.loss(lf, pred, *args, **kwargs)

        return loss